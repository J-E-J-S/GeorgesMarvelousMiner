<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Genet</journal-id><journal-id journal-id-type="iso-abbrev">Front Genet</journal-id><journal-id journal-id-type="publisher-id">Front. Genet.</journal-id><journal-title-group><journal-title>Frontiers in Genetics</journal-title></journal-title-group><issn pub-type="epub">1664-8021</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6882287</article-id><article-id pub-id-type="doi">10.3389/fgene.2019.01054</article-id><article-categories><subj-group subj-group-type="heading"><subject>Genetics</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Sparse Graph Regularization Non-Negative Matrix Factorization Based on Huber Loss Model for Cancer Data Analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Chuan-Yuan</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/776536"/></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Jin-Xing</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="author-notes" rid="fn001">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Na</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zheng</surname><given-names>Chun-Hou</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="aff1">
<sup>1</sup>
<institution>School of Information Science and Engineering, Qufu Normal University</institution>, <addr-line>Rizhao</addr-line>, <country>China</country>
</aff><aff id="aff2">
<sup>2</sup>
<institution>School of Software Engineering, Qufu Normal University</institution>, <addr-line>Qufu</addr-line>, <country>China</country>
</aff><author-notes><fn fn-type="edited-by"><p>Edited by: Hongmin Cai, South China University of Technology, China</p></fn><fn fn-type="edited-by"><p>Reviewed by: Wen-Sheng Chen, Shenzhen University, China; Xiangxiang Zeng, Xiamen University, China</p></fn><corresp id="fn001">*Correspondence: Jin-Xing Liu, <email xlink:href="mailto:sdcavell@126.com" xlink:type="simple">sdcavell@126.com</email>
</corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Bioinformatics and Computational Biology, a section of the journal Frontiers in Genetics</p></fn></author-notes><pub-date pub-type="epub"><day>20</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>10</volume><elocation-id>1054</elocation-id><history><date date-type="received"><day>20</day><month>7</month><year>2019</year></date><date date-type="accepted"><day>01</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2019 Wang, Liu, Yu and Zheng</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Wang, Liu, Yu and Zheng</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Non-negative matrix factorization (NMF) is a matrix decomposition method based on the square loss function. To exploit cancer information, cancer gene expression data often uses the NMF method to reduce dimensionality. Gene expression data usually have some noise and outliers, while the original NMF loss function is very sensitive to non-Gaussian noise. To improve the robustness and clustering performance of the algorithm, we propose a sparse graph regularization NMF based on Huber loss model for cancer data analysis (Huber-SGNMF). Huber loss is a function between <italic>L</italic>
<sub>1</sub>-norm and <italic>L</italic>
<sub>2</sub>-norm that can effectively handle non-Gaussian noise and outliers. Taking into account the sparsity matrix and data geometry information, sparse penalty and graph regularization terms are introduced into the model to enhance matrix sparsity and capture data manifold structure. Before the experiment, we first analyzed the robustness of Huber-SGNMF and other models. Experiments on The Cancer Genome Atlas (TCGA) data have shown that Huber-SGNMF performs better than other most advanced methods in sample clustering and differentially expressed gene selection.</p></abstract><kwd-group><kwd>non-negative matrix factorization</kwd><kwd>Huber loss</kwd><kwd>sample clustering</kwd><kwd>graph regularization</kwd><kwd>robustness</kwd></kwd-group><funding-group><award-group><funding-source id="cn001">National Natural Science Foundation of China<named-content content-type="fundref-id">10.13039/501100001809</named-content></funding-source><award-id rid="cn001">61572284, 61872220, 61873001</award-id></award-group></funding-group><counts><fig-count count="4"/><table-count count="5"/><equation-count count="41"/><ref-count count="45"/><page-count count="11"/><word-count count="5165"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Cancer is considered to be the number one killer of human health. The development of high-throughput sequencing technology has enabled researchers to obtain more comprehensive information about cancer patients (<xref rid="B4" ref-type="bibr">Chen et al., 2019</xref>). The gene expression data of cancer patients can be more used for effective data mining through computational methods (<xref rid="B5" ref-type="bibr">Chen et al., 2018</xref>). In general, cancer gene expression data are characterized by high dimensionality, which is extremely difficult for data analysis. How to effectively reduce the dimensionality of data is the key to analyzing cancer data. Principal component analysis (PCA) (<xref rid="B12" ref-type="bibr">Feng et al., 2019</xref>), locally linear embedding (LLE) (<xref rid="B34" ref-type="bibr">Roweis and Saul, 2000</xref>), and non-negative matrix factorization (NMF) (<xref rid="B44" ref-type="bibr">Yu et al., 2017</xref>) are common methods for reducing the data dimensionality. Unlike several other methods, NMF can find two non-negative matrices and its product can effectively restore the original matrix. The non-negative constraint guarantees additive combinations between different elements. NMF demonstrates its advantages in facial recognition, speech processing, document clustering, and recommendation systems (<xref rid="B13" ref-type="bibr">Guillamet and Vitri&#x000e0;, 2002</xref>; <xref rid="B42" ref-type="bibr">Xu et al., 2003</xref>; <xref rid="B35" ref-type="bibr">Schmidt and Olsson, 2006</xref>; <xref rid="B27" ref-type="bibr">Luo et al., 2014</xref>).</p><p>NMF has developed rapidly in recent years, and several variants of NMF have been proposed to improve the effectiveness of the decomposition. Cai et al. proposed graph regularized NMF (GNMF) for data representation (<xref rid="B3" ref-type="bibr">Cai et al., 2011</xref>). GNMF considers the association between points to preserve the internal structure of the data. Kim et al. applied the <italic>L</italic>
<sup>1</sup>-norm constraint on the coefficient matrix to introduce sparse NMF for clustering (SNMF) (<xref rid="B17" ref-type="bibr">Kim and Park, 2007</xref>). Sparseness is more likely to remove redundant features of data. The most of cancer data have noise and outliers, and the original NMF cannot solve this. Wang et al. introduced Characteristic Gene Selection Based on Robust GNMF (RGNMF) (<xref rid="B36" ref-type="bibr">Wang et al., 2016a</xref>) to improve the robustness of the algorithm. RGNMF assumes that the loss follows Laplacian distribution and uses the loss function of the <italic>L</italic>
<sup>2,1</sup>-norm (<xref rid="B18" ref-type="bibr">Kong et al., 2011</xref>) constraint. The <italic>L</italic>
<sup>2,1</sup>-norm combines the advantages of the <italic>L</italic>
<sup>2</sup>-norm and the <italic>L</italic>
<sup>1</sup>-norm, which impose an <italic>L</italic>
<sup>2</sup>-norm constraint on the entire data space and an <italic>L</italic>
<sup>1</sup>-norm constraint on the sum of the different data points (<xref rid="B10" ref-type="bibr">Ding et al., 2006</xref>).</p><p>The original NMF model is simple to understand and computationally fast, but the squared loss function is too sensitive to outliers and noise. Mao et al. proposed the correntropy induced metric based GNMF (CGNMF) (<xref rid="B28" ref-type="bibr">Mao et al., 2014</xref>) that changed the original loss function. The correntropy uses <italic>L</italic>
<sup>0</sup>-norm approximation for large outliers and noise through kernel function filtering, and the normal data is constrained by the <italic>L</italic>
<sup>2</sup>-norm (<xref rid="B25" ref-type="bibr">Liu et al., 2007</xref>), so it is not sensitive to outliers and noise. Du et al. proposed Huber-NMF (<xref rid="B11" ref-type="bibr">Du et al., 2012</xref>), which is also a loss function that is insensitive to outliers and noise. It uses approximate <italic>L</italic>
<sup>1</sup>-norm processing for outliers and noise, and <italic>L</italic>
<sup>2</sup>-norm for valuable data. Correntropy uses kernel functions to control weights, and Huber loss uses a different function approximation for different data through threshold adjustment. The robustness analysis of these several non-square loss models is given in the experimental part. To compare the performance of the NMF algorithm, the robust PCA (RPCA) based method for discovering differentially expressed genes proposed by <xref rid="B24" ref-type="bibr">Liu et al. (2013)</xref> is added to the experiment.</p><p>In this paper, we propose a model called sparse graph regularization NMF based on Huber Loss Model for Cancer Data Analysis (Huber-SGNMF). It effectively combines Huber loss, manifold structure, and sparse constraint. Huber loss is based on the relationship between <italic>L</italic>
<sup>1</sup>-norm and <italic>L</italic>
<sup>2</sup>-norm to approximate different data. In detail, Huber loss adjusts the square loss or linear loss to the data according to the threshold to enhance the robustness of the model to outliers. Geometric information in high-dimensional data should remain locally constant in low-dimensional representations (<xref rid="B3" ref-type="bibr">Cai et al., 2011</xref>), so graph regularization is added to preserve the manifold structure of the data. Sparse constraints in the model can remove redundant features contained in the data to reduce the amount of model calculations and improve clustering performance (<xref rid="B17" ref-type="bibr">Kim and Park, 2007</xref>).</p><p>The contributions of this article are as follows:</p><list list-type="order"><list-item><p>The squared loss of the original NMF is too sensitive to outliers and noise; so, we use a more robust Huber loss combined with NMF. The Huber loss considers the relationship between the <italic>L</italic>
<sup>1</sup>-norm and the <italic>L</italic>
<sup>2</sup>-norm to effectively handle non-Gaussian noise and large outliers. For the update rules of Huber loss, we use the multiplicative iterative algorithm based on semi-quadratic optimization to find the optimal solution.</p></list-item><list-item><p>The NMF model fits the data in Euclidean space but does not consider the intrinsic geometry of the data space. If the data is related in high-dimensional space, then we believe that the data represented by the low-dimensional should also be closely related. Considering the manifolds embedded in the high-dimensional environment space, we add graph Laplacian as a regularization term to the model. Graph regularization takes into account the impact of recent neighbors on data, and retaining graph structure can make NMF more distinguishable.</p></list-item><list-item><p>Sparse matrices can remove redundant data, reducing data complexity and model computational difficulty. In data analysis, sparsity can improve clustering performance by reducing the difficulty of feature selection. The <italic>L</italic>
<sup>2,1</sup>-norm as a sparse constraint is added to the model because the <italic>L</italic>
<sup>2,1</sup>-norm is robust and can achieve row sparse effect.</p></list-item></list><p>The remainder of this paper is organized as follows. The introduction of related work is shown in Section 2. Models and solution optimization are presented in Section 3. The experiment and analysis are arranged in Section 4. Section 5 summarizes the entire paper.</p></sec><sec id="s2"><title>Related Work</title><sec id="s2_1"><title>Non-Negative Matrix Factorization</title><p>NMF is a dimensionality reduction method based on partial representation. For a given dataset <inline-formula><mml:math id="M1"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, NMF can decompose it into the basic matrix <inline-formula><mml:math id="M2"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and the coefficient matrix <inline-formula><mml:math id="M3"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, with the purpose of approximating the original matrix by two matrix products. In general, the rank of matrix factorization <italic>k</italic> is selected by the number of larger singular values.</p><p>For gene expression data matrix <inline-formula><mml:math id="M4"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, each row represents a gene corresponding to <italic>n</italic> samples, and each column represents a sample composed of <italic>m</italic> genes. Moreover, <bold>U</bold> contains <italic>m</italic> rows of metagene and <bold>V</bold> contains <italic>n</italic> rows of metapattern (<xref rid="B23" ref-type="bibr">Liu et al., 2018</xref>). Each column of <bold>V</bold> is a projection of a corresponding sample vector in <bold>X</bold> according to the basic matrix <bold>U</bold> (<xref rid="B21" ref-type="bibr">Li et al., 2017</xref>). NMF is visualized on gene expression data as shown in <xref ref-type="fig" rid="f1">
<bold>Figure 1</bold>
</xref>.</p><fig id="f1" position="float"><label>Figure 1</label><caption><p>The gene expression data matrix <inline-formula><mml:math id="M5"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is decomposed into a low-dimensional basic matrix <inline-formula><mml:math id="M6"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and a low-dimensional coefficient matrix <inline-formula><mml:math id="M7"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The product of two low-dimensional matrices can approximate the original matrix.</p></caption><graphic xlink:href="fgene-10-01054-g001"/></fig><p>The NMF loss function is minimized as follows:</p><disp-formula><label>(1)</label><mml:math id="M8"><mml:mrow><mml:mi>min</mml:mi><mml:mtext>&#x02003;</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mtext>&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>where <inline-formula><mml:math id="M9"><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the application of the Frobenius norm to the matrix.</p><p>Lee and Seung proposed the use of multiplicative iterative update rules to solve the optimal solution of NMF (<xref rid="B20" ref-type="bibr">Lee and Seung, 1999</xref>). Its update formula is as follows:</p><disp-formula><label>(2)</label><mml:math id="M10"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(3)</label><mml:math id="M11"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>where <italic>u</italic>
<italic><sub>ik</sub></italic> and <italic>v</italic>
<italic><sub>kj</sub></italic> are elements belonging to <bold>U</bold> and <bold>V</bold>, respectively. The non-negative constraints of <bold>U</bold> and <bold>V</bold> only allow additive combinations between different elements, so NMF can learn part-based representations (<xref rid="B3" ref-type="bibr">Cai et al., 2011</xref>).</p></sec><sec id="s2_2"><title>Huber Loss</title><p>Data usually contain a small amount of outliers and noise, which can have a worse effect on model reconstruction. For noise and outliers in the dataset, Huber loss uses weighted <italic>L</italic>
<sub>1</sub>-norm processing because the <italic>L</italic>
<sub>1</sub>-norm is robust and can effectively handle outliers and noise (<xref rid="B14" ref-type="bibr">Guofa et al., 2011</xref>; <xref rid="B45" ref-type="bibr">Yu et al., 2016</xref>). For other valuable data in the dataset, Huber losses still use <italic>L</italic>
<sub>2</sub>-norm loss to fit the data. Huber loss function <italic>&#x003b4;</italic>(<italic>&#x000b7;</italic>) is defined as follows:</p><disp-formula><label>(4)</label><mml:math id="M12"><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mi>c</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula><p>where <italic>c</italic> represents the threshold parameter of the data using the <italic>L</italic>
<sub>1</sub>-norm or the <italic>L</italic>
<sub>2</sub>-norm. This function is a bounded and convex function that minimizes the effects of a single anomaly point (<xref rid="B8" ref-type="bibr">Chreiky et al., 2016</xref>). Huber losses often apply to the insensitive outliers and noise contained in the data, which are often difficult to find using the squared loss function (<xref rid="B11" ref-type="bibr">Du et al., 2012</xref>).</p></sec><sec id="s2_3"><title>Manifold Regularization</title><p>The manifold learning theory (<xref rid="B2" ref-type="bibr">Belkin and Niyogi, 2001</xref>) shows that the internal manifold structure of the data can be effectively simulated by the nearest neighbor of the data points. Each data point finds its nearest <italic>p</italic> neighbors and connects the data points to the neighbors with edges. There are many ways to define the weight of an edge, most commonly 0&#x02013;1 weighted: <bold>W</bold>
<sub>ij</sub>=1, if and only if nodes <italic>i</italic> and <italic>j</italic> are connected by edges. The advantage of this weighting method is that it is easy to calculate.</p><p>Weight matrix <bold>W</bold>
<italic><sub>ij</sub></italic> is only used to measure the intimacy between data points. For the low-dimensional representation <bold>s</bold>
<italic><sub>j</sub></italic> of the high dimensional data <bold>x</bold>
<italic><sub>j</sub></italic>, the Euclidean distance <inline-formula><mml:math id="M13"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>O</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is typically used to measure the similarity between two low-dimensional data points. According to the intimacy weight W, the smoothness of the two low-dimensional vectors can be measured as follows:</p><disp-formula><label>(5)</label><mml:math id="M14"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mphantom/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mphantom/><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>D</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>W</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mphantom/><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where tr(&#x000b7;) denotes the trace of a matrix. The matrix <bold>D</bold> is defined as a diagonal matrix with diagonal elements <inline-formula><mml:math id="M15"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> The graph Laplacian (<xref rid="B26" ref-type="bibr">Liu et al., 2014</xref>) matrix <bold>L</bold> is defined as <bold>L</bold>=<bold>D</bold>-<bold>W</bold>.</p><p>We hope that if the high-dimensional data <bold>x</bold>
<italic><sub>j</sub></italic> and <bold>x</bold>
<italic><sub>l</sub></italic> are very intimate, then <bold>s</bold>
<italic><sub>j</sub></italic> and <bold>s</bold>
<italic><sub>l</sub></italic> should be close enough in low-dimensional representations (<xref rid="B3" ref-type="bibr">Cai et al., 2011</xref>). Therefore, minimizing <italic>R</italic> is added to our model to encode the internal geometry of the data.</p></sec></sec><sec id="s3"><title>Method</title><sec id="s3_1"><title>The Huber-Sgnmf Model</title><p>Based on the Huber loss function, we proposed a novel model that preserves the manifold structure and sparsity simultaneously. The Huber loss is combined with NMF to enhance NMF robustness. To further optimize the model, the graph regularization term and the <italic>L</italic>
<sub>2,1</sub>-norm are added to the loss function as constraints. <italic>L</italic>
<sub>2,1</sub>-norm mathematical expression is as follows:</p><disp-formula><label>(6)</label><mml:math id="M16"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>The Huber-SGNMF final model <italic>O</italic>
<italic><sub>Huber</sub></italic>
<sub>&#x02212;</sub>
<italic><sub>SGNMF</sub></italic> is as follows:</p><disp-formula><label>(7)</label><mml:math id="M17"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mtext>&#x000a0;</mml:mtext><mml:mi>&#x003b4;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>-</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mtext>&#x02009;tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>where tr(&#x000b7;), <italic>&#x003b1;</italic>, and <italic>&#x003b2;</italic> represent the trace of the matrix, the regularization term parameters, and the sparse constraint parameters, respectively. In the experiment, the basic matrix <bold>U</bold> and the coefficient matrix <bold>V</bold> are used for differential gene selection and cluster analysis, respectively.</p></sec><sec id="s3_2"><title>Optimization</title><p>Obviously, the loss function is a non-quadratic optimization problem, and finding the optimal solution is not simple. Fortunately, the semi-quadratic optimization technique that has been proposed can effectively optimize the loss function. The loss function can be reconstructed to find the optimal solution by introducing auxiliary variables. According to the conjugate function and the semi-quadratic technique (<xref rid="B32" ref-type="bibr">Nikolova and Chan, 2007</xref>), the fixed loss function <italic>&#x003c3;</italic>(<bold>Z</bold>) can be constructed as follows:</p><disp-formula><label>(8)</label><mml:math id="M18"><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Z</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x0211d;</mml:mi></mml:mrow></mml:munder><mml:mtext>&#x000a0;</mml:mtext><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Z</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>where <inline-formula><mml:math id="M19"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Z</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> represents the difference between the NMF predicted value and the actual value. <italic>&#x003c3;</italic>(&#x000b7;) indicates the noise or normal data, which is processed using different loss functions. <inline-formula><mml:math id="M20"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the introduced auxiliary variable. <italic>&#x003d5;</italic>(<bold>W</bold>
<italic><sub>ij</sub></italic>) is the conjugate function of <bold>Z</bold>
<italic><sub>ij</sub></italic>. <inline-formula><mml:math id="M21"><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x022c5;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x022c5;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a quadratic term for <bold>Z</bold>
<italic><sub>ij</sub></italic> and <bold>W</bold>
<italic><sub>ij</sub></italic>. The NMF model only needs to consider the quadratic term of the multiplication form:</p><disp-formula><label>(9)</label><mml:math id="M22"><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Z</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>Z</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>Combine Equation (8) and Equation (9) with the loss function (7):</p><disp-formula><label>(10)</label><mml:math id="M23"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mtext>&#x000a0;&#x000a0;</mml:mtext><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>-</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mphantom/><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mtext>&#x000a0;&#x000a0;</mml:mtext><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02297;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>-</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where &#x02297; represents the Hadamard product, which is the product between two matrices&#x02019; elements. Operator &#x02297; takes precedence over other matrices operators. Its Lagrangian function expansion is expressed as follows:</p><disp-formula><label>(11)</label><mml:math id="M24"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003c8;</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>and</p><disp-formula><label>(12)</label><mml:math id="M25"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>G</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003c8;</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003c6;</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><p>where <bold>Q</bold>
<italic><sub>i</sub></italic> and <bold>R</bold>
<italic><sub>j</sub></italic> are defined as <bold>Q</bold>
<italic><sub>i</sub></italic>=<italic>diag</italic>(<bold>W</bold>
<italic><sub>i</sub></italic>
<sub>*</sub>) and <bold>R</bold>=<italic>diag</italic>(<bold>W</bold>
<sub>*</sub>
<italic><sub>j</sub></italic>), respectively. <inline-formula><mml:math id="M26"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003c8;</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003c8;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M27"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>&#x003c6;</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003c6;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are Lagrangian multipliers of non-negative constraints <bold>U</bold> 0 and <bold>V</bold> 0, respectively. <bold>G</bold> is a diagonal matrix with diagonal elements, which is given by:</p><disp-formula><label>(13)</label><mml:math id="M28"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula><p>where <italic>&#x003c9;</italic> is a number that is very close but not equal to zero.</p><p>Let <bold>&#x003c8;U</bold>=0 and &#x003d5;<bold>V</bold>=0 by using Karush&#x02013;Kuhn&#x02013;Tucher (KKT) (<xref rid="B33" ref-type="bibr">Qi and Jiang, 1997</xref>) conditions. The loss function (10) can be iteratively optimized by the following schemes:</p><p>Update <bold>W</bold> when <bold>U</bold> and <bold>V</bold> are fixed. The weight matrix <bold>W</bold> according to equation (8) is defined as follows:</p><disp-formula><label>(14)</label><mml:math id="M29"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Z</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Z</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>where the elements of weight matrix is <italic>w</italic>
<italic><sub>ij</sub></italic>
<italic>&#x003f5;</italic>
<bold>W</bold> Combine the loss function (7) with the equation (14) are as follows:</p><disp-formula><label>(15)</label><mml:math id="M30"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mn>1</mml:mn><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mi>c</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula><p>Update <bold>U</bold> and <bold>V</bold> when <bold>W</bold> is fixed. The update rules for <bold>U</bold> and <bold>V</bold>are as follows:</p><disp-formula><label>(16)</label><mml:math id="M31"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02297;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02297;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula><label>(17)</label><mml:math id="M32"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>G</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02297;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02297;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>G</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The threshold parameter <italic>c</italic> is set to the median of the reconstruction error,</p><disp-formula><label>(18)</label><mml:math id="M33"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>The corresponding algorithm is shown in Algorithm 1.</p><table-wrap id="T5" position="float"><label>Algorithm 1</label><caption><p>Huber-SGNMF.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td valign="top" rowspan="1" colspan="1"/></tr><tr><td valign="top" rowspan="1" colspan="1">Data input: <inline-formula><mml:math id="M34"><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mstyle mathvariant="bold"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td valign="top" rowspan="1" colspan="1">Data output: <inline-formula><mml:math id="M35"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M36"><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and weight matrix <inline-formula><mml:math id="M37"><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td valign="top" rowspan="1" colspan="1">Parameters: <italic>&#x003b1;</italic>,<italic>&#x003b2;</italic>
</td></tr><tr><td valign="top" rowspan="1" colspan="1">Data initialize: <bold>U</bold>&#x02265;0, <bold>V</bold>&#x02265;0</td></tr><tr><td valign="top" rowspan="1" colspan="1">Repeat<break/>Update <bold>G</bold> by (13);<break/>Update <bold>W</bold> by (15);</td></tr><tr><td valign="top" rowspan="1" colspan="1">Update <bold>u</bold>
<italic><sub>ik</sub></italic> by (16);<break/>Update <bold>v</bold>
<italic><sub>jk</sub></italic> by (17);<break/>Update <italic>c</italic> by (18);</td></tr><tr><td valign="top" rowspan="1" colspan="1">End convergence</td></tr></tbody></table></table-wrap></sec><sec id="s3_3"><title>Convergence Analysis</title><p>According to the update rules of Huber-SGNMF, the loss function <italic>O</italic>
<italic><sub>Huber-SGNMF</sub></italic> can converge to the local optimum through theorem 1.</p><p>
<bold>Theorem 1.</bold> The loss function (7) is guaranteed to be non-increasing under the update rules (16) and (17). The loss function is constant when the elements <italic>u</italic>
<italic><sub>ik</sub></italic> and <italic>v</italic>
<italic><sub>kj</sub></italic> have fixed values.</p><p>To prove theorem 1, we introduce the auxiliary function <bold>H</bold> in Algorithm.</p><p>
<bold>Lemma 1.</bold> Suppose <bold>H</bold> (<italic>r</italic>, <italic>r</italic>&#x02032;) is an auxiliary function of F (<italic>r</italic>). If the conditions <bold>H</bold> (<italic>r</italic>,<italic>r</italic>&#x02032;) F(<italic>r</italic>) and <bold>H</bold> (<italic>r</italic>,<italic>r</italic>)=F(<italic>r</italic>) are satisfied, then it can be concluded that F(<italic>r</italic>) is non-increasing from iteration <italic>t</italic> to <italic>t</italic>+1:</p><disp-formula><label>(19)</label><mml:math id="M38"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mtext>&#x02003;</mml:mtext><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:munder><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><p>Proof:</p><disp-formula><label>(20)</label><mml:math id="M39"><mml:mrow><mml:mtext>F</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>F</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>Suppose loss function <italic>O</italic>
<italic><sub>Huber-SGNMF</sub></italic> has a suitable auxiliary function <bold>H</bold>
<italic><sub>Huber</sub></italic> If the minimum updates rule for <bold>H</bold>
<italic><sub>Huber</sub></italic> is equal to (16) and (17), then the convergence of <italic>O</italic>
<italic><sub>Huber-SGNMF</sub></italic> can be proved. Furthermore, the parts of the loss function <italic>O</italic>
<italic><sub>Huber-SGNMF</sub></italic> associated with the elements <italic>u</italic>
<italic><sub>ik</sub></italic>
<italic><sub>&#x003f5;</sub></italic>
<bold>U</bold> and <italic>v</italic>
<italic><sub>kj</sub></italic>
<italic><sub>&#x003f5;</sub></italic>
<bold>V</bold> are represented by F<italic><sub>ik</sub></italic> and F<italic><sub>kj</sub></italic>, respectively. The partial derivative equation of <italic>O</italic>
<italic><sub>Huber-SGNMF</sub></italic> can be derived as follows:</p><disp-formula><label>(21)</label><mml:math id="M40"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mo>&#x02032;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(22)</label><mml:math id="M41"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi mathvariant="italic">ik</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo>&#x02202;</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(23)</label><mml:math id="M42"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi mathvariant="italic">kj</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>G</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula><label>(24)</label><mml:math id="M43"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi mathvariant="italic">kj</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo>&#x02202;</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>Essentially, the algorithm updates each element, which means that if the elements F<italic><sub>ik</sub></italic> and F<italic><sub>kj</sub></italic> are non-increasing, then <italic>O</italic>
<italic><sub>Huber-SGNMF</sub></italic> is also non-increasing.</p><p>
<bold>Lemma 2.</bold> Define <inline-formula><mml:math id="M44"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M45"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as auxiliary functions for <bold>u</bold>
<italic><sub>ik</sub></italic> and <bold>v</bold>
<italic><sub>kj</sub></italic>, respectively. The expansion items are as follows:</p><disp-formula><label>(25)</label><mml:math id="M46"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi mathvariant="italic">ik</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula><label>(26)</label><mml:math id="M47"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi mathvariant="italic">kj</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>G</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Proof:</p><p>According to the lemma 1, <bold>H</bold>
<italic><sub>Huber</sub></italic> (<italic>u,u</italic>)<italic>=</italic>F<italic><sub>ik</sub></italic>(<italic>u</italic>) and <bold>H</bold>
<italic><sub>Huber</sub></italic> (<italic>v,v</italic>)=F<italic><sub>kj</sub></italic>(<italic>v</italic>) can be obtained. We have the following formulas through the Taylor series expansion of the auxiliary function.</p><disp-formula><label>(27)</label><mml:math id="M48"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mtext>F</mml:mtext><mml:mi mathvariant="italic">ik</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mtext>F</mml:mtext><mml:mi mathvariant="italic">ik</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula><label>(28)</label><mml:math id="M49"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mtext>F</mml:mtext><mml:mi mathvariant="italic">kj</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mtext>F</mml:mtext><mml:mi mathvariant="italic">kj</mml:mi><mml:mo>&#x02033;</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Next, <inline-formula><mml:math id="M50"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M51"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> need to be guaranteed.</p><p>According to (25) and (27), expand <inline-formula><mml:math id="M52"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is as follows:</p><disp-formula><label>(29)</label><mml:math id="M53"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>&#x02265;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>since</p><disp-formula><label>(30)</label><mml:math id="M54"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>Q</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>According to (26) and (28), expand <inline-formula><mml:math id="M55"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is as follows:</p><disp-formula><label>(31)</label><mml:math id="M56"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>G</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>since</p><disp-formula><label>(32)</label><mml:math id="M57"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(33)</label><mml:math id="M58"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>G</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi></mml:mstyle><mml:mrow><mml:mi>b</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>and</p><disp-formula><label>(34)</label><mml:math id="M59"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>V</mml:mi><mml:mi>D</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>&#x02265;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;</mml:mtext><mml:mo>&#x02265;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>D</mml:mi></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>L</mml:mi></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>So, <inline-formula><mml:math id="M60"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M61"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi mathvariant="normal">F</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained. In other words, the auxiliary functions F<italic><sub>ik</sub></italic> (<italic>u</italic>) and F<italic><sub>kj</sub></italic> (<italic>v</italic>) of the updated rules (16) and (17) are non-increasing, and the derivation of theorem 1 is completed. Finally, the convergence of the loss function <italic>O</italic>
<italic><sub>Huber-SGNMF</sub></italic> is proved.</p><p>The corresponding convergence analysis curve is shown in <xref ref-type="fig" rid="f2">
<bold>Figure 2</bold>
</xref>.</p><fig id="f2" position="float"><label>Figure 2</label><caption><p>Convergence analysis curve of Huber-SGNMF model. Each curve represents a dataset. PHD and PHDEC are the datasets used in the experiment.</p></caption><graphic xlink:href="fgene-10-01054-g002"/></fig></sec></sec><sec id="s4"><title>Results and Discussion</title><sec id="s4_1"><title>Datasets</title><p>Five gene expression datasets downloaded from TCGA are used in the experiment. TCGA is a gene data sharing system that contains information on thousands of cancer patients and has made great contributions to the path of human exploration of cancer genomics. The experiment used five datasets including cholangiocarcinoma (CHOL), colon adenocarcinoma (COAD), head and neck squamous cell carcinoma (HNSC), pancreatic cancer (PAAD), and esophageal cancer (ESCA).</p><p>To explore the association between genes and multiple cancers, diseased samples from multiple datasets are integrated into one dataset. In detail, the detesteds PAAD, HNSC, and COAD are integrated into one dataset, which is represented as PHD. The detesteds PAAD, HNSC, and COAD are integrated into one dataset, which is represented as PHD. These two integrated datasets contain only diseased samples of different diseases. Datasets are standardized before using, and the data normalization scales data to specific time intervals. Pre-processing data speeds up searching for the best solution and optimizes convergence speed. Since high-dimensional gene expression data contains a large amount of redundant information, PCA (<xref rid="B40" ref-type="bibr">Wu et al., 2017</xref>) is used to reduce the dimensions to 2,000 genes in the pre-processing.</p></sec><sec id="s4_2"><title>Model Robustness</title><p>To analyze the robustness of RGNMF, CGNMF, and Huber-SGNMF, we apply these methods to a composite dataset consisting of 200 two-dimensional data points (<xref ref-type="fig" rid="f3">
<bold>Figure 3A</bold>
</xref>). All data points are distributed in one dimensional space. In <xref ref-type="fig" rid="f3">
<bold>Figure 3A</bold>
</xref>, there is only one contaminated point, and each model can restore the original data normally. The contaminated points in <xref ref-type="fig" rid="f3">
<bold>Figures 3B&#x02013;D</bold>
</xref> are 50 points, 100 points, and 150 points, respectively. In the case where a part of the data is contaminated, only Huber-SGNMF successfully restores the original data. CGNMF and RGNMF are affected by some noise or outliers when restoring data, while NMF is most affected by noise or outliers.</p><fig id="f3" position="float"><label>Figure 3</label><caption><p>In the case of different data points are contaminated, NMF, RGNMF, CGNMF, and Huber-SGNMF restore 200 synthetic two-dimensional data points: <bold>(A)</bold> the data contains 1 noise or outlier, <bold>(B)</bold> the data contains 50 noise or outliers, <bold>(C)</bold> the data contains 100 noise or outliers, <bold>(D)</bold> the data contains 150 noise or outliers.</p></caption><graphic xlink:href="fgene-10-01054-g003"/></fig></sec><sec id="s4_3"><title>Parameter Selection</title><p>In the experiment, we consider the effect of each parameter on the solution model. A grid search method is used to find the optimal parameters of the model. The grid search range is [10<sup>-2</sup>&#x0223c;10<sup>2</sup>]. As shown in <xref ref-type="fig" rid="f4">
<bold>Figure 4</bold>
</xref>, the PHD dataset is used as an example to find the optimal parameters of the Huber-SGNMF model. Specifically, the two datasets are set to the same parameters <italic>&#x003b1;</italic> = 100 and <italic>&#x003b2;</italic> = 0.01 Other methods in the experiment are set up with prior parameters or grid searches to find the optimal parameters.</p><fig id="f4" position="float"><label>Figure 4</label><caption><p>Optimal parameter selection for the Huber-SGNMF model on the PHD dataset. Huber-SGNMF is set with parameters <italic>&#x003b1;</italic> = 100 and <italic>&#x003b2;</italic> = 0.01.</p></caption><graphic xlink:href="fgene-10-01054-g004"/></fig></sec><sec id="s4_4"><title>Performance Evaluation and Comparisons</title><p>To prove the validity of the performance of the model, six states of the art methods including RPCA (<xref rid="B24" ref-type="bibr">Liu et al., 2013</xref>), NMF (<xref rid="B20" ref-type="bibr">Lee and Seung, 1999</xref>), SNMF (<xref rid="B17" ref-type="bibr">Kim and Park, 2007</xref>), GNMF (<xref rid="B3" ref-type="bibr">Cai et al., 2011</xref>), RGNMF (<xref rid="B36" ref-type="bibr">Wang et al., 2016a</xref>), CGNMF (<xref rid="B28" ref-type="bibr">Mao et al., 2014</xref>), and Huber-NMF (<xref rid="B11" ref-type="bibr">Du et al., 2012</xref>) are compared with Huber-SGNMF. In the experiment, the basic matrix and the coefficient matrix are used to differentially gene selection and cluster analysis, respectively.</p><sec id="s4_4_1"><title>Feature Selection Results and Analysis</title><p>Feature selection is the selection of representative features from multiple feature values (<xref rid="B43" ref-type="bibr">Yu and Liu, 2003</xref>). In the analysis of cancer data, the feature selection is to find differentially expressed genes for cancer (that is, pathogenic genes). This is of great significance in exploring the link between cancer and genes (<xref rid="B7" ref-type="bibr">Chen et al., 2017</xref>). For each method, the top 500 genes with the greatest differential expression are analyzed.</p><p>The GeneCards (<uri xlink:type="simple" xlink:href="https://www.genecards.org/">https://www.genecards.org/</uri>) system is used to download all gene libraries associated with the disease. The selected genes are compared with the gene bank to select overlapping genes and obtain a corresponding relevance score. The relevance score is the indicator that GeneCards assesses the association between the gene and the disease. The higher the relevance score is, the greater the intimacy of the gene and the disease. The average relevance score (ARS) and the maximum relevance score (MRS) are used to evaluate the performance of the model.</p><p>The specific experimental results of the seven methods are listed in <xref rid="T1" ref-type="table">
<bold>Table 1</bold>
</xref>. The results show that the genes selected by Huber-SGNMF model have higher ARS and MRS. This means that the model can effectively find the genes associated with cancer. <xref rid="T2" ref-type="table">
<bold>Table 2</bold>
</xref> lists the genes for the top 10 largest relevance scores selected by the Huber-SGNMF model on the PHD dataset. The detailed genetic analysis is as follows. </p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Relevance scores for seven methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" rowspan="1" colspan="1"/><th valign="top" rowspan="1" colspan="1">NMF</th><th valign="top" rowspan="1" colspan="1">SNMF</th><th valign="top" rowspan="1" colspan="1">GNMF</th><th valign="top" rowspan="1" colspan="1">RGNMF</th><th valign="top" rowspan="1" colspan="1">RPCA</th><th valign="top" rowspan="1" colspan="1">CGNMF</th><th valign="top" rowspan="1" colspan="1">Huber-NMF</th><th valign="top" rowspan="1" colspan="1">Huber-SGNMF</th><th valign="top" rowspan="1" colspan="1"/></tr></thead><tbody><tr><td valign="top" rowspan="1" colspan="1">PHD</td><td valign="top" rowspan="1" colspan="1">MRS</td><td valign="top" rowspan="1" colspan="1">116.4</td><td valign="top" rowspan="1" colspan="1">113.99</td><td valign="top" rowspan="1" colspan="1">116.4</td><td valign="top" rowspan="1" colspan="1">164.03</td><td valign="top" rowspan="1" colspan="1">
<bold>194.01</bold>
</td><td valign="top" rowspan="1" colspan="1">113.99</td><td valign="top" rowspan="1" colspan="1">164.03</td><td valign="top" rowspan="1" colspan="1">164.03</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">ARS</td><td valign="top" rowspan="1" colspan="1">22.64</td><td valign="top" rowspan="1" colspan="1">21.75</td><td valign="top" rowspan="1" colspan="1">22.03</td><td valign="top" rowspan="1" colspan="1">22.16</td><td valign="top" rowspan="1" colspan="1">26.03</td><td valign="top" rowspan="1" colspan="1">21.75</td><td valign="top" rowspan="1" colspan="1">25.56</td><td valign="top" rowspan="1" colspan="1">
<bold>27.19</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1">PHDEC</td><td valign="top" rowspan="1" colspan="1">MRS</td><td valign="top" rowspan="1" colspan="1">92.51</td><td valign="top" rowspan="1" colspan="1">96.36</td><td valign="top" rowspan="1" colspan="1">153.66</td><td valign="top" rowspan="1" colspan="1">124.37</td><td valign="top" rowspan="1" colspan="1">
<bold>172.9</bold>
</td><td valign="top" rowspan="1" colspan="1">164.91</td><td valign="top" rowspan="1" colspan="1">145</td><td valign="top" rowspan="1" colspan="1">
<bold>172.9</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">ARS</td><td valign="top" rowspan="1" colspan="1">29.18</td><td valign="top" rowspan="1" colspan="1">30.05</td><td valign="top" rowspan="1" colspan="1">36.58</td><td valign="top" rowspan="1" colspan="1">27.87</td><td valign="top" rowspan="1" colspan="1">37.83</td><td valign="top" rowspan="1" colspan="1">35.07</td><td valign="top" rowspan="1" colspan="1">35.93</td><td valign="top" rowspan="1" colspan="1">
<bold>44.97</bold>
</td></tr></tbody></table><table-wrap-foot><p>Bolded texts denoted best experimental results.</p></table-wrap-foot></table-wrap><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Detailed analysis of the differentially expressed genes in PHD dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" rowspan="1" colspan="1">Gene name</th><th valign="top" rowspan="1" colspan="1">Relevance score</th><th valign="top" rowspan="1" colspan="1">Gene official name</th><th valign="top" rowspan="1" colspan="1">Related diseases</th></tr></thead><tbody><tr><td valign="top" rowspan="1" colspan="1">CTNNB1</td><td valign="top" rowspan="1" colspan="1">164.03</td><td valign="top" rowspan="1" colspan="1">Catenin beta 1</td><td valign="top" rowspan="1" colspan="1">Colorectal cancer and pilomatrixoma</td></tr><tr><td valign="top" rowspan="1" colspan="1">ERBB2</td><td valign="top" rowspan="1" colspan="1">152.33</td><td valign="top" rowspan="1" colspan="1">Erb-B2 receptor tyrosine kinase 2</td><td valign="top" rowspan="1" colspan="1">Lung cancer and ovary adenocarcinoma</td></tr><tr><td valign="top" rowspan="1" colspan="1">CDH1</td><td valign="top" rowspan="1" colspan="1">149.92</td><td valign="top" rowspan="1" colspan="1">Cadherin 1</td><td valign="top" rowspan="1" colspan="1">Gastric cancer and breast cancer</td></tr><tr><td valign="top" rowspan="1" colspan="1">TGFBR2</td><td valign="top" rowspan="1" colspan="1">102.74</td><td valign="top" rowspan="1" colspan="1">Transforming growth factor beta receptor 2</td><td valign="top" rowspan="1" colspan="1">Colorectal cancer and esophageal cancer</td></tr><tr><td valign="top" rowspan="1" colspan="1">CDK4</td><td valign="top" rowspan="1" colspan="1">93.35</td><td valign="top" rowspan="1" colspan="1">Cyclin dependent kinase 4</td><td valign="top" rowspan="1" colspan="1">Myeloma and melanoma</td></tr><tr><td valign="top" rowspan="1" colspan="1">EPCAM</td><td valign="top" rowspan="1" colspan="1">86.79</td><td valign="top" rowspan="1" colspan="1">Epithelial cell adhesion molecule</td><td valign="top" rowspan="1" colspan="1">Pancreatic cancer and gastrointestinal carcinoma</td></tr><tr><td valign="top" rowspan="1" colspan="1">GNAS</td><td valign="top" rowspan="1" colspan="1">76.17</td><td valign="top" rowspan="1" colspan="1">GNAS complex locus</td><td valign="top" rowspan="1" colspan="1">Osseous heteroplasia</td></tr><tr><td valign="top" rowspan="1" colspan="1">ERBB3</td><td valign="top" rowspan="1" colspan="1">74.35</td><td valign="top" rowspan="1" colspan="1">Erb-B2 receptor tyrosine kinase 3</td><td valign="top" rowspan="1" colspan="1">Transitional cell carcinoma</td></tr><tr><td valign="top" rowspan="1" colspan="1">CEACAM5</td><td valign="top" rowspan="1" colspan="1">59.9</td><td valign="top" rowspan="1" colspan="1">Carcinoembryonic antigen related cell Adhesion molecule 5</td><td valign="top" rowspan="1" colspan="1">Colorectal cancer and lung cancer</td></tr><tr><td valign="top" rowspan="1" colspan="1">MAP2K2</td><td valign="top" rowspan="1" colspan="1">51.51</td><td valign="top" rowspan="1" colspan="1">Mitogen-activated protein kinase kinase 2</td><td valign="top" rowspan="1" colspan="1">Head and neck squamous cell carcinoma</td></tr></tbody></table></table-wrap><p>CTNNB1 is a protein-coding gene from which the protein encoded by the gene forms part of an adhesion-linked protein complex. Mutations in the CTNNB1 proto-oncogene are associated with most human colorectal epithelial tumors, and a significant increase in expression in the same tumor may indirectly or directly lead to intestinal adenocarcinoma (<xref rid="B37" ref-type="bibr">Wang et al., 2011</xref>). Moreover, deep sequencing of patients with pancreatic ductal adenocarcinoma also found CTNNB1 mutations (<xref rid="B15" ref-type="bibr">Honda et al., 2013</xref>; <xref rid="B16" ref-type="bibr">Javadinia et al., 2019</xref>). Multiple studies have shown that CTNNB1 mutation analysis is important for PAAD and COAD (<xref rid="B19" ref-type="bibr">Kubota et al., 2015</xref>).</p><p>ERBB2, commonly referred to as HER2, may be critical for enhancing the synergistic effect of PI3K inhibitors in HNSC patients (<xref rid="B30" ref-type="bibr">Michmerhuizen et al., 2019</xref>). It is generally believed that dysregulated ERBB2 signaling plays a key role in the development of pancreatic cancer (<xref rid="B22" ref-type="bibr">Lin et al., 2019</xref>). For the treatment of intestinal adenocarcinoma, ERBB2 mutations and amplification in small intestinal adenocarcinoma patients also make a great contribution (<xref rid="B1" ref-type="bibr">Adam et al., 2019</xref>). Recent studies have shown that HER2 targeted therapy has significantly improved outcomes in patients with breast and stomach problems with ERBB2 mutation/amplification (<xref rid="B29" ref-type="bibr">Meric-Bernstam et al., 2019</xref>).</p><p>The CDH1 gene plays a regulatory role in cell growth (<xref rid="B31" ref-type="bibr">Nagai et al., 2018</xref>), and the CDH1 gene located on chromosome 16q22.1 is considered to be a tumor suppressor of diffuse gastric cancer. By measuring the methylation profile of gastric cancer and breast cancer patients, it is found that CDH1 is closely related to low protein expression (<xref rid="B38" ref-type="bibr">Wang et al., 2016b</xref>; <xref rid="B39" ref-type="bibr">Wang et al., 2016c</xref>). Studies have shown that abnormal expression of CDH1 gene leads to uncontrolled growth of tumor cells (<xref rid="B9" ref-type="bibr">Dial et al., 2007</xref>; <xref rid="B6" ref-type="bibr">Chen et al., 2012</xref>).</p><p>The above experimental results show that Huber-SGNMF model can find pathogenic genes more effectively. Although some genes have not been confirmed, they may be a key part of solving cancer problems in the future.</p></sec><sec id="s4_4_2"><title>Clustering Results and Analysis</title><p>After the Huber-SGNMF model reduces the dimensions of the data, the coefficient matrix is used for k-means clustering. Sample clustering is a common analytical method for cancer diagnosis and molecular subtype discrimination (<xref rid="B41" ref-type="bibr">Xu et al., 2019</xref>). Moreover, multiple evaluation criteria accuracy (ACC), recall (R), precision (P), and F-measure (F) are adopted to judge the model to be feasible and effective. ACC is an evaluation standard that can visually reflect the clustering of samples. It is defined as follows:</p><disp-formula><label>(35)</label><mml:math id="M62"><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext>map</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>Where <italic>&#x003b4;</italic> (&#x02022;) and map (&#x02022;) represent function permutation and delta mapping function, respectively. The actual sample label, the predicted sample label, and the total number of samples are denoted by <italic>a, b</italic> and <italic>n</italic>, respectively.</p><p>Considering clustering accuracy alone does not fully demonstrate clustering performance, and more evaluation criteria need to be introduced. The clustering results can be divided into true positive (TP), true negative (TN), false positive (FP) cases, and false negative (FN) according to real and predictive labels. These four measures are listed in <xref rid="T3" ref-type="table">
<bold>Table 3</bold>
</xref>. The detailed evaluation criteria are as follows.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>Clustering result confusion matrix.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" rowspan="2" colspan="1">The true situation</th><th valign="top" colspan="2" rowspan="1">Clustering result</th></tr><tr><th valign="top" rowspan="1" colspan="1">Positive</th><th valign="top" rowspan="1" colspan="1">Negative</th></tr></thead><tbody><tr><td valign="top" rowspan="1" colspan="1">Positive</td><td valign="top" rowspan="1" colspan="1">TP (true positive)</td><td valign="top" rowspan="1" colspan="1">FN (false negative)</td></tr><tr><td valign="top" rowspan="1" colspan="1">Negative</td><td valign="top" rowspan="1" colspan="1">FP (false positive)</td><td valign="top" rowspan="1" colspan="1">TN (true negative)</td></tr></tbody></table></table-wrap><disp-formula><label>(36)</label><mml:math id="M63"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(37)</label><mml:math id="M64"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(38)</label><mml:math id="M65"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>Since <italic>R, P,</italic> and <italic>F</italic> can only reflect the clustering performance of a certain sample categories, for multi-category problems, the average of each category of indicators is usually used as the evaluation criterions:</p><disp-formula><label>(39)</label><mml:math id="M66"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(40)</label><mml:math id="M67"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula><label>(41)</label><mml:math id="M68"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>where <italic>n</italic> represents the number of sample categories.</p><p>According to the above evaluation criterions, each algorithm is performed 50 times to get an average result. Since the initialization matrix is random, the average value can reduce the chance of the algorithm. <xref rid="T4" ref-type="table">
<bold>Table 4</bold>
</xref> lists the comparative experiments of seven methods based on four evaluation criterions. Compared with the other six methods, our proposed model has the excellent clustering performance under the four evaluation criterions. The specific analysis of the clustering results is as follows:</p><list list-type="order"><list-item><p>Since the squared loss of the original NMF is sensitive to noise and outliers, the squared loss is replaced by Huber loss to improve the robustness of the algorithm. The experimental results show that the clustering performance of RPCA, CGNMF, RGNMF, Huber-NMF, and Huber-SGNMF is higher than standard NMF and GNMF. The reason is that both NMF and GNMF use square loss while other methods use more robust loss function. Moreover, the experimental results show that the robustness of the Huber loss model is higher than the <italic>L</italic>
<sub>2,1</sub> -norm loss and correntropy loss. The RPCA model has higher performance as a state-of-the-art algorithm and is still lower than Huber-SGNMF. The Huber loss use <italic>L</italic>
<sub>1</sub> -norm or <italic>L</italic>
<sub>2</sub> -norm to different data, which can effectively reduce the influence of noise and outliers and enhance the robustness of the algorithm. Compared with NMF, the clustering accuracy of Huber-SGNMF model on the two datasets increased by 4.90 and 5.68%, respectively.</p></list-item><list-item><p>Assuming that data points are related in a high-dimensional state, they should also be relevant in low-dimensional representations. However, the association between data points is difficult to preserve when the data is mapped to low-dimensions. The manifold structure preserves the spatial structure of high-dimensional data in low-dimensional representations, enhancing the correlation between data points. Constructing a sample association graph of gene expression data to preserve the relationship between the samples. The experimental results of several models (NMF and GNMF, Huber-NMF, and Huber-SGNMF) show that the clustering performance of the model with the addition of graph regularity constraints is improved. Compared with Huber-NMF, Huber-SGNMF has improved clustering accuracy by 1.73 and 2.99% in the two datasets, respectively.</p></list-item><list-item><p>Matrix sparseness removes redundant data and simplifies model calculations. The sparsity constraint of the coefficient matrix removes redundant features and improves clustering performance. The experimental results of SNMF and Huber-SGNMF prove this. Compared with SNMF, since Huber-SGNMF improves the loss function and manifold structure, the clustering accuracy in the two datasets is increased by 1.35 and 4.02%, respectively.</p></list-item></list><table-wrap id="T4" position="float"><label>Table 4</label><caption><p>Clustering effect for seven methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" rowspan="1" colspan="1">Dataset</th><th valign="top" rowspan="1" colspan="1">Evaluation</th><th valign="top" rowspan="1" colspan="1">NMF</th><th valign="top" rowspan="1" colspan="1">SNMF</th><th valign="top" rowspan="1" colspan="1">GNMF</th><th valign="top" rowspan="1" colspan="1">RGNMF</th><th valign="top" rowspan="1" colspan="1">RPCA</th><th valign="top" rowspan="1" colspan="1">CGNMF</th><th valign="top" rowspan="1" colspan="1">Huber-NMF</th><th valign="top" rowspan="1" colspan="1">Huber-SGNMF</th></tr></thead><tbody><tr><td valign="top" rowspan="1" colspan="1">PHD</td><td valign="top" rowspan="1" colspan="1">ACC (%)</td><td valign="top" rowspan="1" colspan="1">85.38 &#x000b1; 1.24</td><td valign="top" rowspan="1" colspan="1">88.93 &#x000b1; 0.58</td><td valign="top" rowspan="1" colspan="1">86.05 &#x000b1; 1.97</td><td valign="top" rowspan="1" colspan="1">86.50 &#x000b1; 1.84</td><td valign="top" rowspan="1" colspan="1">86.37 &#x000b1; 2.04</td><td valign="top" rowspan="1" colspan="1">87.18 &#x000b1; 1.43</td><td valign="top" rowspan="1" colspan="1">88.55 &#x000b1; 0.98</td><td valign="top" rowspan="1" colspan="1">
<bold>90.36 &#x000b1; 0.91</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">Macro-R (%)</td><td valign="top" rowspan="1" colspan="1">82.99 &#x000b1; 1.57</td><td valign="top" rowspan="1" colspan="1">86.86 &#x000b1; 0.82</td><td valign="top" rowspan="1" colspan="1">81.02 &#x000b1; 1.09</td><td valign="top" rowspan="1" colspan="1">84.28 &#x000b1; 2.40</td><td valign="top" rowspan="1" colspan="1">84.10 &#x000b1; 2.79</td><td valign="top" rowspan="1" colspan="1">85.00 &#x000b1; 1.79</td><td valign="top" rowspan="1" colspan="1">86.41 &#x000b1; 1.27</td><td valign="top" rowspan="1" colspan="1">
<bold>88.50 &#x000b1; 1.19</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">Macro-P (%)</td><td valign="top" rowspan="1" colspan="1">84.88 &#x000b1; 1.74</td><td valign="top" rowspan="1" colspan="1">89.08 &#x000b1; 0.86</td><td valign="top" rowspan="1" colspan="1">83.55 &#x000b1; 3.76</td><td valign="top" rowspan="1" colspan="1">85.68 &#x000b1; 2.74</td><td valign="top" rowspan="1" colspan="1">85.58 &#x000b1; 2.96</td><td valign="top" rowspan="1" colspan="1">86.77 &#x000b1; 2.02</td><td valign="top" rowspan="1" colspan="1">88.32 &#x000b1; 1.36</td><td valign="top" rowspan="1" colspan="1">
<bold>90.18 &#x000b1; 1.28</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">Macro-F (%)</td><td valign="top" rowspan="1" colspan="1">83.92 &#x000b1; 1.65</td><td valign="top" rowspan="1" colspan="1">87.95 &#x000b1; 0.84</td><td valign="top" rowspan="1" colspan="1">82.25 &#x000b1; 3.51</td><td valign="top" rowspan="1" colspan="1">84.92 &#x000b1; 2.60</td><td valign="top" rowspan="1" colspan="1">84.83 &#x000b1; 2.88</td><td valign="top" rowspan="1" colspan="1">85.87 &#x000b1; 1.90</td><td valign="top" rowspan="1" colspan="1">87.35 &#x000b1; 1.31</td><td valign="top" rowspan="1" colspan="1">
<bold>89.33 &#x000b1; 1.23</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1">PHDEC</td><td valign="top" rowspan="1" colspan="1">ACC (%)</td><td valign="top" rowspan="1" colspan="1">69.84 &#x000b1; 0.26</td><td valign="top" rowspan="1" colspan="1">71.51 &#x000b1; 0.31</td><td valign="top" rowspan="1" colspan="1">70.15 &#x000b1; 0.08</td><td valign="top" rowspan="1" colspan="1">71.86 &#x000b1; 0.69</td><td valign="top" rowspan="1" colspan="1">75.02 &#x000b1; 0.32</td><td valign="top" rowspan="1" colspan="1">73.81 &#x000b1; 0.27</td><td valign="top" rowspan="1" colspan="1">72.53 &#x000b1; 0.21</td><td valign="top" rowspan="1" colspan="1">
<bold>75.52 &#x000b1; 0.20</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">Macro-R (%)</td><td valign="top" rowspan="1" colspan="1">63.95 &#x000b1; 0.18</td><td valign="top" rowspan="1" colspan="1">65.33 &#x000b1; 0.14</td><td valign="top" rowspan="1" colspan="1">61.98 &#x000b1; 0.38</td><td valign="top" rowspan="1" colspan="1">64.45 &#x000b1; 0.87</td><td valign="top" rowspan="1" colspan="1">68.37 &#x000b1; 0.28</td><td valign="top" rowspan="1" colspan="1">66.74 &#x000b1; 0.15</td><td valign="top" rowspan="1" colspan="1">67.09 &#x000b1; 0.07</td><td valign="top" rowspan="1" colspan="1">
<bold>69.02 &#x000b1; 0.07</bold>
</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">Macro-P (%)</td><td valign="top" rowspan="1" colspan="1">61.34 &#x000b1; 0.26</td><td valign="top" rowspan="1" colspan="1">62.45 &#x000b1; 0.19</td><td valign="top" rowspan="1" colspan="1">58.77 &#x000b1; 0.10</td><td valign="top" rowspan="1" colspan="1">62.80 &#x000b1; 0.97</td><td valign="top" rowspan="1" colspan="1">
<bold>65.81 &#x000b1; 0.50</bold>
</td><td valign="top" rowspan="1" colspan="1">64.47 &#x000b1; 0.27</td><td valign="top" rowspan="1" colspan="1">63.92 &#x000b1; 0.25</td><td valign="top" rowspan="1" colspan="1">65.56 &#x000b1; 0.25</td></tr><tr><td valign="top" rowspan="1" colspan="1"/><td valign="top" rowspan="1" colspan="1">Macro-F (%)</td><td valign="top" rowspan="1" colspan="1">64.17 &#x000b1; 0.20</td><td valign="top" rowspan="1" colspan="1">63.79 &#x000b1; 0.21</td><td valign="top" rowspan="1" colspan="1">60.24 &#x000b1; 0.27</td><td valign="top" rowspan="1" colspan="1">63.49 &#x000b1; 0.87</td><td valign="top" rowspan="1" colspan="1">66.92 &#x000b1; 0.29</td><td valign="top" rowspan="1" colspan="1">65.51 &#x000b1; 0.17</td><td valign="top" rowspan="1" colspan="1">65.34 &#x000b1; 0.12</td><td valign="top" rowspan="1" colspan="1">
<bold>67.17 &#x000b1; 0.10</bold>
</td></tr></tbody></table><table-wrap-foot><p>Bolded texts denoted best experimental results.</p></table-wrap-foot></table-wrap><p>In summary, the experimental results based on the four evaluation indicators demonstrate the excellent clustering performance of the Huber-SGNMF model. Compared with NMF, the clustering performance of Huber-SGNMF has improved 5.30 and 4.49% on average in PHD dataset and PHDEC dataset, respectively. Huber-SGNMF clustering performance improves 1.93 and 2.07% on average compared to Huber-NMF. The above experimental results strongly prove the effectiveness of Huber-SGNMF in clustering performance.</p></sec></sec></sec><sec sec-type="conclusions" id="s5"><title>Conclusion</title><p>In this paper, we propose a novel model based on Huber loss: Huber-SGNMF, which is dedicated to samples clustering and differentially expressed gene selection. On the one hand, the squared loss is replaced by Huber loss to enhance algorithm robustness. On the other hand, sparse penalty and graph regularization terms are added to the model to enhance the sparsity of the matrix and preserve data geometry information. Numerous experimental results confirm that the Huber-SGNMF method is more effective. In the future work, we will actively explore more effective constraints based on the traditional NMF method to improve the robustness and sparsity of the method.</p></sec><sec sec-type="data-availability" id="s6"><title>Data Availability Statement</title><p>The datasets for this study can be downloaded in the The Cancer Genome Atlas [<uri xlink:type="simple" xlink:href="https://cancergenome.nih.gov/">https://cancergenome.nih.gov/</uri>]. </p></sec><sec id="s7"><title>Author Contributions</title><p>C-YW and NY proposed and designed the algorithm. J-XL demonstrated the robustness of the algorithm and analyzed the experimental data. C-YW and C-HZ drafted the manuscript. </p></sec><sec id="s8"><title>Conflict of Interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><title>Acknowledgments</title><p>This work was supported in part by the grants provided by the National Science Foundation of China, Nos. 61872220, 61873001, and 61572284.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Adam</surname><given-names>L.</given-names></name><name><surname>San Lucas</surname><given-names>F. A.</given-names></name><name><surname>Fowler</surname><given-names>R.</given-names></name><name><surname>Yu</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>W.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>DNA sequencing of small bowel adenocarcinomas identifies targetable recurrent mutations in the ERBB2 signaling pathway</article-title>. <source>Clin. Cancer Res.</source>
<volume>25</volume> (<issue>2</issue>), <fpage>641</fpage>&#x02013;<lpage>651</lpage>. <pub-id pub-id-type="doi">10.1158/1078-0432.CCR-18-1480</pub-id>
<pub-id pub-id-type="pmid">30352910</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Belkin</surname><given-names>M.</given-names></name><name><surname>Niyogi</surname><given-names>P.</given-names></name></person-group> (<year>2001</year>). &#x0201c;<article-title>Laplacian eigenmaps and spectral techniques for embedding and clustering</article-title>,&#x0201d; in <source>Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic</source>. (<publisher-name>MIT Press</publisher-name>).</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Cai</surname><given-names>D.</given-names></name><name><surname>He</surname><given-names>X.</given-names></name><name><surname>Han</surname><given-names>J.</given-names></name><name><surname>Huang</surname><given-names>T. S.</given-names></name></person-group> (<year>2011</year>). <article-title>Graph regularized non-negative matrix factorization for data representation</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>33</volume> (<issue>8</issue>), <fpage>1548</fpage>&#x02013;<lpage>1560</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2010.231</pub-id>
<pub-id pub-id-type="pmid">21173440</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Han</surname><given-names>G.</given-names></name><name><surname>Xu</surname><given-names>A.</given-names></name><name><surname>Cai</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <article-title>Identification of multidimensional regulatory modules through multi-graph matching with network constraints</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <pub-id pub-id-type="doi">10.1109/TBME.2019.2927157</pub-id>
</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Han</surname><given-names>G.</given-names></name><name><surname>Cai</surname><given-names>H.</given-names></name><name><surname>Cai</surname><given-names>J.</given-names></name></person-group> (<year>2018</year>). <article-title>HOGMMNC: a higher order graph matching with multiple network constraints model for gene-drug regulatory modules identification</article-title>. <source>Bioinformatics</source>
<volume>35</volume> (<issue>4</issue>), <fpage>602</fpage>&#x02013;<lpage>610</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty662</pub-id>
</mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>M.</given-names></name><name><surname>Gutierrez</surname><given-names>G. J.</given-names></name><name><surname>Ronai</surname><given-names>Z. A.</given-names></name></person-group> (<year>2012</year>). <article-title>The anaphase-promoting complex or cyclosome supports cell survival in response to endoplasmic reticulum stress</article-title>. <source>PloS One</source>
<volume>7</volume> (<issue>4</issue>), <elocation-id>e35520</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0035520</pub-id>
<pub-id pub-id-type="pmid">22539978</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Feng</surname><given-names>M.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name><name><surname>He</surname><given-names>B.</given-names></name><name><surname>Du</surname><given-names>X.</given-names></name><name><surname>Yu</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>A novel approach to select differential pathways associated with hypertrophic cardiomyopathy based on gene coexpression analysis</article-title>. <source>Mol. Med. Rep.</source>
<volume>16</volume> (<issue>1</issue>), <fpage>773</fpage>&#x02013;<lpage>777</lpage>. <pub-id pub-id-type="doi">10.3892/mmr.2017.6667</pub-id>
<pub-id pub-id-type="pmid">28586052</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Chreiky</surname><given-names>R.</given-names></name><name><surname>Delmaire</surname><given-names>G.</given-names></name><name><surname>Puigt</surname><given-names>M.</given-names></name><name><surname>Roussel</surname><given-names>G.</given-names></name><name><surname>Abche</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). &#x0201c;<article-title>Informed split gradient non-negative matrix factorization using huber cost function for source apportionment</article-title>,&#x0201d; in: 2016 <source>IEEE International Symposium on Signal Processing and Information Technology</source> (<italic>ISSPIT</italic>): <italic>IEEE</italic>), <fpage>69</fpage>&#x02013;<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1109/ISSPIT.2016.7886011</pub-id>
</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Dial</surname><given-names>J. M.</given-names></name><name><surname>Petrotchenko</surname><given-names>E. V.</given-names></name><name><surname>Borchers</surname><given-names>C. H.</given-names></name></person-group> (<year>2007</year>). <article-title>Inhibition of APCCdh1 activity by Cdh1/Acm1/Bmh1 ternary complex formation</article-title>. <source>J. Boil. Chem.</source>
<volume>282</volume> (<issue>8</issue>), <fpage>5237</fpage>&#x02013;<lpage>5248</lpage>. <pub-id pub-id-type="doi">10.1074/jbc.M606589200</pub-id>
</mixed-citation></ref><ref id="B10"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Ding</surname><given-names>C.</given-names></name><name><surname>Zhou</surname><given-names>D.</given-names></name><name><surname>He</surname><given-names>X.</given-names></name><name><surname>Zha</surname><given-names>H.</given-names></name></person-group> (<year>2006</year>). &#x0201c;<article-title>R 1-PCA: rotational invariant L 1-norm principal component analysis for robust subspace factorization</article-title>,&#x0201d; in: <conf-name>Proceedings of the 23rd international conference on Machine learning</conf-name>: <conf-sponsor>ACM</conf-sponsor>), <fpage>281</fpage>&#x02013;<lpage>288</lpage>.</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Du</surname><given-names>L.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Shen</surname><given-names>Y.-D.</given-names></name></person-group> (<year>2012</year>). &#x0201c;<article-title>Robust nonnegative matrix factorization via half-quadratic minimization</article-title>,&#x0201d; in 2012 <conf-name>IEEE 12th International Conference on Data Mining</conf-name> (<conf-sponsor>IEEE</conf-sponsor>), <fpage>201</fpage>&#x02013;<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1109/ICDM.2012.39</pub-id>
</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name><surname>Feng</surname><given-names>C.-M.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>J.-X.</given-names></name><name><surname>Gao</surname><given-names>Y.-L.</given-names></name><name><surname>Zheng</surname><given-names>C.-H.</given-names></name></person-group> (<year>2019</year>). <article-title>Supervised discriminative sparse PCA for com-characteristic gene selection and tumor classification on multiview biological data</article-title>. <source>IEEE Trans. Neural Netw. Learn. Syst</source>. <pub-id pub-id-type="doi">10.1109/TNNLS.2019.2893190</pub-id>
</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Guillamet</surname><given-names>D.</given-names></name><name><surname>Vitri&#x000e0;</surname><given-names>J.</given-names></name></person-group> (<year>2002</year>). &#x0201c;<article-title>Non-negative matrix factorization for face recognition</article-title>,&#x0201d; in <source>Catalonian Conference on Artificial Intelligence</source>. (<conf-sponsor>Springer</conf-sponsor>), <fpage>336</fpage>&#x02013;<lpage>344</lpage>. <pub-id pub-id-type="doi">10.1007/3-540-36079-4_29</pub-id>
</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Guofa</surname><given-names>S.</given-names></name><name><surname>Ling</surname><given-names>X.</given-names></name><name><surname>Feng</surname><given-names>L.</given-names></name><name><surname>Mingfeng</surname><given-names>J.</given-names></name><name><surname>Stuart</surname><given-names>C.</given-names></name></person-group> (<year>2011</year>). <article-title>On epicardial potential reconstruction using regularization schemes with the L1-norm data term</article-title>. <source>Phys. Med. Biol.</source>
<volume>56</volume> (<issue>1</issue>), <fpage>57</fpage>&#x02013;<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1088/0031-9155/56/1/004</pub-id>
<pub-id pub-id-type="pmid">21119225</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Honda</surname><given-names>S.</given-names></name><name><surname>Okada</surname><given-names>T.</given-names></name><name><surname>Miyagi</surname><given-names>H.</given-names></name><name><surname>Minato</surname><given-names>M.</given-names></name><name><surname>Suzuki</surname><given-names>H.</given-names></name><name><surname>Taketomi</surname><given-names>A.</given-names></name></person-group> (<year>2013</year>). <article-title>Spontaneous rupture of an advanced pancreatoblastoma: Aberrant RASSF1A methylation and CTNNB1 mutation as molecular genetic markers</article-title>. <source>J. Pediatr. Surg.</source>
<volume>48</volume> (<issue>4</issue>), <fpage>e29</fpage>&#x02013;<lpage>e32</lpage>. <pub-id pub-id-type="doi">10.1016/j.jpedsurg.2013.02.038</pub-id>
</mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Javadinia</surname><given-names>S. A.</given-names></name><name><surname>Shahidsales</surname><given-names>S.</given-names></name><name><surname>Fanipakdel</surname><given-names>A.</given-names></name><name><surname>Joudi-Mashhad</surname><given-names>M.</given-names></name><name><surname>Mehramiz</surname><given-names>M.</given-names></name><name><surname>Talebian</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Therapeutic potential of targeting the Wnt/&#x003b2;-catenin pathway in the treatment of pancreatic cancer</article-title>. <source>J. Cell. Biochem.</source>
<volume>120</volume> (<issue>5</issue>), <fpage>6833</fpage>&#x02013;<lpage>6840</lpage>. <pub-id pub-id-type="doi">10.1002/jcb.27835</pub-id>
</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kim</surname><given-names>H.</given-names></name><name><surname>Park</surname><given-names>H.</given-names></name></person-group> (<year>2007</year>). <article-title>Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis</article-title>. <source>Bioinformatics</source>
<volume>23</volume> (<issue>12</issue>), <fpage>1495</fpage>&#x02013;<lpage>1502</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btm134</pub-id>
<pub-id pub-id-type="pmid">17483501</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Kong</surname><given-names>D.</given-names></name><name><surname>Ding</surname><given-names>C. H. Q.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name></person-group> (<year>2011</year>). &#x0201c;<article-title>Robust nonnegative matrix factorization using L21-norm</article-title>,&#x0201d; in <source>Proceedings of the 20th ACM international conference on Information and knowledge management</source> (<conf-sponsor>ACM</conf-sponsor>) <fpage>673</fpage>&#x02013;<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1145/2063576.2063676</pub-id>
</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kubota</surname><given-names>Y.</given-names></name><name><surname>Kawakami</surname><given-names>H.</given-names></name><name><surname>Natsuizaka</surname><given-names>M.</given-names></name><name><surname>Kawakubo</surname><given-names>K.</given-names></name><name><surname>Marukawa</surname><given-names>K.</given-names></name><name><surname>Kudo</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>CTNNB1 mutational analysis of solid-pseudopapillary neoplasms of the pancreas using endoscopic ultrasound-guided fine-needle aspiration and next-generation deep sequencing</article-title>. <source>J. Gastroenterol.</source>
<volume>50</volume> (<issue>2</issue>), <fpage>203</fpage>&#x02013;<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1007/s00535-014-0954-y</pub-id>
<pub-id pub-id-type="pmid">24700283</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lee</surname><given-names>D. D.</given-names></name><name><surname>Seung</surname><given-names>H. S.</given-names></name></person-group> (<year>1999</year>). <article-title>Learning the parts of objects by non-negative matrix factorization</article-title>. <source>Nature</source>
<volume>401</volume>, <fpage>788</fpage>. <pub-id pub-id-type="doi">10.1038/44565</pub-id>
<pub-id pub-id-type="pmid">10548103</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Cui</surname><given-names>G.</given-names></name><name><surname>Dong</surname><given-names>Y.</given-names></name></person-group> (<year>2017</year>). <article-title>Graph regularized non-negative low-rank matrix factorization for image clustering</article-title>. <source>IEEE Trans. Cybern.</source>
<volume>47</volume> (<issue>11</issue>), <fpage>3840</fpage>&#x02013;<lpage>3853</lpage>. <pub-id pub-id-type="doi">10.1109/TCYB.2016.2585355</pub-id>
<pub-id pub-id-type="pmid">27448379</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lin</surname><given-names>T.</given-names></name><name><surname>Ren</surname><given-names>Q.</given-names></name><name><surname>Zuo</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>R.</given-names></name><name><surname>Xie</surname><given-names>L.</given-names></name><name><surname>Lin</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Valproic acid exhibits anti-tumor activity selectively against EGFR/ErbB2/ErbB3-coexpressing pancreatic cancer <italic>via</italic> induction of ErbB family members-targeting microRNAs</article-title>. <source>J. Exp. Clin. Cancer Res.</source>
<volume>38</volume> (<issue>1</issue>), <fpage>150</fpage>. <pub-id pub-id-type="doi">10.1186/s13046-019-1160-9</pub-id>
<pub-id pub-id-type="pmid">30961642</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>J.-X.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Gao</surname><given-names>Y.-L.</given-names></name><name><surname>Zheng</surname><given-names>C.-H.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>J.</given-names></name></person-group> (<year>2018</year>). <article-title>Regularized non-negative matrix factorization for identifying differentially expressed genes and clustering samples: a survey</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
<volume>15</volume> (<issue>3</issue>), <fpage>974</fpage>&#x02013;<lpage>987</lpage>. <pub-id pub-id-type="doi">10.1109/TCBB.2017.2665557</pub-id>
<pub-id pub-id-type="pmid">28186906</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>J.-X.</given-names></name><name><surname>Wang</surname><given-names>Y.-T.</given-names></name><name><surname>Zheng</surname><given-names>C.-H.</given-names></name><name><surname>Sha</surname><given-names>W.</given-names></name><name><surname>Mi</surname><given-names>J.-X.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name></person-group> (<year>2013</year>). &#x0201c;<article-title>Robust PCA based method for discovering differentially expressed genes</article-title>,&#x0201d; in <source>BMC bioinformatics.</source> (BioMed Central), <volume>14</volume>(<issue>8</issue>) <fpage>S3</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-14-S8-S3</pub-id>
</mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Pokharel</surname><given-names>P. P.</given-names></name><name><surname>Principe</surname><given-names>J. C.</given-names></name></person-group> (<year>2007</year>). <article-title>Correntropy: properties and applications in non-gaussian signal processing</article-title>. <source>IEEE Trans. Signal Process.</source>
<volume>55</volume> (<issue>11</issue>), <fpage>5286</fpage>&#x02013;<lpage>5298</lpage>. <pub-id pub-id-type="doi">10.1109/TSP.2007.896065</pub-id>
</mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Zhai</surname><given-names>D.</given-names></name><name><surname>Zhao</surname><given-names>D.</given-names></name><name><surname>Zhai</surname><given-names>G.</given-names></name><name><surname>Gao</surname><given-names>W.</given-names></name></person-group> (<year>2014</year>). <article-title>Progressive image denoising through hybrid graph Laplacian regularization: a unified framework</article-title>. <source>IEEE Trans. Image Process.</source>
<volume>23</volume> (<issue>4</issue>), <fpage>1491</fpage>&#x02013;<lpage>1503</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2014.2303638</pub-id>
<pub-id pub-id-type="pmid">24565791</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Luo</surname><given-names>X.</given-names></name><name><surname>Zhou</surname><given-names>M.</given-names></name><name><surname>Xia</surname><given-names>Y.</given-names></name><name><surname>Zhu</surname><given-names>Q.</given-names></name></person-group> (<year>2014</year>). <article-title>An efficient non-negative matrix-factorization-based approach to collaborative filtering for recommender systems</article-title>. <source>IEEE Trans. Ind. Inform.</source>
<volume>10</volume> (<issue>2</issue>), <fpage>1273</fpage>&#x02013;<lpage>1284</lpage>. <pub-id pub-id-type="doi">10.1109/TII.2014.2308433</pub-id>
</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Mao</surname><given-names>B.</given-names></name><name><surname>Guan</surname><given-names>N.</given-names></name><name><surname>Tao</surname><given-names>D.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Luo</surname><given-names>Z.</given-names></name></person-group> (<year>2014</year>). &#x0201c;<article-title>Correntropy induced metric based graph regularized non-negative matrix factorization</article-title>,&#x0201d; <conf-name>Proceedings 2014 IEEE international conference on Security, Pattern Analysis, and Cybernetics (SPAC).</conf-name> (<conf-sponsor>IEEE</conf-sponsor>), <fpage>163</fpage>&#x02013;<lpage>168</lpage>. <pub-id pub-id-type="doi">10.1109/SPAC.2014.6982679</pub-id>
</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Meric-Bernstam</surname><given-names>F.</given-names></name><name><surname>Johnson</surname><given-names>A. M.</given-names></name><name><surname>Dumbrava</surname><given-names>E. E. I.</given-names></name><name><surname>Raghav</surname><given-names>K.</given-names></name><name><surname>Balaji</surname><given-names>K.</given-names></name><name><surname>Bhatt</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Advances in HER2-targeted therapy: novel agents and opportunities beyond breast and gastric cancer</article-title>. <source>Clin. Cancer Res.</source>
<volume>25</volume> (<issue>7</issue>), <fpage>2033</fpage>&#x02013;<lpage>2041</lpage>. <pub-id pub-id-type="doi">10.1158/1078-0432.CCR-18-2275</pub-id>
<pub-id pub-id-type="pmid">30442682</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Michmerhuizen</surname><given-names>N. L.</given-names></name><name><surname>Leonard</surname><given-names>E.</given-names></name><name><surname>Matovina</surname><given-names>C.</given-names></name><name><surname>Harris</surname><given-names>M.</given-names></name><name><surname>Herbst</surname><given-names>G.</given-names></name><name><surname>Kulkarni</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Rationale for using irreversible epidermal growth factor receptor inhibitors in combination with phosphatidylinositol 3-kinase inhibitors for advanced head and neck squamous cell carcinoma</article-title>. <source>Mol. Pharmacol.</source>
<volume>95</volume> (<issue>5</issue>), <fpage>528</fpage>&#x02013;<lpage>536</lpage>. <pub-id pub-id-type="doi">10.1124/mol.118.115162</pub-id>
<pub-id pub-id-type="pmid">30858165</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Nagai</surname><given-names>M.</given-names></name><name><surname>Shibata</surname><given-names>A.</given-names></name><name><surname>Ushimaru</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <article-title>Cdh1 degradation is mediated by APC/C-Cdh1 and SCF-Cdc4 in budding yeast</article-title>. <source>Biochem. Biophys. Res. Commun.</source>
<volume>506</volume> (<issue>4</issue>), <fpage>932</fpage>&#x02013;<lpage>938</lpage>. <pub-id pub-id-type="doi">10.1016/j.bbrc.2018.10.179</pub-id>
<pub-id pub-id-type="pmid">30396569</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Nikolova</surname><given-names>M.</given-names></name><name><surname>Chan</surname><given-names>R. H.</given-names></name></person-group> (<year>2007</year>). <article-title>The equivalence of half-quadratic minimization and the gradient linearization iteration</article-title>. <source>IEEE Trans. Image Process.</source>
<volume>16</volume> (<issue>6</issue>), <fpage>1623</fpage>&#x02013;<lpage>1627</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2007.896622</pub-id>
<pub-id pub-id-type="pmid">17547139</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Qi</surname><given-names>L.</given-names></name><name><surname>Jiang</surname><given-names>H.</given-names></name></person-group> (<year>1997</year>). <article-title>Semismooth Karush-Kuhn-Tucker equations and convergence analysis of newton and quasi-newton methods for solving these equations</article-title>. <source>Math. Oper. Res.</source>
<volume>22</volume> (<issue>2</issue>), <fpage>301</fpage>&#x02013;<lpage>325</lpage>. <pub-id pub-id-type="doi">10.1287/moor.22.2.301</pub-id>
</mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Roweis</surname><given-names>S. T.</given-names></name><name><surname>Saul</surname><given-names>L. K.</given-names></name></person-group> (<year>2000</year>). <article-title>Nonlinear dimensionality reduction by locally linear embedding</article-title>. <source>Science</source>
<volume>290</volume> (<issue>5500</issue>), <fpage>2323</fpage>&#x02013;<lpage>2326</lpage>. <pub-id pub-id-type="doi">10.1126/science.290.5500.2323</pub-id>
<pub-id pub-id-type="pmid">11125150</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>M. N.</given-names></name><name><surname>Olsson</surname><given-names>R. K.</given-names></name></person-group> (<year>2006</year>). &#x0201c;<article-title>Single-channel speech separation using sparse non-negative matrix factorization</article-title>,&#x0201d; in <conf-name>Ninth International Conference on Spoken Language Processing</conf-name> (<conf-sponsor>IEEE</conf-sponsor>).</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>J.-X.</given-names></name><name><surname>Gao</surname><given-names>Y.-L.</given-names></name><name><surname>Zheng</surname><given-names>C.-H.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name></person-group> (<year>2016</year>a). <article-title>Characteristic gene selection based on robust graph regularized non-negative matrix factorization</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
<volume>13</volume> (<issue>6</issue>), <fpage>1059</fpage>&#x02013;<lpage>1067</lpage>. <pub-id pub-id-type="doi">10.1109/TCBB.2015.2505294</pub-id>
<pub-id pub-id-type="pmid">26672047</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>H. L.</given-names></name><name><surname>Hart</surname><given-names>J.</given-names></name><name><surname>Fan</surname><given-names>L.</given-names></name><name><surname>Mustafi</surname><given-names>R.</given-names></name><name><surname>Bissonnette</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Upregulation of glycogen synthase kinase 3&#x003b2; in human colorectal adenocarcinomas correlates with accumulation of CTNNB1</article-title>. <source>Clin. Colorectal Cancer</source>
<volume>10</volume> (<issue>1</issue>), <fpage>30</fpage>&#x02013;<lpage>36</lpage>. <pub-id pub-id-type="doi">10.3816/CCC.2011.n.004</pub-id>
<pub-id pub-id-type="pmid">21609933</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Wang</surname><given-names>B.</given-names></name><name><surname>Zhang</surname><given-names>Y.-M.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name></person-group> (<year>2016</year>b). <article-title>The association between CDH1 promoter methylation and patients with ovarian cancer: a systematic meta-analysis</article-title>. <source>J. Ovarian Res.</source>
<volume>9</volume>, <fpage>23</fpage>. <pub-id pub-id-type="doi">10.1186/s13048-016-0231-1</pub-id>
<pub-id pub-id-type="pmid">27067410</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y. Q.</given-names></name><name><surname>Yuan</surname><given-names>Y.</given-names></name><name><surname>Jiang</surname><given-names>S.</given-names></name><name><surname>Jiang</surname><given-names>H.</given-names></name></person-group> (<year>2016</year>c). <article-title>Promoter methylation and expression of CDH1 and susceptibility and prognosis of eyelid squamous cell carcinoma</article-title>. <source>Tumor Biol.</source>
<volume>37</volume> (<issue>7</issue>), <fpage>9521</fpage>&#x02013;<lpage>9526</lpage>. <pub-id pub-id-type="doi">10.1007/s13277-016-4851-2</pub-id>
</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Wu</surname><given-names>M.-J.</given-names></name><name><surname>Liu</surname><given-names>J.-X.</given-names></name><name><surname>Gao</surname><given-names>Y.-L.</given-names></name><name><surname>Kong</surname><given-names>X.-Z.</given-names></name><name><surname>Feng</surname><given-names>C.-M.</given-names></name></person-group> (<year>2017</year>). &#x0201c;<article-title>Feature selection and clustering <italic>via</italic> robust graph-laplacian PCA based on capped L 1-norm</article-title>,&#x0201d; in 2017 <conf-name>IEEE international conference on Bioinformatics and Biomedicine (BIBM)</conf-name> (<conf-sponsor>IEEE</conf-sponsor>), <fpage>1741</fpage>&#x02013;<lpage>1745</lpage>. <pub-id pub-id-type="doi">10.1109/BIBM.2017.8217923</pub-id>
</mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Xu</surname><given-names>A.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Han</surname><given-names>G.</given-names></name><name><surname>Cai</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <article-title>Simultaneous interrogation of cancer omics to identify subtypes with signi&#x0fb01;cant clinical differences</article-title>. <source>Front. Genet.</source>
<volume>10</volume>, <fpage>236</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2019.00236</pub-id>
<pub-id pub-id-type="pmid">30984238</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Xu</surname><given-names>W.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Gong</surname><given-names>Y.</given-names></name></person-group> (<year>2003</year>). &#x0201c;<article-title>Document clustering based on non-negative matrix factorization</article-title>,&#x0201d; in <conf-name>Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval</conf-name> (<conf-sponsor>ACM</conf-sponsor>), <fpage>267</fpage>&#x02013;<lpage>273</lpage>.</mixed-citation></ref><ref id="B43"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Yu</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name></person-group> (<year>2003</year>). &#x0201c;<article-title>Feature selection for high-dimensional data: A fast correlation-based filter solution</article-title>,&#x0201d; in <conf-name>Proceedings of the 20th international conference on machine learning (ICML-03).</conf-name> (<conf-sponsor>AAAI</conf-sponsor>), <fpage>856</fpage>&#x02013;<lpage>863</lpage>.</mixed-citation></ref><ref id="B44"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Yu</surname><given-names>N.</given-names></name><name><surname>Liu</surname><given-names>J.-X.</given-names></name><name><surname>Gao</surname><given-names>Y.-L.</given-names></name><name><surname>Zheng</surname><given-names>C.-H.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Wu</surname><given-names>M.-J.</given-names></name></person-group> (<year>2017</year>). &#x0201c;<article-title>Graph regularized robust non-negative matrix factorization for clustering and selecting differentially expressed genes</article-title>,&#x0201d; in <conf-name>2017 IEEE international conference on Bioinformatics and Biomedicine (BIBM)</conf-name> (<conf-sponsor>IEEE</conf-sponsor>), <fpage>1752</fpage>&#x02013;<lpage>1756</lpage>. <pub-id pub-id-type="doi">10.1109/BIBM.2017.8217925</pub-id>
</mixed-citation></ref><ref id="B45"><mixed-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Yu</surname><given-names>T.</given-names></name><name><surname>Zhao</surname><given-names>Z.</given-names></name><name><surname>Yan</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name></person-group> (<year>2016</year>). &#x0201c;<article-title>Robust L1-norm matrixed locality preserving projection for discriminative subspace learning</article-title>,&#x0201d; in <conf-name>2016 International Joint Conference on Neural Networks (IJCNN)</conf-name> (<conf-sponsor>IEEE</conf-sponsor>), <fpage>4199</fpage>&#x02013;<lpage>4204</lpage>.</mixed-citation></ref></ref-list></back></article>