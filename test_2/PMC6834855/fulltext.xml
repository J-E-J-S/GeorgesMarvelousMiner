<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6834855</article-id><article-id pub-id-type="publisher-id">52196</article-id><article-id pub-id-type="doi">10.1038/s41598-019-52196-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Athena: Automated Tuning of k-mer based Genomic Error Correction Algorithms using Language Models</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Abdallah</surname><given-names>Mustafa</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Mahgoub</surname><given-names>Ashraf</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ahmed</surname><given-names>Hany</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Chaterji</surname><given-names>Somali</given-names></name><address><email>schaterji@schaterji.io</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1937 2197</institution-id><institution-id institution-id-type="GRID">grid.169077.e</institution-id><institution>School of Electrical and Computer Engineering, </institution><institution>Purdue University, </institution></institution-wrap>West Lafayette, USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0639 9286</institution-id><institution-id institution-id-type="GRID">grid.7776.1</institution-id><institution>Department of Electronics and Electrical Communications Engineering, </institution><institution>Cairo University, </institution></institution-wrap>Cairo, Egypt </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1937 2197</institution-id><institution-id institution-id-type="GRID">grid.169077.e</institution-id><institution>Department of Agricultural and Biological Engineering, </institution><institution>Purdue University, </institution></institution-wrap>West Lafayette, USA </aff></contrib-group><pub-date pub-type="epub"><day>6</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>6</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>9</volume><elocation-id>16157</elocation-id><history><date date-type="received"><day>1</day><month>4</month><year>2019</year></date><date date-type="accepted"><day>7</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2019</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The performance of most error-correction (EC) algorithms that operate on genomics reads is dependent on the proper choice of its configuration parameters, such as the value of <italic>k</italic> in <italic>k</italic>-mer based techniques. In this work, we target the problem of finding the best values of these configuration parameters to optimize error correction and consequently improve genome assembly. We perform this in an adaptive manner, adapted to different datasets <italic>and</italic> to EC tools, due to the observation that different configuration parameters are optimal for different datasets, <italic>i</italic>.<italic>e</italic>., from different platforms and species, and vary with the EC algorithm being applied. We use language modeling techniques from the Natural Language Processing (NLP) domain in our algorithmic suite, Athena, to automatically tune the performance-sensitive configuration parameters. Through the use of <italic>N</italic>-Gram and Recurrent Neural Network (RNN) language modeling, we validate the intuition that the EC performance can be computed quantitatively and efficiently using the &#x0201c;perplexity&#x0201d; metric, repurposed from NLP. After training the language model, we show that the perplexity metric calculated from a sample of the test (or production) data has a strong negative correlation with the quality of error correction of erroneous NGS reads. Therefore, we use the perplexity metric to guide a hill climbing-based search, converging toward the best configuration parameter value. Our approach is suitable for both <italic>de novo</italic> and comparative sequencing (resequencing), eliminating the need for a reference genome to serve as the ground truth. We find that Athena can automatically find the optimal value of <italic>k</italic> with a very high accuracy for 7 real datasets and using 3 different <italic>k</italic>-mer based EC algorithms, Lighter, Blue, and Racer. The inverse relation between the perplexity metric and alignment rate exists under all our tested conditions&#x02014;for real and synthetic datasets, for all kinds of sequencing errors (insertion, deletion, and substitution), and for high and low error rates. The absolute value of that correlation is at least 73%. In our experiments, the best value of <italic>k</italic> found by <bold>A</bold><bold><sc>thena</sc></bold> achieves an alignment rate within 0.53% of the oracle best value of <italic>k</italic> found through brute force searching (<italic>i</italic>.<italic>e</italic>., scanning through the entire range of <italic>k</italic> values). Athena&#x02019;s selected value of <italic>k</italic> lies within the top-3 best k values using N-Gram models and the top-5 best k values using RNN models With best parameter selection by Athena, the assembly quality (NG50) is improved by a Geometric Mean of 4.72X across the 7 real datasets.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computational models</kwd><kwd>Computer science</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000002</institution-id><institution>U.S. Department of Health &#x00026; Human Services | National Institutes of Health (NIH)</institution></institution-wrap></funding-source><award-id>1R01AI123037</award-id><principal-award-recipient><name><surname>Chaterji</surname><given-names>Somali</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Lilly Endowment (Wabash Heartland Innovation Network)</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Rapid advances in next-generation sequencing (NGS) technologies, with the resulting drops in sequencing costs, offer unprecedented opportunities to characterize genomes across the tree-of-life. While NGS techniques allow for rapid parallel sequencing, they are more error-prone than Sanger reads and generate different error profiles, <italic>e.g</italic>., substitutions, insertions, and deletions. The genome analysis workflow needs to be carefully orchestrated so errors in reads are not magnified downstream. Consequently, multiple error-correction (EC) techniques have been developed for improved performance for applications ranging from <italic>de novo</italic> variant calling to differential expression, iterative <italic>k</italic>-mer selection for improved genome assembly<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> and for long-read error correction<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par3">Importantly, the values of the performance-sensitive configuration parameters are dependent not only on the dataset but also on the specific EC tool (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). The performance of many EC algorithms is highly dependent on the proper choice of configuration parameters, <italic>e.g</italic>., <italic>k</italic>-value (length of the substring) in <italic>k</italic>-spectrum-based techniques. Selecting different <italic>k</italic>-values has a trade-off such that small values increase the overlap probability between reads, however, an unsuitably small value degrades EC performance because it does not allow the algorithm to discern correct <italic>k</italic>-mers from erroneous ones. In contrast, unsuitably high <italic>k</italic>-values decrease the overlap probability and hurts EC performance because most <italic>k</italic>-mers will now appear unique. The <italic>k</italic>-mers that appear above a certain threshold frequency, and are therefore expected to be legitimate, are <italic>solid k-mers</italic>, the others are called <italic>insolid or untrusted k-mers</italic>. In <italic>k</italic>-spectrum-based methods, the goal is to convert insolid <italic>k</italic>-mers to solid ones with a minimum number of edit operations. Thus, an adaptive method for finding the best <italic>k</italic>-value and other parameters is needed for improved EC performance, and in turn, genome assembly.</p><p id="Par4">Many existing EC solutions (<italic>e.g</italic>., Reptile<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, Quake<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, Lighter<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, Blue<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>) require users to specify the <italic>k</italic>-mer size. Although these EC tools do not rely on a reference genome to perform correction, the best configuration value is usually found by exploration over the range of <italic>k</italic>-values<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> and evaluating performance metrics, <italic>e.g</italic>., EC Gain, Alignment Rate, Accuracy, Recall, and Precision using a reference genome. Therefore, a reference genome is typically needed to serve as ground truth for such evaluations, making this tuning approach infeasible for <italic>de novo</italic> sequencing tasks. Existing tools leave the best parameter choice to the end user and this has been explicitly pointed out as an open area of work in<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>. However, a number of recent surveys highlighted that the automated choice of parameters for the specific dataset being processed is crucial for the user to avoid inadvertently selecting the wrong parameters<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>.</p><p id="Par5">Some existing tools (e.g., KMERGENIE<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>) provide intuitive abundance histograms or heuristics to guide <italic>k</italic>-value selection when performing de Bruijn graph based genome assembly. However, they only account for the dataset when performing the optimal <italic>k</italic>-value selection. We find that this approach is unsuitable for our problem (i.e., finding best <italic>k</italic>-value for error correction) as the optimal <italic>k</italic>-value here also depends on the correction algorithm (e.g., the optimal <italic>k</italic>-values for Blue and Lighter in our evaluation vary, Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). Also, the user is finally responsible for interpreting the visualization and selecting the optimal <italic>k</italic>-mer value. To make the above argument specific, we found that <italic>k</italic>&#x02009;=&#x02009;25 achieves within 0.04% from the best EC Gain performance for dataset <italic>D1</italic> when using Blue. On the other hand, if the same <italic>k</italic>-value was used for <italic>D1</italic> again but this time with the tool Lighter, the EC Gain drops by 26.8% from the maximum Gain (Tables&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref> and <xref rid="MOESM1" ref-type="media">2</xref> in Appendix).</p><p id="Par6">In addition, there is no single <italic>k</italic>-value that works optimally for all datasets, even when using the <italic>same</italic> EC tool. For example, we found that <italic>k</italic>&#x02009;=&#x02009;25 gives the best EC Gain performance for <italic>D5</italic> using Lighter, showing an EC Gain of 83.8%. However, the same <italic>k</italic>-value used for <italic>D3</italic> with Lighter gives an EC Gain of only 65% compared to a Gain of 95.34% when using the optimal <italic>k</italic>-value (Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref> in Appendix). Thus, there is a need for a <italic>data-driven and tool-specific</italic> method to select the optimal <italic>k</italic>-value.</p><p id="Par7">Our solution, Athena finds the best value of the configuration parameters for correcting errors in genome sequencing, such as the value of <italic>k</italic> in <italic>k</italic>-mer based methods (Just as Athena is the Greek Goddess of wisdom and a fierce warrior, we wish our technique to unearth the genomic codes underlying disease in a fearless war against maladies). Further, Athena does not require access to a reference genome to determine the optimal parameter configuration. In our evaluation, we use Bowtie2 for alignment and measure alignment rate as a metric to evaluate Athena. However, alignment is <italic>not</italic> needed for Athena to work as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. Athena, like other EC tools, leverages the fact that NGS reads have the property of reasonably high coverage, 30X&#x02013;150X coverage depth is commonplace. From this, it follows that the likelihood of correct overlaps for a given portion of the genome will outnumber the likelihood of erroneous ones<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Athena uses a language model (LM) to estimate the correctness of the observed sequence considering the frequency of each subsequence and its fitness with respect to the context. This is integral to traditional NLP tasks such as speech recognition, machine translation, or text summarization in which LM is a probability distribution capturing certain characteristics of a sequence of symbols and words, which in this case is specialized to the dataset <italic>and</italic> to the EC tool of choice for optimal performance.<fig id="Fig1"><label>Figure 1</label><caption><p>Overview of Athena&#x02019;s workflow. First, we train the language model using the entire set of uncorrected reads for the specific dataset. Second, we perform error correction on a subsample from the uncorrected reads using an EC tool (<italic>e.g</italic>., Lighter or Blue) and a range of <italic>k</italic>-values. Third, we compute perplexity of each corrected sample, corrected with a specific EC tool, and decide on the best <italic>k</italic>-value for the next iteration, <italic>i.e</italic>., the one corresponding to the lowest perplexity metric because EC quality is negatively correlated with the perplexity metric. This process continues until the termination criteria are met. Finally, the complete set of reads is corrected with the best <italic>k</italic>-value found and then used for evaluation.</p></caption><graphic xlink:href="41598_2019_52196_Fig1_HTML" id="d29e541"/></fig></p><p id="Par8">In our context, we use LM to estimate the probabilistic likelihood that some observed sequence is solid or insolid, in the context of the specific genome. We create an LM using as training, the enire original dataset (with uncorrected reads). We then run the EC algorithm on a subset of the overall data with a specific value of the configuration parameter. Subsequently, we use the trained LM at runtime to compute a metric called the &#x0201c;Perplexity metric&#x0201d; (which is widely used in the NLP literature). We show empirically that the perplexity metric has a strong negative correlation with the quality of the error correction (measured typically through the metric called &#x0201c;EC gain&#x0201d;). Crucially, the Perplexity metric evaluation does not require the computationally expensive alignment to a reference genome, even when available. Through a stochastic optimization method, we evaluate the search space to pick the best configuration-parameter value for the EC algorithm-<italic>k</italic> in <italic>k</italic>-mer-based methods and the Genome Length (GL) in the RACER EC tool. Moreover, most EC tools are evaluated based on their direct ability to reduce error rates rather than to improve genome assembly. Although in general, assembly benefits from the error correction pre-processing step, sub-optimal error correction can <italic>reduce</italic> assembly quality due to conversion of benign errors into damaging ones<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. In our evaluation, we see that the EC improvement due to Athena also leads to higher quality assembly (Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>).</p><p id="Par9">In summary, this paper makes the following contributions.<list list-type="order"><list-item><p id="Par10">We compare and contrast two LM variants of Athena, <italic>N</italic>-gram and RNN-based. Through this, we show that N-Gram modeling can be faster to train while char-RNN provides similar accuracy to <italic>N</italic>-gram, albeit with significantly lower memory footprint, conducive to multi-tenant analysis pipelines (<italic>e.g</italic>., MG-RAST for metagenomics processing<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>).</p></list-item><list-item><p id="Par11">We introduce a likelihood-based metric, the Perplexity metric (repurposed from NLP), to evaluate EC quality without the need for a reference genome. This is the first such use of this metric in the computational genomics domain.</p></list-item><list-item><p id="Par12">We compare and contrast two LM variants of Athena, <italic>N</italic>-gram and RNN-based. Through this, we show that N-Gram modeling can be faster to train while char-RNN provides similar accuracy to <italic>N</italic>-gram, albeit with significantly lower memory footprint, conducive to multi-tenant analysis pipelines (<italic>e.g</italic>., MG-RAST for metagenomics processing<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>).</p></list-item><list-item><p id="Par13">We apply Athena to 3 <italic>k</italic>-mer based EC tools: Lighter<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, Blue<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, and RACER<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> on 7 real datasets, with varied error rates and read lengths. We show that Athena was successful in finding either the best parameters (<italic>k</italic> for Lighter and Blue, and <italic>GenomeLength</italic> for RACER) or parameters that perform within 0.53% of the overall alignment rate to the best values using exhaustive search against a reference genome. We couple EC, with best parameter selection by Athena, to the Velvet genome assembler and find that it improves assembly quality (NG50) by a Geometric Mean of 4.72X across 7 evaluated datasets.</p></list-item></list></p></sec><sec id="Sec2"><title>Background</title><sec id="Sec3"><title>Error correction and evaluation</title><p id="Par14">The majority of error correction tools share the following intuition: high-fidelity sequences (or, solid sequences) can be used to correct errors in low-fidelity sequences (or, in-solid sequences). However, they vary significantly in the way they differentiate between solid and in-solid sequences. For example<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, corrects genomic reads containing insolid <italic>k</italic>-mers using a minimum number of edit operations such that these reads contain only solid <italic>k</italic>-mers after correction. The evaluation of <italic>de novo</italic> sequencing techniques rely on likelihood-based metrics such as ALE<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> and CGAL<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, without relying on the availability of a reference genome. On the other hand, comparative sequencing or re-sequencing, such as to study structural variations among two genomes, do have reference genomes available.</p></sec><sec id="Sec4"><title>Language modeling</title><p id="Par15">To increase the accuracy of detecting words in speech recognition, language modeling techniques have been used to see which word combinations have higher likelihood of occurrence than others, thus improving context-based semantics. Thus, language modeling is being used in many applications such as speech recognition<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, text retrieval, and many NLP applications. The main task of these statistical models is to capture historical information and predict the future sequences based on that information<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Language models are classified into two main categories: (i) Count-based methods that represent traditional statistical models, usually involve estimating N-gram probabilities via counting and subsequent smoothing. (ii) Continuous-space language modeling is based on training deep learning algorithms. In recent years, continuous-space LMs such as fully-connected Neural Probabilistic Language Models (NPLM) and Recurrent Neural Network language models (RNNs) are proposed. Now we describe in detail each class of our language models.</p></sec><sec id="Sec5"><title>N-Gram-based language modeling</title><p id="Par16">This type of modeling is word-based. The main task that N-Gram based models<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> have been used for is to estimate the likelihood of observing a word <italic>W</italic><sub><italic>i</italic></sub>, given the set of previous words <italic>W</italic><sub>0</sub>, &#x02026;<italic>W</italic><sub><italic>i</italic>&#x02212;1</sub>, estimated using the following equation:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P({W}_{0},{W}_{1}\mathrm{,...,}{W}_{m})=\mathop{\prod }\limits_{i\mathrm{=1}}^{m}\,P({W}_{i}|{W}_{i-1}\mathrm{,...,}{W}_{1})\approx \mathop{\prod }\limits_{i\mathrm{=1}}^{m}\,P({W}_{i}|{W}_{i-1}\mathrm{,...,}{W}_{i-n})$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>,...,</mml:mn><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mn>=1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mspace width=".25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>,...,</mml:mn><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02248;</mml:mo><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mn>=1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mspace width=".25em"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>,...,</mml:mn><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2019_52196_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>n</italic> represents the number of history words the model uses to predict the next word. Obviously, a higher <italic>n</italic> results in better prediction, at the cost of higher training time resulting from a more complex model. Also notice that for this model to operate, it has to store all conditional probability values and hence has a high memory footprint.</p></sec><sec id="Sec6"><title>Char-RNN-Based language modeling</title><p id="Par17">Recurrent neural network (RNN) is a very popular class of neural networks for dealing with sequential data, frequently encountered in the NLP domain. The power of RNN is that each neuron or unit can use its internal state memory to save information from the previous input and use that state, together with the current input, to determine what the next output should be. Character-level RNN models, <italic>char-RNN</italic> for short, operate by taking a chunk of text and modeling the probability distribution of the next character in the sequence, given a sequence of previous characters. This then allows it to generate new text, one character at a time<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. RNNs consist of three main layers: Input Layer, Hidden Layer, and Output Layer. First, Input Layer takes <italic>x</italic><sub><italic>t</italic></sub> vector, which is input at a time step <italic>t</italic>, usually a one-hot encoding vector of the <italic>t</italic><sup><italic>th</italic></sup> word or character of the input sentence. Second, Hidden Layer consists of the hidden state at the same time step <italic>s</italic><sub><italic>t</italic></sub>, which represents the memory of this network. It is calculated as a non-linear function <italic>f</italic> (<italic>e.g</italic>., tanh) of the previous hidden state <italic>s</italic><sub><italic>t</italic>&#x02212;1</sub> and the input at current time step <italic>x</italic><sub><italic>t</italic></sub> with the following relation:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{t}=f(U{x}_{t}+W{s}_{t-1}\mathrm{).}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>).</mml:mn></mml:math><graphic xlink:href="41598_2019_52196_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par18">Here, <italic>W</italic> is a matrix that consists of hidden weights of this hidden layer. Finally, Output Layer consists of a vector <italic>o</italic><sub><italic>t</italic></sub>, which represents the output at step <italic>t</italic> and contains prediction probabilities for the next character in the sentence. Formally, its length equals the size of the vocabulary and is calculated using a softmax function. Backpropagation was used to train the RNN to update weights and minimize the error between the observed and the estimated next word. For Deep RNN architectures, there are multiple parameters that affect the performance of the model. The two main parameters are: <italic>Number of Hidden Layers</italic> and <italic>Number of Neurons per Layer</italic>. For our Char-RNN language modeling, vocabulary would include the four nucleotide bases as characters A, C, G, and T. Each input is a one-hot encoding vector for the four nucleotides. Each output vector at each time step also has the same dimension.</p></sec><sec id="Sec7"><title>Perplexity of the language model</title><p id="Par19">Perplexity is a measurement of how well a language model predicts a sample. In NLP, perplexity is one of the most effective ways of evaluating the goodness of fit of a language model since a language model is a probability distribution over entire sentences of text<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. For example, 5 per word perplexity of a model translates to the model being as confused on test data as if it had to select uniformly and independently from 5 possibilities for each word. Thus, a lower perplexity indicates that language model is better at making predictions. For an N-Gram language model, perplexity of a sentence is the inverse probability of the test set, normalized by the number of words<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PP(W)=\sqrt[m]{\frac{1}{P({W}_{1},{W}_{2}\mathrm{,....,}{W}_{m})}}\approx \sqrt[m]{\frac{1}{{\prod }_{i\mathrm{=1}}^{m}P({W}_{i}|{W}_{i-1}\mathrm{,...,}{W}_{i-n})}}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mroot><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mn>,....,</mml:mn><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mi>m</mml:mi></mml:mroot><mml:mo>&#x02248;</mml:mo><mml:mroot><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mn>=1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>,...,</mml:mn><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mi>m</mml:mi></mml:mroot></mml:math><graphic xlink:href="41598_2019_52196_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par20">It is clear from (<xref rid="Equ3" ref-type="">3</xref>) that minimizing perplexity is the same as maximizing the probability of the observed set of <italic>m</italic> words from <italic>W</italic><sub>1</sub> to <italic>W</italic><sub><italic>m</italic></sub>.</p><p id="Par21">For RNN, perplexity is measured as the exponential of the mean of the cross-entropy loss (CE) as shown in (<xref rid="Equ4" ref-type="">4</xref>)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, where <inline-formula id="IEq1"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><mml:math id="M8"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2019_52196_Article_IEq1.gif"/></alternatives></inline-formula> is the predicted next character&#x02013;the output of the RNN&#x02013;and |<italic>V</italic>| is the vocabulary size used during training.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CE(y,\hat{y})=-\mathop{\sum }\limits_{i=1}^{|V|}\,p({y}_{i})\log (p({\hat{y}}_{i}\mathrm{)).}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:munderover><mml:mspace width=".25em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mn>)).</mml:mn></mml:math><graphic xlink:href="41598_2019_52196_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par22">Although these two models estimate the perplexity metric differently, they achieve the same purpose, which is estimating the correctness of a sequence given the trained probability distribution. In the next section, we describe how our system Athena uses these models to find the best k-value for a given tool and dataset.</p></sec></sec><sec id="Sec8"><title>Our Solution: Athena</title><sec id="Sec9"><title>Application of language models</title><p id="Par23">We use two different LM variants in our Athena algorithm. We describe them next.</p><sec id="Sec10"><title>N-Gram language models</title><p id="Par24">We train an N-Gram model<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, which is word-based, from the input set of reads before correction. This N-Gram model needs word-based segmentation of the input read as a pre-processing phase. Then, we use this trained LM to evaluate EC performance.</p></sec><sec id="Sec11"><title>RNN language models</title><p id="Par25">The second technique is RNN-based LM<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, using different RNN architectures, <italic>e.g</italic>., standard RNNs, LSTMs, and GRUs. These models can be trained either as word-based models or character-based models. We train our RNN variant as character-based model to avoid having to make the decision about how to segment the genomic string, as we have to do for the N-Gram model.</p></sec><sec id="Sec12"><title>Training time and memory footprint</title><p id="Par26">Contrasting our 2 LM variants: Although training N-Gram LMs is much faster relative to RNN-based models (3&#x02013;5&#x02009;minutes for N-Gram <italic>vs</italic>. 10&#x02013;11&#x02009;hours for RNN-based across the 7 datasets), they still have the requirement of splitting a read into words of specific length. Further, RNN-based models have much lower memory footprint and storage requirements relative to N-Gram. This is because N-Gram models need to store conditional probabilities in large tables with an average size of 0.6&#x02013;1.1 GB across the 7 datasets. In contrast, RNNs only need to store the network architecture and weights with an average size of 3&#x02013;5 MB across the 7 datasets. For instance, the size of N-gram model for D7 is 2 GB while the size for the RNN model is only 3.5 MB. With respect to run time, N-gram takes 16&#x02009;minutes for the whole dataset D7 while RNN takes 30&#x02009;minutes on a 50&#x02009;K sample of D7. Also, we used an LSTM variant of Athena and found that it took 3 times longer to train and test but gave insignificant improvement in perplexity over RNN.</p></sec></sec><sec id="Sec13"><title>Intuition for the use of the perplexity metric</title><p id="Par27">Our design uses the Perplexity metric to provide an accurate, and importantly, quick estimation of the EC quality with the current configuration parameter value(s). The Perplexity metric is based on the LM trained on the entire original (uncorrected) dataset. The Perplexity metric is then calculated on the dataset of the corrected reads (entire dataset for N-Gram and 1% for RNN) to measure the EC performance. It measures how well LM can predict the next element in an input stream. Suppose the input stream is <italic>H</italic> and the next element is <italic>e</italic>. Then, the Perplexity metric is inversely proportional to the probability of seeing &#x0201c;e&#x0201d; in the stream, given the history <italic>H</italic> for the learned model. Moreover, the Perplexity metric has the advantage of taking the context of the element (<italic>e.g</italic>., previous and subsequent <italic>k</italic>-mers) into consideration when estimating the probability of observing that element. This context-awareness feature is not considered in simple <italic>k</italic>-mer counting methods. This method works because we see empirically that there is a high negative correlation of the Perplexity metric with both EC metrics&#x02013;Alignment Rate and EC Gain. Given this anti-correlation, we can rely on the Perplexity metric as an evaluation function, and apply a simple search technique (<italic>e.g</italic>., hill climbing) to find the best <italic>k</italic>-value for a given dataset. In this description, for simplicity of exposition, we use the <italic>k</italic>-value in <italic>k</italic>-mer based techniques as an example of Athena -tuned configuration parameter. However, Athena can tune any other relevant configuration parameter in EC algorithms and we experimentally show the behavior with another parameter&#x02013;Genome Length&#x02013;in the RACER tool. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> shows an example how Perplexity can evaluate the likelihood of a sequence of <italic>k</italic>-mers using their frequencies and contextual dependencies. In this example, we notice that the corrected read set (<italic>i.e</italic>., on the right) has a considerably lower Perplexity value (15.2), relative to the erroneous set (77.72). Thus, our intuition that the Perplexity metric reflects the correctness of the read dataset holds true here through the observed negative relationship.<fig id="Fig2"><label>Figure 2</label><caption><p>An example showing how the perplexity metric encodes errors in genomic reads. The read on the left is an erroneous read selected from dataset D3, while the read on the right is the same read, after correction with Lighter. When using language modeling to compute the perplexity for both reads, we notice that the read on the right has a lower perplexity value (15.2), relative to the erroneous read (77.72), as the sequence of <italic>k</italic>-mers after correction has a higher probability of occurrence. Also notice that the probability of a sequence of <italic>k</italic>-mers depends on both their frequencies and their relative order in the read, which allows the perplexity metric to capture how likely it is to observe this <italic>k</italic>-mer given the neighboring <italic>k</italic>-mers in a given read.</p></caption><graphic xlink:href="41598_2019_52196_Fig2_HTML" id="d29e1341"/></fig></p></sec><sec id="Sec14"><title>Search through the parameter space</title><p id="Par28">Our objective is to find the best <italic>k</italic>-value that will minimize the Perplexity of the corrected dataset. The Perplexity function is denoted by <italic>f</italic> in (<xref rid="Equ5" ref-type="">5</xref>).<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{k}_{opt}={\rm{\arg }}\,{{\rm{\min }}}_{{k}_{i}}\,Perplexity={\rm{\arg }}\,{{\rm{\min }}}_{{k}_{i}}\,f(LM,{D}_{0},{k}_{i})\end{array}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mi>P</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41598_2019_52196_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par29">Here, LM: trained language model, <italic>D</italic><sub>0</sub>: uncorrected read set, and <italic>k</italic>: the configuration parameter we wish to tune. <italic>f</italic> is a discrete function as <italic>k</italic>-values are discrete, and therefore, its derivative is not computable. Thus, a gradient-based optimization technique is inapplicable. Hence, we use a simple hill-climbing technique to find the value of <italic>k</italic> that gives the minimum value of <italic>f</italic>, for the given LM and <italic>D</italic><sub>0</sub> in (<xref rid="Equ5" ref-type="">5</xref>).</p><p id="Par30">The following pseudo-code describes the steps used for finding the best <italic>k</italic>-value for a given dataset. We start with Algorithm 1, which invokes Algorithm 2 multiple times, each time with a different starting value. We begin by training an LM on the original uncorrected read set (<italic>D</italic><sub>0</sub>). Second, we assume that the best value of <italic>k</italic> lies in a range from <italic>A</italic> to <italic>B</italic> (initially set to either the tool&#x02019;s recommended range, or between 1 and <italic>L</italic>, where <italic>L</italic> is the read size).</p><p id="Par31">We apply an existing EC algorithm (Lighter, Blue, or RACER in our evaluation) with different initial values <inline-formula id="IEq2"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$({k}_{0},\ldots ,{k}_{m})\in (A,B)$$\end{document}</tex-math><mml:math id="M14"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math><inline-graphic xlink:href="41598_2019_52196_Article_IEq2.gif"/></alternatives></inline-formula> to avoid getting stuck in local minima, going through multiple iterations for a given initial value. We evaluate the Perplexity for the corrected dataset with current values of <italic>k</italic>: <italic>k</italic><sub><italic>i</italic></sub>, and its neighbors, (<italic>k</italic><sub><italic>i</italic></sub>&#x02009;&#x02212;&#x02009;<italic>&#x003b4;</italic> and <italic>k</italic><sub><italic>i</italic></sub>&#x02009;+&#x02009;<italic>&#x003b4;</italic>). Athena takes <italic>&#x003b4;</italic> as a user input. The larger values of <italic>&#x003b4;</italic> allows Athena to search the <italic>k</italic>-mers space faster, but it has the downside of missing good values of <italic>k</italic><sup><italic>*</italic></sup>. The default value for <italic>&#x003b4;</italic> is 1. Notice that using <italic>&#x003b4;</italic>&#x02009;=&#x02009;1 is not the same as applying exhaustive search to all possible values of k, as with the hill-climbing technique that Athena uses, the search is terminated when the current <italic>k</italic>-mer is better than its neighbours (as shown in step 2 in Algorithm 2). In each iteration, we apply hill-climbing search to identify the next best value of <italic>k</italic><sub><italic>i</italic></sub> for the following iteration. The algorithm terminates whenever the Perplexity relative to <italic>k</italic><sub><italic>i</italic></sub> is less than the perplexities of both its neighbors or the maximum number of (user-defined) iterations is reached. However, all shown results are with respect to only one initial value (<italic>i.e</italic>., <italic>m</italic>&#x02009;=&#x02009;0 in <italic>k</italic><sub>0</sub>, <italic>k</italic><sub><italic>i</italic></sub>, &#x02026;, <italic>k</italic><sub><italic>m</italic></sub>).</p><sec id="Sec15"><title>Time and space complexity</title><p id="Par32">Because we apply hill climbing search to find the best value of <italic>k</italic>, the worst-case time complexity of the proposed algorithm is <italic>L</italic>&#x02009;&#x000d7;&#x02009;|<italic>S</italic>&#x02032;|, where <italic>L</italic> is the upper bound of the range of <italic>k</italic>-values to search for and |<italic>S</italic>&#x02032;| is size of selected sample. For the space complexity, Athena only needs to save the Perplexity values of previously investigated <italic>k</italic>-values, which is also linear in terms of <italic>L</italic>.<fig position="anchor" id="Figa"><label>Algorithm 1</label><caption><p>Correct Set of Reads.</p></caption><graphic position="anchor" xlink:href="41598_2019_52196_Figa_HTML" id="d29e1698"/></fig><fig position="anchor" id="Figb"><label>Algorithm 2</label><caption><p>Find Optimal <italic>k</italic>.</p></caption><graphic position="anchor" xlink:href="41598_2019_52196_Figb_HTML" id="d29e1710"/></fig></p></sec></sec></sec><sec id="Sec16"><title>Evaluation with Real Datasets</title><p id="Par33">In this section, we evaluate Athena variants separately by correcting errors in 6 real datasets and evaluating the quality of the resultant assembly.</p><sec id="Sec17"><title>Implementation notes and dataset</title><p id="Par34">We implement the N-Gram model using SRILM toolkit<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. SRILM is an open source toolkit that supports building statistical N-Gram LMs, in addition to evaluating the likelihood of new sequences using the Perplexity metric. For the RNN LM implementation, we build on the TensorFlow platform<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Specifically, we utilized Character-level RNN models, char-RNN for short, which takes a chunk of text and models the probability distribution of the next character in the sequence, given a sequence of previous characters. After correction, we run the Bowtie2 aligner<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> and measure the Alignment Rate and the Error Correction Gain (EC Gain). A higher value for either metric implies superior error correction. We do a sweep through a range of <italic>k</italic>-values and measure the alignment rate to determine if the Athena -generated <italic>k</italic>-value is optimal or its distance from optimality.</p><p id="Par35">For interpreting the execution time results, our experiments were performed on Dell Precision T3500 Workstation, with 8 CPU cores, each running at 3.2&#x02009;GHZ, 12GB RAM, and Ubuntu 16.04 Operating System. We use 3 EC tools, in pipeline mode with Athena, namely, Lighter, Blue, and RACER. Blue uses a <italic>k</italic>-mer consensus to target different kinds of errors, <italic>e.g</italic>., substitution, deletion and insertion errors, as well as uncalled bases (represented by N). This improves the performance of both alignment and assembly<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. In contrast, Lighter is much faster as it uses only a sample of <italic>k</italic>-mers to perform correction. We use the automated subsampling factor (called alpha) selection option in Lighter, which is estimated based on the genome size. Third, RACER uses a different configuration parameter distinct from the <italic>k</italic>-value, specifically Genome Length (GL), and we are able to tune GL as well. Our ability to tune any of these EC algorithm&#x02019;s parameters is in line with our vision and ongoing work to design extensible blocks of software to expedite algorithmic development in bioinformatics<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Incidentally, we started using another popular EC tool, Reptile, but it only allowed for a smaller range of <italic>k</italic>-values, beyond which it ran into out-of-memory errors. Hence, to demonstrate results with the full range of <italic>k</italic> values, we restricted ourselves to Lighter, Blue, and RACER. Our datasets are Illumina short reads (Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>), used in multiple prior studies (<italic>e.g</italic>.<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>). For these, there exist ground-truth reference genomes, which we use to evaluate the EC quality. The seven datasets have different read lengths (from 36&#x02009;bp to 250&#x02009;bp) and different error rates (from &#x0003c;3% to 43%).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Datasets&#x02019; description with coverage, number of reads, read lengths, genome type, and the Accesson number.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Coverage</th><th>#Reads</th><th>Read Length</th><th>Genome Type</th><th>Accession Number</th></tr></thead><tbody><tr><td>D1</td><td>80X</td><td>20.8M</td><td>136&#x02009;bp</td><td><italic>E. coli</italic> str. K-12 substr</td><td>SRR001665</td></tr><tr><td>D2</td><td>71X</td><td>7.1M</td><td>47&#x02009;bp</td><td><italic>E. coli</italic> str. K-12 substr</td><td>SRR022918</td></tr><tr><td>D3</td><td>173X</td><td>18.1M</td><td>36&#x02009;bp</td><td><italic>Acinetobacter</italic> sp. ADP1</td><td>SRR006332</td></tr><tr><td>D4</td><td>62X</td><td>3.5M</td><td>75&#x02009;bp</td><td>
<italic>B. subtilis</italic>
</td><td>DRR000852</td></tr><tr><td>D5</td><td>166X</td><td>7.1M</td><td>100&#x02009;bp</td><td><italic>L. interrogans C</italic> sp. ADP1</td><td>SRR397962</td></tr><tr><td>D6</td><td>70X</td><td>33.6M</td><td>250&#x02009;bp</td><td>
<italic>A. thaliana</italic>
</td><td>ERR2173372</td></tr><tr><td>D7</td><td>67X</td><td>202M</td><td>101&#x02009;bp</td><td>
<italic>Homo sapiens</italic>
</td><td>SRR1658570</td></tr></tbody></table><table-wrap-foot><p>Coverage is estimated according to Illumina&#x02019;s documentation<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec18"><title>Optimal parameter selection</title><p id="Par36">The results of using Athena with Lighter, Blue, and RACER tools are shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> for each dataset. The value of <italic>k</italic> found through exhaustive testing (or, GL for RACER) along with the EC quality is given first. Then it is noted when the Athena -selected configuration matches this theoretical best. We find that in 27 of 36 cases (75%) the configuration found by Athena matches the theoretical best. In the remaining cases, our chosen parameters are within 0.53% in overall alignment rate to the best values. We see that the optimal parameter value almost always corresponds to the lowest perplexity scores computed using Athena&#x02019;s language modeling (Tables&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref> and <xref rid="MOESM1" ref-type="media">2</xref> in Appendix). Further, the anti-correlation between perplexity and alignment rate holds for both optimal and non-optimal <italic>k</italic>-values. This shows that our hypothesis is valid across a range of <italic>k</italic>-values. We notice that the feasible range of <italic>k</italic>-values in Blue is (20, 32), distinct from Lighter&#x02019;s. Another interesting observation is that the optimal <italic>k</italic>-values are different across the two different EC tools, Lighter and Blue, for the same dataset, as observed before<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Athena can be applied to a different configuration parameter, GL for the RACER tool, in line with our design as a general-purpose tuning tool.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison of Lighter, Blue, and RACER using 7 datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th colspan="3">Exhaustive Search</th><th colspan="3">With Athena (RNN)</th><th colspan="3">With Athena (N-gram)</th></tr><tr><th colspan="10">Lighter</th></tr><tr><th/><th>Selected <italic>k</italic></th><th>Alignment Rate (%)</th><th>EC Gain (%)</th><th>Selected <italic>k</italic></th><th>Alignment Rate (%)</th><th>EC Gain (%)</th><th>Selected <italic>k</italic></th><th>Alignment Rate(%)</th><th>EC Gain (%)</th></tr></thead><tbody><tr><td>
<bold>D1</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>17</bold></td><td>98.95%</td><td>96.30%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D2</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>15</bold></td><td>61.42%</td><td>73.80%</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>17</bold></td><td>61.15%</td><td>80.10%</td><td colspan="3">Same as RNN</td></tr><tr><td>
<bold>D3</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>15</bold></td><td>80.44%</td><td>86.78%</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>17</bold></td><td>80.39%</td><td>95.34%</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D4</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>17</bold></td><td>93.95%</td><td>89.87%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D5</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>17</bold></td><td>92.15%</td><td>81.70%</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>25</bold></td><td>92.09%</td><td>83.80%</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D6</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>25</bold></td><td>86.16%</td><td>
<bold>NA</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>17</bold></td><td>85.63%</td><td>
<bold>NA</bold>
</td><td colspan="3">Same as RNN</td></tr><tr><td>
<bold>D7</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>15</bold></td><td>40.53%</td><td>37.58%</td><td colspan="3">Same as Exhaustive Search</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>17</bold></td><td>40.24%</td><td>7.70%</td></tr><tr><td colspan="10">
<bold>Blue</bold>
</td></tr><tr><td>
<bold>D1</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20</bold></td><td>99.53%</td><td>99%</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>25</bold></td><td>99.29%</td><td>98.60%</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D2</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20</bold></td><td>57.44%</td><td>4.61%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D3</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20</bold></td><td>84.17%</td><td>99.20%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D4</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20</bold></td><td>95.31%</td><td>98.50%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D5</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20</bold></td><td>92.33%</td><td>88.90%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D6</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>30</bold></td><td>86.18%</td><td>
<bold>NA</bold>
</td><td colspan="3">Same as Exhaustive Search</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>25</bold></td><td>86.07%</td><td>
<bold>NA</bold>
</td></tr><tr><td>
<bold>D7</bold>
</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>25</bold></td><td>17.19%</td><td>3.57%</td><td><bold>k</bold>&#x02009;<bold>=</bold>&#x02009;<bold>30</bold></td><td>16.96%</td><td>1.47%</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td colspan="10">
<bold>RACER</bold>
</td></tr><tr><td>
<bold>D1</bold>
</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>4.7M</bold></td><td>99.26%</td><td>84.80%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D2</bold>
</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>4.7M</bold></td><td>81.15%</td><td>92.90%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D3</bold>
</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>3.7M</bold></td><td>84.11%</td><td>88.27%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D4</bold>
</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>4.2M</bold></td><td>95.33%</td><td>97%</td><td colspan="3">Same as Exhaustive Search</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D5</bold>
</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>4.2M</bold></td><td>92.29%</td><td>81.63%</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20M</bold></td><td>92.28%</td><td>80.50%</td><td colspan="3">Same as Exhaustive Search</td></tr><tr><td>
<bold>D6</bold>
</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>120M</bold></td><td>86.36%</td><td>
<bold>NA</bold>
</td><td colspan="3">Same as Exhaustive Search</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20M</bold></td><td>86.12%</td><td>
<bold>NA</bold>
</td></tr><tr><td>
<bold>D7</bold>
</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>3M</bold></td><td>17.55%</td><td>21.10%</td><td><bold>GL</bold>&#x02009;<bold>=</bold>&#x02009;<bold>20M</bold></td><td>17.40%</td><td>26.50%</td><td colspan="3">Same as Exhaustive Search</td></tr></tbody></table><table-wrap-foot><p>This is for finding the best <italic>k</italic>-value (GL for RACER) using Athena variants <italic>vs</italic>. exhaustive search. We find either the optimal value or within 0.53% (over Alignment Rate) and within 8.5% (EC Gain) of the theoretical best (in the worst case), consistent with the reported results by Lighter (Figure 5 in<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>). These slightly sub-optimal configurations is due to the impact of sub-sampling. However, with appropriate sampling rate selection, A<bold><sc>thena</sc></bold> achieves configurations that is 0.53% of the oracle best configuration (found with exhaustive searching). We also notice that for RACER, GL found by Athena is within 3% of the reference GL (except for the RNN model with <italic>D5</italic>, which still achieves very close performance for both Alignment Rate and EC Gain).</p><p>The Gain metric is not shown for <italic>D6</italic> as the tool used to compute it was not able to handle reads of length 250&#x02009;bp. We notice that the best genome length found by Athena for D7 (human genome) is 20Mbp, which is very low compared to the actual human genome length (&#x02248;3&#x02009;k Mbp). This shows that using heuristics to estimate the optimal value of K based on only one parameter (genome length) can produce significantly suboptimal performance, even if the actual value of the genome length is provided. Moreover, GenomeLength parameter in Racer represents the approximate length of the DNA molecule that originated the reads. If only parts of a genome were sequenced, then only the total length of those parts should be used, instead of the length of the total genome. Dataset D7 is just for a part of the genome (24&#x02009;Mbp), and Athena &#x02018;s genome length selection of 20Mbp shows the efficacy of Athena for this usecase.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec19"><title>N-gram language model results</title><p id="Par37">We start by training an N-Gram language model from the original dataset. We divide each read into smaller segments (words) of length <italic>L</italic><sub><italic>s</italic></sub> (set to 7 by default). A careful reader may be concerned that selecting the best value of <italic>L</italic><sub><italic>s</italic></sub> is a difficult problem in itself, of the same order of difficulty as our original problem. Fortunately, this is <italic>not</italic> the case and <italic>L</italic><sub><italic>s</italic></sub> selection turns out to be a much simpler task. From domain knowledge, we know that if we use <italic>L</italic><sub><italic>s</italic></sub>&#x02009;=&#x02009;4 or less, the frequencies of the different words will be similar as this increases the overlap probability between the generated words, thus reducing the model&#x02019;s discriminatory power, while a large value will mean that the model&#x02019;s memory footprint increases. We find that for a standard desktop-class machine with 32&#x02009;GB of memory, <italic>L</italic><sub><italic>s</italic></sub>&#x02009;=&#x02009;8 is the maximum that can be accommodated. Further, we find that the model performance is <italic>not</italic> very sensitive in the range (5&#x02013;7), so we end up using <italic>L</italic><sub><italic>s</italic></sub>&#x02009;=&#x02009;7. The same argument holds for selecting a history of words, and we use a tri-gram model (history of 3 words, i.e., n&#x02009;=&#x02009;3) for all our experiments. Second, we compare the perplexity metric for datasets corrected with different <italic>k</italic>-values and compare the perplexity metric (without a reference genome) to the alignment rate (using a reference genome). We always report the <italic>average perplexity</italic>, which is just the total perplexity averaged across all words. Our results show a high negative correlation between the two metrics on the 7 datasets (&#x02264;&#x02212;0.930 for the first six datasets and &#x02264;&#x02212;0.723 for D7), as shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. To reiterate, the benefit of using the perplexity metric is that it can be computed without the ground truth and even where a reference genome is available, it is more computationally efficient than aligning to the reference and then computing the alignment rate.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of Overall Alignment Rate of Fiona versus RACER (with and without A<bold><sc>thena</sc></bold>&#x02019;s tuning).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>&#x02014;</th><th colspan="2">Correlation to Alignment</th><th colspan="3">Comparison with FIONA</th><th colspan="2">Assembly quality</th><th colspan="2">Runtime Improvement</th></tr><tr><th>Dataset</th><th>Correlation (N-Gram)</th><th>Correlation (RNN)</th><th>Fiona&#x02009;+&#x02009;Bowtie2 (Alignment Rate)</th><th>RACER w/o Athena&#x02009;+&#x02009;Bowtie2 (Alignment Rate)</th><th>RACER w/Athena&#x02009;+&#x02009;Bowtie2 (Alignment Rate)</th><th>NG50 of Velvet w/o EC</th><th>NG50 of Velvet w/(Racer&#x02009;+&#x02009;Athena)</th><th>Athena</th><th>Bowtie2</th></tr></thead><tbody><tr><td>D1</td><td>&#x02212;0.977</td><td>&#x02212;0.938</td><td>99.25%</td><td>85.01%</td><td><bold>99.26</bold>%</td><td>3019</td><td>6827 (2.26X)</td><td>1&#x02009;m 38&#x02009;s</td><td>10&#x02009;m 5&#x02009;s</td></tr><tr><td>D2</td><td>&#x02212;0.981</td><td>&#x02212;0.969</td><td>73.75%</td><td>58.66%</td><td><bold>81.15</bold>%</td><td>47</td><td>2164 (46X)</td><td>49&#x02009;s</td><td>3&#x02009;m 53&#x02009;s</td></tr><tr><td>D3</td><td>&#x02212;0.982</td><td>&#x02212;0.968</td><td>83.12%</td><td>80.79%</td><td><bold>84.11</bold>%</td><td>1042</td><td>4164 (4X)</td><td>1&#x02009;m 39&#x02009;s</td><td>7&#x02009;m 50&#x02009;s</td></tr><tr><td>D4</td><td>&#x02212;0.946</td><td>&#x02212;0.930</td><td>
<bold>95.33%</bold>
</td><td>93.86%</td><td>
<bold>95.33%</bold>
</td><td>118</td><td>858 (7.27X)</td><td>52&#x02009;s</td><td>3&#x02009;m 8&#x02009;s</td></tr><tr><td>D5</td><td>&#x02212;0.970</td><td>&#x02212;0.962</td><td><bold>92.34</bold>%</td><td>90.91%</td><td>92.29%</td><td>186</td><td>2799 (15X)</td><td>1&#x02009;m 40&#x02009;s</td><td>9&#x02009;m 42&#x02009;s</td></tr><tr><td>D6</td><td>&#x02212;0.944</td><td>&#x02212;0.979</td><td><bold>87.43</bold>%</td><td>85.76%</td><td>86.84%</td><td>1098</td><td>1237 (1.12X)</td><td>6&#x02009;m 40&#x02009;s</td><td>1&#x02009;h 42&#x02009;m</td></tr><tr><td>D7</td><td>&#x02212;0.723</td><td>&#x02212;0.862</td><td>NA</td><td>17.17%</td><td>17.55%</td><td>723</td><td>754 (1.04X)</td><td>16&#x02009;m</td><td>71&#x02009;m</td></tr></tbody></table><table-wrap-foot><p>RACER requires the user to enter a value for the &#x0201c;Genome Length&#x0201d;, which has no default value. Therefore, &#x0201c;RACER w/o Athena&#x0201d; is RACER operating with a fixed Genome Length of 1M. Columns 5 &#x00026; 6 demonstrate the strong anti-correlation values between Perplexity and Alignment Rate. The last two columns show the assembly quality (in terms of NG50) before and after correction by RACER, tuned with Athena. Improvements in NG50 are shown between parentheses, while NGA50 and the amount of assembly errors metrics showed similar improvements and hence omitted. We also show the search time comparison for estimating the perplexity metric with Athena (N-gram) for a point in search space <italic>vs</italic>. estimating overall alignment rate with Bowtie2.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec20"><title>Char-RNN language model results</title><p id="Par38">For training our RNN, we used the &#x0201c;tensorflow-char-rnn&#x0201d; library<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. After parameter tuning, we use the following architecture for our experiments: 2 hidden layers with 300 neurons per layer, output layer with size 4 (<italic>i.e</italic>., corresponding to the four base pairs), mini-batch size 200, and learning rate 2<italic>e</italic><sup>&#x02212;3</sup> respectively. For each of the 7 datasets, we used 90% for training and 10% for validation, with no overlap, for coming up with the optimal RNN architecture.</p><p id="Par39">For our char-RNN results, we find that the perplexity metric has a strong negative relation to the overall alignment rate (Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>), with the absolute value of the correlation always greater than 0.86. Here, we have to sample the corrected set for calculating the perplexity measure because using an RNN to perform the calculation is expensive. This is because it involves, for predicting each character, doing 4 feed-forward passes (corresponding to the one-hot encodings for A, T, G, or C), each through 600 neurons. Empirically, for a test sample size of 50&#x02009;K, this translates to approximately 30&#x02009;minutes on a desktop-class machine. In the experiments with the real datasets, we use 50&#x02009;<italic>K</italic> samples with uniform sampling, and in synthetic experiments, we use 100&#x02009;<italic>K</italic> samples (i.e., only 1% of the dataset). Importantly, the strong quantitative relationship between perplexity and the EC quality is maintained even at the low sampling rate (0.5% for real dataset). Note that this test sample size is different from the sample size used by the EC tool to perform the correction, shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, which must be at least of coverage 30X to ensure accurate <italic>k</italic>-mer analysis<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>.</p></sec><sec id="Sec21"><title>Comparison with a self-tuning EC tool</title><p id="Par40">Here, we compare Athena with the EC tool, Fiona<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, which estimates its parameters internally. The purpose of this comparison is to show that Athena can tune <italic>k</italic>-mer-based approaches (RACER specifically for this experiment) to achieve comparable performance to suffix array-based approaches (<italic>e.g</italic>., Fiona), reducing the gap between the two approaches.</p><p id="Par41">The works by<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> and<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> show a similar comparison between different EC approaches concluding that the automatic selection of configuration parameters, based on the datasets, is crucial for EC performance. However, they do not perform such parameter tuning automatically. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> presents the overall alignment rate for our 6 evaluation datasets, calculated after doing correction by Fiona. We notice that RACER, when tuned with Athena, outperforms automatic tuning by Fiona in 3 of the 7 datasets (i.e., by about 0.01%, 7.4% and 1% on <italic>D</italic>1, <italic>D</italic>2, and <italic>D</italic>3 respectively), while they are equal in one dataset. Finally, Fiona is better on <italic>D</italic>5 by 0.05% and on <italic>D</italic>6 by 0.59%. Notice that Racer&#x02019;s runtime is 5&#x02013;6X faster compared to Fiona, which is similar to the runtimes reported in<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Moreover, we omit the result of Fiona with D7 as the tool took more than 6&#x02009;hours without producing the corrected reads.</p></sec><sec id="Sec22"><title>Impact on assembly quality and searching time</title><p id="Par42">Here we show the impact on genome assembly quality of using an EC tool tuned with Athena. We use Velvet<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> to perform the assembly and QUAST<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> to evaluate the assembly quality. We compare the NG50 before and after correction done by RACER using the best GL found by Athena. The results (Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>) show a significant improvement on NG50 by 2.26X, 46X, 4X, 7.27X, 15X, 1.2X, and 1.04X respectively. For <italic>D7</italic>, the improvement is the lowest, since <italic>D7</italic> has the lowest alignment rate across all datasets. We also collect the NGA50 scores and it shows identical improvements as the NG50. These improvements are consistent with what was reported in<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>, which measured improvement due to the use of EC tools with manually tuned configuration parameters.</p><sec id="Sec23"><title>Search time improvement with Athena</title><p id="Par43">Consider that in our problem statement, we are trying to search through a space of configuration parameters in order to optimize a metric (EC Gain or Alignment Rate). The search space can be large and since the cost of searching shows up as a runtime delay, it is important to reduce the time that it takes to evaluate that metric of each search point. Although EC tools don&#x02019;t need a reference genome to operate, current state-of-the-art method use a reference genome to tune EC performance. As shown in<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>, the best value of configuration parameter <italic>k</italic> is found by iteratively picking a single <italic>k</italic>-value, run the EC tool with that value, then perform alignment (with one of several available tools such as Bowtie2), and finally compute the metric for that value. In contrast, with Athena, to explore one point in the search space, we run the EC algorithm with the <italic>k</italic>-value, and then compute the Perplexity metric, which does <italic>not</italic> involve the time-consuming alignment step. Here, we evaluate the relative time spent in exploring one point in the search space using Athena vis-<italic>&#x000e0;</italic>-vis the current state-of-the-art. The result is shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. For this comparison, the alignment is done by Bowtie2<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. We find that using the baseline approach, each step in the search takes respectively 6.2X, 4.8X, and 4.7X, 3.6X, 5.82X, 15.3X, and 4.4X longer for the 7 datasets. That is because the time taken by the LM to calculate the perplexity is linear in terms of the input (number of reads x read length), while the runtimes of alignment algorithms are superlinear<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>.</p><p id="Par44">Further, while we use the hill-climbing technique to search through the space, today&#x02019;s baseline methods use exhaustive search, such as in Lighter<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> and thus the end-to-end runtime advantage of Athena will be magnified.</p></sec></sec><sec id="Sec24"><title>Impact of sub-sampling</title><p id="Par45">One drawback of sub-sampling is the reduction of coverage, which can negatively impact the accuracy of k-mer analysis. Therefore, we select the sub-sampling ratio so that the sample size has a coverage of at least 30X, which was found sufficient by several prior works (such as<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>) for accurate k-mer analysis. When the coverage is less than 30X, the distribution of solid <italic>k</italic>-mers <italic>vs</italic>. non-solid <italic>k</italic>-mers becomes very close, for which EC tools (<italic>e.g</italic>., Lighter with dataset D2) will not perform any correction for any given value of <italic>k</italic>. So it is a limitation in EC tools in general to require higher coverage for accurate error correction. For datasets with coverage less than 30X, we use the complete dataset without any subsampling.</p><p id="Par46">We use the well-known Lander-Waterman&#x02019;s formula<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> to estimate the required sample-size to reach such coverage. If the data coverage is less than 30X, the whole dataset is used.</p><p id="Par47">We also notice from Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> that Athena sometimes proposes slightly sub-optimal configurations compared to oracle best (within 0.53% in terms of overall alignment rate). This is due to the impact of sub-sampling as the best configuration for the sample can be slightly different from the best configuration for the complete dataset. We study the impact of sub-sampling on the accuracy of Athena. We use <italic>D2</italic> for this experiment as it is the one with the highest error rate across all 7 datasets. First, we select two random samples from <italic>D2</italic> with sampling rates of 35% (30X) and 70% (50X) respectively and investigate the correlation between perplexity and EC gain. As shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, perplexity and EC gain have a very high inverse correlation, for all three tools, across the entire range of <italic>k</italic>-values. This shows that sampling in Athena has the advantage of significant improvements in runtime, while preserving the accuracy of optimal parameter selection.<fig id="Fig3"><label>Figure 3</label><caption><p>Impact of sub-sampling on perplexity and gain. We compare the perplexity and gain with samples of sizes 35% and 70% of the <italic>D2</italic> dataset. We observe the negative correlation between both metrics and also the positive correlation between the values of each metric on the two samples.</p></caption><graphic xlink:href="41598_2019_52196_Fig3_HTML" id="d29e3487"/></fig></p><p id="Par48">Second, we select a random sample from <italic>D2</italic> with a sampling rate of 40%. We scan over the range of values of the tuning parameter (<italic>k</italic>-mer size for Lighter and Blue, Genome length for RACER) and calculate perplexity and EC gain for each value. We repeat this experiment with 70 different random samples and find that the selected value did not change in all runs, which also matches the best value with exhaustive searching over the complete dataset. This shows that the sub-sampling does not overfit the data and preserves the accuracy of the optimal parameter selection.</p></sec></sec><sec id="Sec25"><title>Evaluation with Synthetically Injected Errors</title><p id="Par49">Here we experiment on datasets where we synthetically inject errors of three kinds - insertion, deletion, and substitution. The real data sets used in our evaluation in Section belonged to Illumina sequencing platform and therefore had primarily substitution errors (about 99% of all errors). However, other platforms such as 454 or Ion torrent sequencing suffers primarily from insertions and deletions<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Hence our synthetic injections are meant to uncover if the relationship between perplexity and error rate holds for these other error types. We start by randomly collecting 100&#x02009;K short reads from the reference genome (<italic>i.e</italic>., almost error-free) for two organisms used in the real datasets&#x02013;<italic>E. coli</italic> (D1, D2) and <italic>Acinetobacter</italic> (D3). Afterward, we inject errors of each type as follows:<list list-type="order"><list-item><p id="Par50"><bold>Deletion:</bold> We select the index of injection (position of the deleted segment), which follows a uniform distribution U(0, <italic>L</italic>&#x02009;&#x02212;&#x02009;<italic>d</italic>), where <italic>L</italic> is the length of the read and <italic>d</italic> is the length to delete.</p></list-item><list-item><p id="Par51"><bold>Insertion:</bold> Similar to deletion errors, the index of insertion follows a uniform distribution U(0, <italic>L</italic>&#x02009;&#x02212;&#x02009;<italic>I</italic>), where <italic>I</italic> is the length of inserted segment. Moreover, each base pair in the inserted segment follows a uniform distribution over the four base pairs (A, C, G, or T).</p></list-item><list-item><p id="Par52"><bold>Substitution:</bold> For this type of synthetic errors, we select <italic>k</italic> positions of substitutions following a uniform distribution U(0, <italic>L</italic>). Then, we substitute each base pair of the <italic>k</italic> positions with another base pair following a uniform distribution over the four base pairs (A, C, G, or T). Note that there is one-in-four chance that the substitution results in the same base pair.</p></list-item></list></p><p id="Par53">We test the proposed approach against two levels of synthetic errors: low error rate (Uniform in (1, 5) bp in error per read), and high error rate (Uniform in (6, 10) bp in error per read). In all cases, the language model is trained using the original data set, mentioned in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>, with the natural ambient rate of error. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> shows the results for the perplexity metric in the data set <italic>after</italic> error injection (<italic>i.e</italic>., without doing correction), for the three error types and an equi-probable mixture of the three. The results show that for all types of errors, the direct quantitative relation between error rate and perplexity holds&#x02013;the perplexity values are higher for high error rate injection. One observation is that the values of perplexity for insertion and substitution errors are higher than for deletion errors. For example, insertion and substitution perplexities are 3X &#x00026; 4.7X the deletion perplexity for high error rate data sets, and 2X &#x00026; 1.5X for low error rate data sets. This is expected because in deletion errors, the segment of the read after the index of deletion is a correct segment and hence is expected by the language model. On the other hand, for insertion and substitutions, the added synthetic base pairs will most likely construct wrong segments. Such segments will have very low counts in the language model and therefore produce higher overall perplexities. Another observation is that the perplexities for the injection into data set D1 are higher than for D3. This is likely because D3 had a higher natural ambient rate of error and hence the additional injection does not cause as much increase in the perplexity metric.<fig id="Fig4"><label>Figure 4</label><caption><p>N-Gram (Figure <bold>A</bold>,<bold>B</bold>) and RNN (Figure <bold>C</bold>,<bold>D</bold>) Perplexity metric for different types of synthetic errors: Indels and Substitution errors, and a mixture of the three for <italic>E. coli</italic> str reference genome (Figure <bold>A</bold>,<bold>C</bold>) and <italic>Acinetobacter</italic> sp. reference genome (Figure <bold>B</bold>,<bold>D</bold>). We compare two versions of such errors: high and low error rates.</p></caption><graphic xlink:href="41598_2019_52196_Fig4_HTML" id="d29e3622"/></fig></p></sec><sec id="Sec26"><title>Related Work</title><sec id="Sec27"><title>Error correction approaches</title><p id="Par54">EC tools can be mainly divided into three categories: <italic>k</italic>-spectrum based, suffix tree/array-based, and multiple sequence alignment-based (MSA) methods. Each tool takes one or more configuration parameters. While we have experimented with Athena applied to the first kind, with some engineering effort, it can be applied to tuning tools that belong to the other two categories.</p></sec><sec id="Sec28"><title>Language modeling in genomics</title><p id="Par55">In the genomics domain, LM was used in<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> to find the characteristics of organisms in which N-Gram analysis was applied to 44 different bacterial and archaeal genomes and to the human genome. In subsequent work, they used N-Gram-based LM for extracting patterns from whole genome sequences. Others<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> have used LM to enhance domain recognition in protein sequences. For example<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, has used N-Gram analysis specifically to create a Bayesian classifier to predict the localization of a protein sequence over 10 distinct eukaryotic organisms. RNNs can be thought of as a generalization of Hidden Markov Models (HMMs) and HMMs have been applied in several studies that seek to annotate epigenomic data. For example<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, presents a fast method using spectral learning with HMMs for annotating chromatin states in the human genome. Thus, we are seeing a steady rise in the use of ML techniques, traditionally used in NLP, being used to make sense of -omics data.</p></sec><sec id="Sec29"><title>Automatic parameter tuning</title><p id="Par56">used a Feature-based Accuracy Estimator as a parameter advisor for the Opal aligner software<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>. The field of computer systems has had several successful solutions for automatic configuration tuning of complex software systems. Our own work<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> plus others<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> have shown how to do this for distributed databases, while other works have done this for distributed computing frameworks like Hadoop<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup> or cloud configurations<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. We take inspiration from them but our constraints and requirements are different (such as, avoiding reliance on ground truth corrected sequences).</p></sec></sec><sec id="Sec30" sec-type="discussion"><title>Discussion</title><p id="Par57">The space to search for finding the optimal configuration is non-convex in general. Therefore, it is possible to get stuck in a local minima, and hence, we use multiple random initializations. However, in our evaluation, we find that a single initialization suffices. Some EC tools have a number of performance-sensitive configuration parameters with interdependencies. There, systems such as Rafiki<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> can encode the dependencies, while relying on Athena &#x02018;s LM to compute the corresponding performance metric, converging toward optimal parameters. With some engineering effort, Athena can be used to optimize the <italic>k</italic>-value in DBG-based assemblers as well, though there will be different optimization passes since the optimal values are likely to be different for the error correction and assembly stages.</p><p id="Par58">Finally, a careful reader may wonder if we can use copy number of all solid <italic>k</italic>-mers instead of the perplexity metric. The problem with this approach is that it will require a predefined frequency threshold to identify the solid <italic>k</italic>-mers. Using the perplexity metric, there is no need for such a threshold. Also the perplexity metric takes into account the context of the <italic>k</italic>-mer (<italic>i.e</italic>., previous and subsequent <italic>k</italic>-mers) in deciding the accuracy of the EC tool output. Also, notice that our main target is to find the optimal <italic>k</italic>-mer size, and different <italic>k</italic>-mers will have different thresholds as well.</p></sec><sec id="Sec31" sec-type="conclusion"><title>Conclusion</title><p id="Par59">The performance of most EC tools for NGS reads is highly dependent on the proper choice of its configuration parameters, <italic>e.g</italic>., <italic>k</italic>-value selection in <italic>k</italic>-mer based techniques. It is computationally expensive to search through the entire range of parameters to determine the optimal value, which varies from one dataset to another. Using our Athena suite, we target the problem of automatically tuning these parameters using language modeling techniques from the NLP domain <italic>without</italic> the need for a ground truth genome. Through N-Gram and char-RNN language modeling, we compute the &#x0201c;perplexity&#x0201d; metric, a novel one for this problem domain, and find that the metric has a high negative correlation with the quality of genomic assembly and can be computed efficiently without using a reference genome. The perplexity metric then guides a hill climbing-based search toward the best <italic>k</italic>-value. We evaluate Athena with 6 different real datasets, plus with synthetically injected errors. We find that the predictive performance of the perplexity metric is maintained under all scenarios. Further, using the perplexity metric, Athena can search for and arrive at the best <italic>k</italic>-value, or within 0.53% of the assembly quality obtained using brute force. Athena suggests a <italic>k</italic>-value within the top-3 best <italic>k</italic>-values for N-Gram models and the top-5 best <italic>k</italic>-values for RNN models (the top values were determined by brute force searching).</p></sec><sec sec-type="supplementary-material"><title>Supplementary information</title><sec id="Sec32"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2019_52196_MOESM1_ESM.pdf"><caption><p>Appendix</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Mustafa Abdallah and Ashraf Mahgoub.</p></fn></fn-group><sec><title>Supplementary information</title><p>is available for this paper at 10.1038/s41598-019-52196-4.</p></sec><ack><title>Acknowledgements</title><p>This work is supported in part by the NIH R01 Grant 1R01AI123037, a Lilly Endowment grant, a gift from Adobe Research, and funding from Purdue&#x02019;s College of Engineering and Department of Ag. and Biological Engineering. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>M.A., A.M., H.A., and S.C. participated in the computational analyses. M.A., A.M. and S.C. wrote the manuscript. S.C. provided overall guidance and funding for the project. All authors read and edited the final manuscript.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par61">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Mahadik, K., Wright, C., Kulkarni, M., Bagchi, S. &#x00026; Chaterji, S. Scalable genomic assembly through parallel de bruijn graph construction for multiple k-mers. In <italic>Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</italic>, 425&#x02013;431 (ACM, 2017).</mixed-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szalay</surname><given-names>T</given-names></name><name><surname>Golovchenko</surname><given-names>JA</given-names></name></person-group><article-title>De novo sequencing and variant calling with nanopores using poreseq</article-title><source>Nat. biotechnology</source><year>2015</year><volume>33</volume><fpage>1087</fpage><pub-id pub-id-type="doi">10.1038/nbt.3360</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Sameith, K., Roscito, J. G. &#x00026; Hiller, M. Iterative error correction of long sequencing reads maximizes accuracy and improves contig assembly. <italic>Briefings bioinformatics</italic> bbw003 (2016).</mixed-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Dorman</surname><given-names>KS</given-names></name><name><surname>Aluru</surname><given-names>S</given-names></name></person-group><article-title>Reptile: representative tiling for short read error correction</article-title><source>Bioinforma.</source><year>2010</year><volume>26</volume><fpage>2526</fpage><lpage>2533</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btq468</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Schatz</surname><given-names>MC</given-names></name><name><surname>Salzberg</surname><given-names>SL</given-names></name></person-group><article-title>Quake: quality-aware detection and correction of sequencing errors</article-title><source>Genome biology</source><year>2010</year><volume>11</volume><fpage>R116</fpage><pub-id pub-id-type="doi">10.1186/gb-2010-11-11-r116</pub-id><?supplied-pmid 21114842?><pub-id pub-id-type="pmid">21114842</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>L</given-names></name><name><surname>Florea</surname><given-names>L</given-names></name><name><surname>Langmead</surname><given-names>B</given-names></name></person-group><article-title>Lighter: fast and memory-efficient sequencing error correction without counting</article-title><source>Genome biology</source><year>2014</year><volume>15</volume><fpage>509</fpage><pub-id pub-id-type="doi">10.1186/s13059-014-0509-9</pub-id><?supplied-pmid 25398208?><pub-id pub-id-type="pmid">25398208</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenfield</surname><given-names>P</given-names></name><name><surname>Duesing</surname><given-names>K</given-names></name><name><surname>Papanicolaou</surname><given-names>A</given-names></name><name><surname>Bauer</surname><given-names>DC</given-names></name></person-group><article-title>Blue: correcting sequencing errors using consensus and context</article-title><source>Bioinforma.</source><year>2014</year><volume>30</volume><fpage>2723</fpage><lpage>2732</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu368</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Kao, W.-C., Chan, A. H. &#x00026; Song, Y. S. Echo: a reference-free short-read error correction algorithm. <italic>Genome research</italic> (2011).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Peng, Y., Leung, H. C., Yiu, S.-M. &#x00026; Chin, F. Y. Idba&#x02013;a practical iterative de bruijn graph de novo assembler. In <italic>RECOMB</italic>, 426&#x02013;440 (2010).</mixed-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Chockalingam</surname><given-names>SP</given-names></name><name><surname>Aluru</surname><given-names>S</given-names></name></person-group><article-title>A survey of error-correction methods for next-generation sequencing</article-title><source>Briefings bioinformatics</source><year>2012</year><volume>14</volume><fpage>56</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1093/bib/bbs015</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chikhi</surname><given-names>R</given-names></name><name><surname>Medvedev</surname><given-names>P</given-names></name></person-group><article-title>Informed and automated k-mer size selection for genome assembly</article-title><source>Bioinforma.</source><year>2013</year><volume>30</volume><fpage>31</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt310</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heydari</surname><given-names>M</given-names></name><name><surname>Miclotte</surname><given-names>G</given-names></name><name><surname>Demeester</surname><given-names>P</given-names></name><name><surname>Van de Peer</surname><given-names>Y</given-names></name><name><surname>Fostier</surname><given-names>J</given-names></name></person-group><article-title>Evaluation of the impact of illumina error correction tools on de novo genome assembly</article-title><source>BMC bioinformatics</source><year>2017</year><volume>18</volume><fpage>374</fpage><pub-id pub-id-type="doi">10.1186/s12859-017-1784-8</pub-id><?supplied-pmid 28821237?><pub-id pub-id-type="pmid">28821237</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Meyer, F. <italic>et al</italic>. Mg-rast version 4&#x02014;lessons learned from a decade of low-budget ultra-high-throughput metagenome analysis. <italic>Briefings Bioinforma</italic>. <bold>105</bold> (2017).</mixed-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ilie</surname><given-names>L</given-names></name><name><surname>Molnar</surname><given-names>M</given-names></name></person-group><article-title>Racer: Rapid and accurate correction of errors in reads</article-title><source>Bioinforma.</source><year>2013</year><volume>29</volume><fpage>2490</fpage><lpage>2493</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt407</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Clark, S. C., Egan, R., Frazier, P. I. &#x00026; Wang, Z. Ale: a generic assembly likelihood evaluation framework for assessing the accuracy of genome and metagenome assemblies. <italic>Bioinforma</italic>. 29, 435&#x02013;443 (2013).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Fabri, A. &#x00026; Teillaud, M. Cgal-the computational geometry algorithms library. In <italic>10e colloque national en calcul des structures</italic>, <bold>6</bold> (2011).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Elaraby, M. S., Abdallah, M., Abdou, S. &#x00026; Rashwan, M. A deep neural networks (dnn) based models for a computer aided pronunciation learning system. In <italic>International Conference on Speech and Computer</italic>, 51&#x02013;58 (Springer, 2016).</mixed-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhai</surname><given-names>C</given-names></name></person-group><article-title>Statistical language models for information retrieval</article-title><source>Synth. Lect. on Hum. Lang. Technol.</source><year>2008</year><volume>1</volume><fpage>1</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.2200/S00158ED1V01Y200811HLT001</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>PF</given-names></name><name><surname>Desouza</surname><given-names>PV</given-names></name><name><surname>Mercer</surname><given-names>RL</given-names></name><name><surname>Pietra</surname><given-names>VJD</given-names></name><name><surname>Lai</surname><given-names>JC</given-names></name></person-group><article-title>Class-based n-gram models of natural language</article-title><source>Comput. linguistics</source><year>1992</year><volume>18</volume><fpage>467</fpage><lpage>479</lpage></element-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Graves, A. Generating sequences with recurrent neural networks. <italic>arXiv preprint arXiv:1308.0850</italic> (2013).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Azzopardi, L., Girolami, M. &#x00026; Van Rijsbergen, K. <italic>Investigating the relationship between language model perplexity and ir precision-recall measures</italic>. (2003).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Inan, H., Khosravi, K. &#x00026; Socher, R. Tying word vectors and word classifiers: A loss framework for language modeling. <italic>arXivpreprint arXiv:1611.01462</italic> (2016).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Kombrink, S., Mikolov, T., Karafi&#x000e1;t, M. &#x00026; Burget, L. Recurrent neural network based language modeling in meeting recognition. In <italic>Twelfth annual conference of the international speech communication association</italic> (2011).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Stolcke, A. Srilm &#x02013; an extensible language modeling toolkit. In <italic>ICSLP</italic>, 901&#x02013;904 (2002).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Abadi, M. <italic>et al</italic>. Tensorflow: A system for large-scale machine learning. In <italic>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</italic>, 265&#x02013;283 (2016).</mixed-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langmead</surname><given-names>B</given-names></name><name><surname>Salzberg</surname><given-names>SL</given-names></name></person-group><article-title>Fast gapped-read alignment with bowtie 2</article-title><source>Nat. methods</source><year>2012</year><volume>9</volume><fpage>357</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1923</pub-id><?supplied-pmid 22388286?><pub-id pub-id-type="pmid">22388286</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Mahadik, K. <italic>et al</italic>. Sarvavid: a domain specific language for developing scalable computational genomics applications. In <italic>Proceedings of the 2016 International Conference on Supercomputing</italic>, 34 (ACM, 2016).</mixed-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schr&#x000f6;der</surname><given-names>J</given-names></name><name><surname>Schr&#x000f6;der</surname><given-names>H</given-names></name><name><surname>Puglisi</surname><given-names>SJ</given-names></name><name><surname>Sinha</surname><given-names>R</given-names></name><name><surname>Schmidt</surname><given-names>B</given-names></name></person-group><article-title>Shrec: a short-read error correction method</article-title><source>Bioinforma.</source><year>2009</year><volume>25</volume><fpage>2157</fpage><lpage>2163</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp379</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Guo, H. <italic>et al</italic>. degsm: memory scalable construction of large scale de bruijn graph. <italic>bioRxiv</italic> 388454 (2018).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Liu, B. <italic>et al</italic>. Estimation of genomic characteristics by analyzing k-mer frequency in de novo genome projects. <italic>arXivpreprint arXiv:1308.2012</italic> (2013).</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>MH</given-names></name><etal/></person-group><article-title>Fiona: a parallel and automatic strategy for read error correction</article-title><source>Bioinforma.</source><year>2014</year><volume>30</volume><fpage>i356</fpage><lpage>i363</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu440</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molnar</surname><given-names>M</given-names></name><name><surname>Ilie</surname><given-names>L</given-names></name></person-group><article-title>Correcting illumina data</article-title><source>Briefings bioinformatics</source><year>2014</year><volume>16</volume><fpage>588</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1093/bib/bbu029</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allam</surname><given-names>A</given-names></name><name><surname>Kalnis</surname><given-names>P</given-names></name><name><surname>Solovyev</surname><given-names>V</given-names></name></person-group><article-title>Karect: accurate correction of substitution, insertion and deletion errors for next-generation sequencing data</article-title><source>Bioinforma.</source><year>2015</year><volume>31</volume><fpage>3421</fpage><lpage>3428</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btv415</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Zerbino, D. &#x00026; Birney, E. Velvet: algorithms for de novo short read assembly using de bruijn graphs. <italic>Genome research gr</italic>&#x02013;074492 (2008).</mixed-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurevich</surname><given-names>A</given-names></name><name><surname>Saveliev</surname><given-names>V</given-names></name><name><surname>Vyahhi</surname><given-names>N</given-names></name><name><surname>Tesler</surname><given-names>G</given-names></name></person-group><article-title>Quast: quality assessment tool for genome assemblies</article-title><source>Bioinforma.</source><year>2013</year><volume>29</volume><fpage>1072</fpage><lpage>1075</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt086</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baichoo</surname><given-names>S</given-names></name><name><surname>Ouzounis</surname><given-names>CA</given-names></name></person-group><article-title>Computational complexity of algorithms for sequence comparison, short-read assembly and genome alignment</article-title><source>Biosyst.</source><year>2017</year><volume>156</volume><fpage>72</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.biosystems.2017.03.003</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lander</surname><given-names>ES</given-names></name><name><surname>Waterman</surname><given-names>MS</given-names></name></person-group><article-title>Genomic mapping by fingerprinting random clones: a mathematical analysis</article-title><source>Genomics</source><year>1988</year><volume>2</volume><fpage>231</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/0888-7543(88)90007-9</pub-id><?supplied-pmid 3294162?><pub-id pub-id-type="pmid">3294162</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Ganapathiraju, M. K. <italic>et al</italic>. Comparative n-gram analysis of whole-genome sequences. 2nd <italic>Int. Conf. on Hum. Lang. Technol. Res. (HLT)</italic> 76&#x02013;81 (2002).</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coin</surname><given-names>L</given-names></name><name><surname>Bateman</surname><given-names>A</given-names></name><name><surname>Durbin</surname><given-names>R</given-names></name></person-group><article-title>Enhanced protein domain discovery by using language modeling techniques from speech recognition</article-title><source>Proc. Natl. Acad. Sci.</source><year>2003</year><volume>100</volume><fpage>4516</fpage><lpage>4520</lpage><pub-id pub-id-type="doi">10.1073/pnas.0737502100</pub-id><?supplied-pmid 12668763?><pub-id pub-id-type="pmid">12668763</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>BR</given-names></name><name><surname>Guda</surname><given-names>C</given-names></name></person-group><article-title>ngloc: an n-gram-based bayesian method for estimating the subcellular proteomes of eukaryotes</article-title><source>Genome biology</source><year>2007</year><volume>8</volume><fpage>R68</fpage><pub-id pub-id-type="doi">10.1186/gb-2007-8-5-r68</pub-id><?supplied-pmid 17472741?><pub-id pub-id-type="pmid">17472741</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>KC</given-names></name></person-group><article-title>Spectacle: fast chromatin state annotation using spectral learning</article-title><source>Genome biology</source><year>2015</year><volume>16</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1186/s13059-015-0598-0</pub-id><pub-id pub-id-type="pmid">25583448</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">DeBlasio, D. &#x00026; Kececioglu, J. Parameter advising for multiple sequence alignment. In <italic>BMC bioinformatics</italic>, vol. 16, A3 (BioMed Central, 2015).</mixed-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheeler</surname><given-names>TJ</given-names></name><name><surname>Kececioglu</surname><given-names>JD</given-names></name></person-group><article-title>Multiple alignment by aligning alignments</article-title><source>Bioinforma.</source><year>2007</year><volume>23</volume><fpage>i559</fpage><lpage>i568</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btm226</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Mahgoub, A. <italic>et al</italic>. Rafiki: a middleware for parameter tuning of nosql datastores for dynamic metagenomics workloads. In <italic>Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference</italic>, 28&#x02013;40 (ACM, 2017).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Van Aken, D., Pavlo, A., Gordon, G. J. &#x00026; Zhang, B. Automatic database management system tuning through large-scale machine learning. In Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD), 1009&#x02013;1024 (ACM, 2017).</mixed-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bei</surname><given-names>Z</given-names></name><etal/></person-group><article-title>RFHOC: A Random-Forest Approach to Auto-Tuning Hadoop&#x02019;s Configuration</article-title><source>IEEE Transactions on Parallel Distributed Syst.</source><year>2016</year><volume>27</volume><fpage>1470</fpage><lpage>1483</lpage><pub-id pub-id-type="doi">10.1109/TPDS.2015.2449299</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Li, M. et al. Mronline: Mapreduce online performance tuning. In <italic>Proceedings of the 23rd international symposium on High-performance parallel and distributed computing</italic>, 165&#x02013;176 (ACM, 2014).</mixed-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipourfard</surname><given-names>O</given-names></name><etal/></person-group><article-title>Cherrypick: Adaptively unearthing the best cloud configurations for big data analytics</article-title><source>NSDI</source><year>2017</year><volume>2</volume><fpage>4</fpage><lpage>2</lpage></element-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Illumina. Estimating Sequencing Coverage.</mixed-citation></ref></ref-list></back></article>