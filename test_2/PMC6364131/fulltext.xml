<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.5.0//EN//XML?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName art550.dtd?><?SourceDTD.Version 5.5.0?><?ConverterInfo.XSLTName elsevier2nlmx2.xsl?><?ConverterInfo.Version 1?><?origin publisher?><?FILEmeta_GPB323 xml ?><?FILEmain xml ?><?FILEmain pdf ?><?FILEgr1 jpg ?><?FILEgr2 jpg ?><?FILEgr3 jpg ?><?FILEgr4 jpg ?><?FILEgr5 jpg ?><?FILEgr6 jpg ?><?FILEmmc1 pdf ?><?FILEsi1 gif ?><?FILEsi2 gif ?><?FILEsi3 gif ?><?FILEsi4 gif ?><?FILEsi5 gif ?><?FILEsi6 gif ?><?FILEsi7 gif ?><?FILEsi8 gif ?><?FILEsi9 gif ?><?FILEsi10 gif ?><?FILEsi11 gif ?><?FILEsi12 gif ?><?FILEsi14 gif ?><?FILEsi15 gif ?><?FILEsi16 gif ?><?FILEsi17 gif ?><?FILEsi18 gif ?><?FILEsi19 gif ?><?FILEsi20 gif ?><?FILEsi21 gif ?><?FILEsi22 gif ?><?FILEsi23 gif ?><?FILEsi24 gif ?><?FILEsi25 gif ?><?FILEsi26 gif ?><?FILEsi27 gif ?><?FILEsi28 gif ?><?FILEsi29 gif ?><?FILEsi30 gif ?><?FILEsi31 gif ?><?FILEsi32 gif ?><?FILEsi33 gif ?><?FILEsi34 gif ?><?FILEsi35 gif ?><front><journal-meta><journal-id journal-id-type="nlm-ta">Genomics Proteomics Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">Genomics Proteomics Bioinformatics</journal-id><journal-title-group><journal-title>Genomics, Proteomics &#x00026; Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1672-0229</issn><issn pub-type="epub">2210-3244</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6364131</article-id><article-id pub-id-type="publisher-id">S1672-0229(18)30439-X</article-id><article-id pub-id-type="doi">10.1016/j.gpb.2018.08.003</article-id><article-categories><subj-group subj-group-type="heading"><subject>Method</subject></subj-group></article-categories><title-group><article-title>VASC: Dimension Reduction and Visualization of Single-cell RNA-seq Data by Deep Variational Autoencoder</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Dongfang</given-names></name><xref rid="fn1" ref-type="fn">a</xref></contrib><contrib contrib-type="author"><name><surname>Gu</surname><given-names>Jin</given-names></name><email>jgu@tsinghua.edu.cn</email><xref rid="cor1" ref-type="corresp">&#x0204e;</xref><xref rid="fn2" ref-type="fn">b</xref></contrib></contrib-group><aff id="af005">MOE Key Laboratory of Bioinformatics, BNRIST Bioinformatics Division &#x00026; Center for Synthetic and Systems Biology, Department of Automation, Tsinghua University, Beijing 100084, China</aff><author-notes><corresp id="cor1"><label>&#x0204e;</label>Corresponding author. <email>jgu@tsinghua.edu.cn</email></corresp><fn id="fn1"><label>a</label><p id="np010">ORCID: 0000-0003-1368-028X.</p></fn><fn id="fn2"><label>b</label><p id="np015">ORCID: 0000-0003-3968-8036.</p></fn></author-notes><pub-date pub-type="pmc-release"><day>18</day><month>12</month><year>2018</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="ppub"><month>10</month><year>2018</year></pub-date><pub-date pub-type="epub"><day>18</day><month>12</month><year>2018</year></pub-date><volume>16</volume><issue>5</issue><fpage>320</fpage><lpage>331</lpage><history><date date-type="received"><day>23</day><month>3</month><year>2018</year></date><date date-type="rev-recd"><day>9</day><month>7</month><year>2018</year></date><date date-type="accepted"><day>8</day><month>8</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 The Authors</copyright-statement><copyright-year>2018</copyright-year><license license-type="CC BY" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><abstract id="ab005"><p>Single-cell RNA sequencing (scRNA-seq) is a powerful technique to analyze the transcriptomic heterogeneities at the single cell level. It is an important step for studying cell sub-populations and lineages, with an effective low-dimensional representation and <bold>visualization</bold> of the original scRNA-Seq data. At the single cell level, the transcriptional fluctuations are much larger than the average of a cell population, and the low amount of RNA transcripts will increase the rate of technical <bold>dropout</bold> events. Therefore, scRNA-seq data are much noisier than traditional bulk RNA-seq data. In this study, we proposed the <bold>deep variational autoencoder</bold> for scRNA-seq data (VASC), a deep multi-layer generative model, for the unsupervised <bold>dimension reduction</bold> and visualization of scRNA-seq data. VASC can explicitly model the dropout events and find the nonlinear hierarchical feature representations of the original data. Tested on over 20 datasets, VASC shows superior performances in most cases and exhibits broader dataset compatibility compared to four state-of-the-art dimension reduction and visualization methods. In addition, VASC provides better representations for very rare cell populations in the 2D visualization. As a case study, VASC successfully re-establishes the cell dynamics in pre-implantation embryos and identifies several candidate marker genes associated with early embryo development. Moreover, VASC also performs well on a 10&#x000d7; Genomics dataset with more cells and higher dropout rate.</p></abstract><kwd-group id="kg005"><title>Keywords</title><kwd>Single cell RNA sequencing</kwd><kwd>Deep variational autoencoder</kwd><kwd>Dimension reduction</kwd><kwd>Visualization</kwd><kwd>Dropout</kwd></kwd-group></article-meta><notes><p id="ms005">Handled by Fuchou Tang</p></notes></front><body><sec id="s0005"><title>Introduction</title><p id="p0005">Characterizing the cellular states at the single cell level is crucial for understanding the cell&#x02013;cell heterogeneities and the biological mechanisms that cannot be observed in the average behaviors of a bulk of cells. Single-cell RNA sequencing (scRNA-seq) is a promising high-throughput technique to simultaneously profile the transcriptomes of a large number of individual cells <xref rid="b0005" ref-type="bibr">[1]</xref>. Thousands of genes are simultaneously expressed in a single cell. Expression levels of these genes are usually tightly regulated in regard to a limited number of cellular states. Finding an effective low-dimensional representation of the scRNA-seq data is the basic step for the data visualization and the downstream analysis, such as the cell lineage establishment and the cell sub-population identification <xref rid="b0010" ref-type="bibr">[2]</xref>. Currently, several traditional dimension reduction methods used for the bulk RNA-seq data analysis, such as principal components analysis (PCA) <xref rid="b0015" ref-type="bibr">[3]</xref> and t-distributed stochastic neighbor embedding (t-SNE) <xref rid="b0020" ref-type="bibr">[4]</xref>, are still widely used for the scRNA-seq data analysis. However, the transcriptional burst effects and low amounts of RNA transcripts in single cells make the scRNA-seq data much noisier than the bulk RNA-seq data. For example, the scRNA-seq data have many unexpected dropout events (many data points are zero or near-zero) <xref rid="b0025" ref-type="bibr">[5]</xref>. These noises make those traditional methods inefficient. To improve the analysis, one useful strategy is to explicitly mimic the data generation process by a probabilistic model. For example, the zero-inflated factor analysis (ZIFA), which combines the probabilistic factor analysis with conditional dropout probability, was developed to find the latent low dimension subspace <xref rid="b0030" ref-type="bibr">[6]</xref>. However, ZIFA can only model linear patterns by a single hidden layer, which limits its performance on the datasets with complex cellular states in the original data space. Another strategy is to embed the cells into another low-dimensional space by preserving the cell&#x02013;cell similarity (or distance) in the original data space. But, this kind of methods, such as single-cell interpretation via multiple kernel learning (SIMLR) <xref rid="b0035" ref-type="bibr">[7]</xref>, frequently change the basic topological information in the embedded space.</p><p id="p0010">In recent years, deep probabilistic hidden models have shown superior performances in representing complex features of high-dimensional data, especially for images and speeches <xref rid="b0040" ref-type="bibr">[8]</xref>, <xref rid="b0045" ref-type="bibr">[9]</xref>. In this study, we developed a deep model, deep variational autoencoder for scRNA-seq data (VASC), to analyze and visualize the scRNA-seq data. VASC can capture non-linear variations and automatically learn a hierarchical representation of the input data. In addition, it uses the Gumbel distribution to better model the zero and near-zero dropout events. We systematically compared VASC with several state-of-the-art dimension reduction methods on 20 datasets. Results show that VASC has superior performance in most cases and exhibits a broader dataset compatibility.</p></sec><sec id="s0010"><title>Methods</title><sec id="s0015"><title>VASC: the method overview</title><p id="p0015">VASC, a generative model based on the deep variational autoencoder (VAE) <xref rid="b0045" ref-type="bibr">[9]</xref>, <xref rid="b0050" ref-type="bibr">[10]</xref>, <xref rid="b0055" ref-type="bibr">[11]</xref>, was designed to find an effective low-dimensional representation and facilitate the visualization of scRNA-seq datasets. It modeled the distribution of high-dimensional original data P(<bold><italic>X</italic></bold>), by a set of latent variables <bold><italic>z</italic></bold> (the dimension of <bold><italic>z</italic></bold> should be much lower than <bold><italic>X</italic></bold>, in particular, being two for visualization). The primary goal of VASC is to find the optimal <bold><italic>z</italic></bold> capturing the intrinsic information of the input data. In a probabilistic view, the posterior distribution P(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) could be treated as the best distribution of <bold><italic>z</italic></bold> given the observed data <bold><italic>X</italic></bold>. However, P(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) is usually intractable. Variational inference is thus proposed to solve this problem by designing another common distribution family Q(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) (also known as variational distribution) to approximate P(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>). The minimization of the Kullback&#x02013;Leibler (KL) divergence between the two distributions is usually adopted for the approximation. The variational distribution Q(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) should be sufficiently representative to model the complex information of P(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) in the scRNA-seq datasets, and on the other hand, should be tractable for efficient computation. In VASC, deep neural networks were used to explicitly model the variational distribution. Unlike the traditional variational inference methods, deep neural networks can approximate arbitrary functions and can be optimized efficiently using the stochastic gradient descent methods.</p><p id="p0020">Generally, VASC has three major parts, namely, the encoder network, the decoder network, and the zero-inflated (ZI) layer (<xref rid="f0005" ref-type="fig">Figure 1</xref>). The encoder network, designed as a three-layer neural network, generates the parameters of the variational distribution. It should be noted that before the first layer, we added a &#x0201c;dropout&#x0201d; noise layer <xref rid="b0060" ref-type="bibr">[12]</xref>, which randomly set some data points in the original expression matrix as zero. From a computational point of view, it introduced additional random noises for the sample training, which can reduce the overfitting risk during the learning process. We assumed a multi-dimensional Gaussian distribution for Q(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) of latent variables <bold><italic>z</italic></bold> given the expression values <bold><italic>X</italic></bold>, of which mean and variance parameters could be generated by the encoder network. Then, the learned Q(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) was used to re-generate pseudo samples <bold><italic>X</italic></bold>&#x02019; by the decoder network, another three-layer neural network. Finally, a ZI layer, based on a double-exponential distribution, was designed to mimic the dropout events by randomly setting some data points as zero <xref rid="b0030" ref-type="bibr">[6]</xref>, <xref rid="b0065" ref-type="bibr">[13]</xref>. The Gumbel distribution instead of the conditional binomial distribution was used in the ZI layer for the back-propagation <xref rid="b0070" ref-type="bibr">[14]</xref>, <xref rid="b0075" ref-type="bibr">[15]</xref>. VASC was optimized by a stochastic gradient descent-based RMSprop methods <xref rid="b0080" ref-type="bibr">[16]</xref>, aimed to minimize an auxiliary loss function of the KL divergence between Q(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>) and P(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>). After the auto-encoding procedure, a 2D representation was learned for visualization and other downstream analysis.<fig id="f0005"><label>Figure 1</label><caption><p><bold>Overview of VASC workflow</bold></p><p>VASC consists of three parts: the encoder network, the decoder network, and the zero-inflated layer. Both the encoder and decoder networks are designed as three-layer fully-connected neural networks. VASC, variational autoencoder for scRNA-seq data; <inline-formula><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:math></inline-formula>, input expression profile for one cell; <inline-formula><mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mspace width="3.33333pt"/></mml:mrow><mml:mrow><mml:mtext>and</mml:mtext><mml:mspace width="0.333333em"/><mml:mspace width="0.333333em"/></mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, mean and covariance of the latent Gaussian distribution; <inline-formula><mml:math id="M3" altimg="si3.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:math></inline-formula>, samples from the latent Gaussian distribution; <inline-formula><mml:math id="M4" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">&#x0223c;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, recovered expression profile by the decoder network; <inline-formula><mml:math id="M5" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow></mml:math></inline-formula>, recovered expression profile after zero inflation; KL, Kullback&#x02013;Leibler divergence; Q(<italic>z|X</italic>), variational distribution; P(<italic>z</italic>), prior standard normal distribution. Loss(X,Y) indicates the binary entropy between original profile and recovered profile plus the KL divergence between variational distribution and prior distribution.</p></caption><graphic xlink:href="gr1"/></fig></p></sec><sec id="s0020"><title>Datasets</title><p id="p0025">To demonstrate the performance of VASC, we analyzed 22 scRNA-seq datasets (<xref rid="t0005" ref-type="table">Table 1</xref>). The first 20 datasets were obtained from the Hemberg group (<ext-link ext-link-type="uri" xlink:href="https://hemberg-lab.github.io/scRNA.seq.datasets/" id="ir010">https://hemberg-lab.github.io/scRNA.seq.datasets/</ext-link>), with &#x02018;scater&#x02019; toolkit <xref rid="b0170" ref-type="bibr">[34]</xref> used for quality control. The human pre-implantation embryo dataset (Petropoulus) <xref rid="b0160" ref-type="bibr">[32]</xref> with detailed annotations was obtained via ArrayExpress (<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/arrayexpress/" id="ir015">https://www.ebi.ac.uk/arrayexpress/</ext-link>; accession No. E-MTAB-3929). The PBMC3k dataset was downloaded from 10&#x000d7; Genomics (<ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/datasets" id="ir020">https://support.10xgenomics.com/single-cell-gene-expression/datasets</ext-link>).<table-wrap id="t0005" position="float"><label>Table 1</label><caption><p>The list of scRNA-seq datasets used in this study</p></caption><table frame="hsides" rules="groups"><thead><tr><th><bold>Dataset No.</bold></th><th colspan="2"><bold>Dataset name</bold></th><th><bold>No. of cells</bold></th><th><bold>No. of genes</bold></th><th><bold>Protocol</bold></th><th><bold>No. of reads</bold></th><th><bold>No. of cell types</bold></th><th><bold>Ref.</bold></th></tr></thead><tbody><tr><td>1</td><td rowspan="6">Baron</td><td>Human-1</td><td>1937</td><td rowspan="4">20,125</td><td rowspan="6">inDrop</td><td rowspan="6">Around 6000</td><td rowspan="4">14</td><td rowspan="6"><xref rid="b0085" ref-type="bibr">[17]</xref></td></tr><tr><td>2</td><td>Human-2</td><td>1724</td></tr><tr><td>3</td><td>Human-3</td><td>3605</td></tr><tr><td>4</td><td>Human-4</td><td>1303</td></tr><tr><td>5</td><td>Mouse-1</td><td>822</td><td rowspan="2">14,878</td><td rowspan="2">13</td></tr><tr><td>6</td><td>Mouse-2</td><td>1064</td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>7</td><td colspan="2">Biase</td><td>56</td><td>25,733</td><td>SMARTer</td><td>37.9 million</td><td>4</td><td><xref rid="b0090" ref-type="bibr">[18]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>8</td><td colspan="2">Camp</td><td>777</td><td>19,020</td><td>SMARTer</td><td>1&#x02013;5 million</td><td>7</td><td><xref rid="b0095" ref-type="bibr">[19]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>9</td><td colspan="2">Darmanis</td><td>466</td><td>22,088</td><td>SMARTer</td><td>2,838,000</td><td>9</td><td><xref rid="b0100" ref-type="bibr">[20]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>10</td><td colspan="2">Deng</td><td>268</td><td>22,431</td><td>Smart-Seq<break/>Smart-Seq2</td><td>1&#x02013;70 million</td><td>6</td><td><xref rid="b0105" ref-type="bibr">[21]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>11</td><td colspan="2">Goolam</td><td>124</td><td>41,427</td><td>Smart-Seq2</td><td>1&#x02013;10 million</td><td>5</td><td><xref rid="b0110" ref-type="bibr">[22]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>12</td><td colspan="2">Klein</td><td>2717</td><td>24,175</td><td>inDrop</td><td>208,000</td><td>4</td><td><xref rid="b0115" ref-type="bibr">[23]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>13</td><td colspan="2">Kolodziejczyk</td><td>704</td><td>38,615</td><td>SMARTer</td><td>9 million</td><td>9</td><td><xref rid="b0120" ref-type="bibr">[24]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>14</td><td colspan="2">Li</td><td>561</td><td>55,186</td><td>SMARTer</td><td>-</td><td>9</td><td><xref rid="b0125" ref-type="bibr">[25]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>15</td><td colspan="2">Patel</td><td>430</td><td>5948</td><td>Smart-Seq</td><td>-</td><td>5</td><td><xref rid="b0130" ref-type="bibr">[26]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>16</td><td colspan="2">Pollen</td><td>301</td><td>23,730</td><td>SMARTer</td><td>&#x0223c;50,000</td><td>11</td><td><xref rid="b0135" ref-type="bibr">[27]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>17</td><td colspan="2">Usoskin</td><td>622</td><td>25,334</td><td>STRT-Seq</td><td>1.14 million</td><td>11</td><td><xref rid="b0140" ref-type="bibr">[28]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>18</td><td colspan="2">Xin</td><td>1600</td><td>39,851</td><td>SMARTer</td><td>&#x0223c;0.95 million</td><td>8</td><td><xref rid="b0145" ref-type="bibr">[29]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>19</td><td colspan="2">Yan</td><td>90</td><td>20,214</td><td>Tang</td><td>35.3 million</td><td>6</td><td><xref rid="b0150" ref-type="bibr">[30]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>20</td><td colspan="2">Zeisel</td><td>3005</td><td>19,972</td><td>STRT-Seq</td><td>500,000</td><td>9</td><td><xref rid="b0155" ref-type="bibr">[31]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>21</td><td colspan="2">Petropoulos</td><td>1529</td><td>19,651</td><td>Smart-Seq2</td><td>-</td><td>7</td><td><xref rid="b0160" ref-type="bibr">[32]</xref></td></tr><tr><td colspan="9">&#x000a0;</td></tr><tr><td>22</td><td colspan="2">PBMC3k</td><td>2700</td><td>32,738</td><td>10X</td><td>&#x0223c;2000 UMIs</td><td>8</td><td><xref rid="b0165" ref-type="bibr">[33]</xref></td></tr></tbody></table><table-wrap-foot><fn><p><italic>Note</italic>: All protocols and reads were extracted from the original publications. UMI, unique molecular identifier.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id="s0025"><title>VAE</title><p id="p0030">VASC is a deep VAE-based generative model and is designed for the visualization and low-dimensional representation of the scRNA-seq data. VAE aims to model the distribution P(<bold><italic>X</italic></bold>) of data points in a high-dimensional space <italic>&#x003c7;</italic>, with the aid of low-dimensional latent variables <bold><italic>z</italic></bold>. The whole model is divided into two procedures, that is, (1) generating the samples of <bold><italic>z</italic></bold> in the latent low-dimensional subspace, and (2) mapping them to the original space <italic>&#x003c7;</italic>. The critical point is to generate <bold><italic>z</italic></bold> having the high probability to recover the observed data matrix <bold><italic>X</italic></bold>. In this way, the generated <bold><italic>z</italic></bold> may be possible to capture the intrinsic information of the original data. The best choice to generate <bold><italic>z</italic></bold>, in theory, is the posterior P(<bold><italic>z|X</italic></bold>), which however, is usually too complicated and intractable. VAE tries to use a variational probability Q(<bold><italic>z|X</italic></bold>) to approximate the posterior, by minimizing the Kullback&#x02013;Leibler (KL) divergence (<italic>D</italic>) between Q(<bold><italic>z|X</italic></bold>) and P(<bold><italic>z|X</italic></bold>):<disp-formula id="e0005"><label>(1)</label><mml:math id="M6" altimg="si6.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mtext>|</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mtext>|</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mtext>]</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mfenced open="[" close="]"><mml:mrow><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mtext>|</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p><p id="p0035">By applying the Bayes rule and rearranging the order, it can be re-written as:<disp-formula id="e0010"><label>(2)</label><mml:math id="M7" altimg="si7.gif" overflow="scroll"><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mtext>|</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mtext>|</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mtext>]</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mfenced open="[" close="]"><mml:mrow><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mtext>|</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where P(<bold><italic>X</italic></bold>) is a constant and <inline-formula><mml:math id="M8" altimg="si8.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> represents expectation over z that is sampled from Q. Therefore, minimizing the KL divergence is equivalent to maximizing the right-hand part of Equation <xref rid="e0010" ref-type="disp-formula">(2)</xref>. The right-hand part has a natural autoencoder structure, with the encoder Q<bold>(<italic>z|X</italic></bold>) from <bold><italic>X</italic></bold> to <bold><italic>z</italic></bold> and the decoder P<bold>(<italic>X|z</italic>)</bold> from <bold><italic>z</italic></bold> to <bold><italic>X</italic></bold>. Two deep fully-connected neural networks can be used to model these two parts.</p></sec><sec id="s0030"><title>VASC method</title><p id="p0040">The whole VASC structure is shown in <xref rid="f0005" ref-type="fig">Figure 1</xref>. The model designs and the learning algorithms are described in detail as below.</p><sec id="s0035"><title>Input layer</title><p id="p0045">VASC uses the expression matrix from scRNA-seq data as inputs. The whole expression matrix of the transcriptome was fed directly to the model with no gene filter applied. The data were log-transformed to make the results more robust. The most important transformation, however, was to re-scale the expression of every gene in any single cell in the range [0,1] by dividing the maximum expression value of an individual gene from the same cell.</p></sec><sec id="s0040"><title>Dropout layer</title><p id="p0050">A dropout layer <xref rid="b0060" ref-type="bibr">[12]</xref> was added immediately after the input layer, with the dropout rate set as 0.5, which is larger than the usual choice in deep models for input layers. This layer set some features to zeros during the encoding phase, to increase the performance in model learning <xref rid="b0175" ref-type="bibr">[35]</xref>. This layer should be a good choice for scRNA-seq data because it may be regarded as artificial and additional &#x0201c;dropout&#x0201d; events, and forces subsequent layers to learn to avoid dropout noises.</p></sec><sec id="s0045"><title>Encoder network</title><p id="p0055">The encoder network was designed as a three-layer fully-connected neural network with decreasing dimensions 512, 128, and 32. The first layer did not use non-linear activation, which acted as an embedded PCA transformation. Many complex algorithms, including t-SNE, benefit from the PCA transformation. L1-norm regularization was added for the weights in this layer, which penalized the sparsity of the model. The next two layers were accompanied by ReLU activation, which made the output sparse and stable for deep models <xref rid="b0180" ref-type="bibr">[36]</xref>.</p></sec><sec id="s0050"><title>Latent sampling layer</title><p id="p0060">Latent variables <bold><italic>z</italic></bold> were modeled by a Gaussian distribution, with the standard normal prior N(0,<bold><italic>I</italic></bold>). The encoder network was used to estimate its posterior parameters. Usually, both the parameters <inline-formula><mml:math id="M9" altimg="si9.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mspace width="3.33333pt"/></mml:mrow><mml:mrow><mml:mtext>and</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> needed to be estimated, with a linear activation used to estimate <inline-formula><mml:math id="M10" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula>. According to our experiments, it is better to fix <inline-formula><mml:math id="M11" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow></mml:math></inline-formula> and set <inline-formula><mml:math id="M12" altimg="si12.gif" overflow="scroll"><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, if the dataset only has small sample size. For the datasets with large sample size (more than 1000 cells), <inline-formula><mml:math id="M13" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow><mml:mspace width="3.33333pt"/></mml:mrow></mml:math></inline-formula> can also be trained by the encoder network. A &#x02018;softplus&#x02019; activation was used for the estimation of <inline-formula><mml:math id="M14" altimg="si14.gif" overflow="scroll"><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Since the neural network does not have a stochastic layer and thus could not be tackled by back-propagation algorithm, a re-parameterization trick was used to remove the randomness in input data. It is easy to see, drawing a sample <bold><italic>z</italic></bold> from <inline-formula><mml:math id="M15" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> is equivalent to drawing a sample <inline-formula><mml:math id="M16" altimg="si16.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo mathvariant="bold" stretchy="false">&#x0223c;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="M17" altimg="si17.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> and then let <inline-formula><mml:math id="M18" altimg="si18.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mspace width="3.33333pt"/></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#x003a3;</mml:mi></mml:mrow></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo mathvariant="bold" stretchy="false">&#x0223c;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> (see Section 1 of <xref rid="s0160" ref-type="sec">File S1</xref> for more details).</p></sec><sec id="s0055"><title>Decoder network</title><p id="p0065">The decoder network used the generated <bold><italic>z</italic></bold> to recover the original expression matrix, which was designed as a three-layer fully-connected neural network with dimensions of hidden units 32, 128, and 512, respectively, and an output layer. The first three layers used &#x02018;ReLU&#x02019; activations and the final layer with sigmoid to make the output within [0,1] (this is why the [0,1] re-scaling transformation must be applied in the input layer).</p></sec><sec id="s0060"><title>ZI layer</title><p id="p0070">An additional ZI layer was added after the decoder network. Adapted from the model used by ZIFA <xref rid="b0030" ref-type="bibr">[6]</xref>, we modeled the dropout events by the probability <inline-formula><mml:math id="M19" altimg="si19.gif" overflow="scroll"><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">&#x0223c;</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math id="M20" altimg="si20.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">&#x0223c;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the recovered expression value by the decoder network. Back-propagation, as mentioned before, cannot deal with stochastic units; moreover, it cannot deal with discrete units either. A Gumbel-softmax distribution <xref rid="b0075" ref-type="bibr">[15]</xref> was thus introduced to overcome these difficulties. Suppose <inline-formula><mml:math id="M21" altimg="si21.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:math></inline-formula> is the probability for dropout and <inline-formula><mml:math id="M22" altimg="si22.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the sample <italic>s</italic> from Gumbel-softmax distribution was obtained by:<disp-formula id="e0015"><label>(3)</label><mml:math id="M23" altimg="si23.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">exp</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c4;</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mspace width="3.33333pt"/></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">exp</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c4;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">exp</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c4;</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mspace width="3.33333pt"/></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:mspace width="3.33333pt"/></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M24" altimg="si24.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were sampled from a Gumbel (0,1) distribution. The samples could then be obtained by first drawing an auxiliary sample <inline-formula><mml:math id="M25" altimg="si25.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Uniform</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> and then computing <inline-formula><mml:math id="M26" altimg="si26.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo>log</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mo>log</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>. As the hyper-parameter <inline-formula><mml:math id="M27" altimg="si27.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c4;</mml:mi></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the generated samples from the Gumbel-softmax distribution should be identical to the samples from the Bernoulli distribution. In practice, too small values of <inline-formula><mml:math id="M28" altimg="si28.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula> makes the gradient of the whole network too small and the optimization algorithm cannot work. Our experiments showed that it would be better by setting <inline-formula><mml:math id="M29" altimg="si28.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula> between 0.5&#x02013;1 for the datasets of small sample size. For the datasets with more cells, an annealing strategy may yield better results (see Section 1 of <xref rid="s0160" ref-type="sec">File S1</xref> for details).</p></sec><sec id="s0065"><title>Loss function</title><p id="p0075">The loss function as shown in the Equation <xref rid="e0010" ref-type="disp-formula">(2)</xref> is composed of two components. The first part, because of the scale of our data, [0,1], was computed by binary cross-entropy loss function. The second part, controlling the divergence between posterior distribution and the prior <inline-formula><mml:math id="M30" altimg="si17.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, could be computed analytically (see Section 1 of <xref rid="s0160" ref-type="sec">File S1</xref> for more details).</p></sec><sec id="s0070"><title>Optimization</title><p id="p0080">The whole structure, now, could be optimized end-to-end using the stochastic gradient descent-based optimization algorithm. We chose the RMSprop method <xref rid="b0080" ref-type="bibr">[16]</xref> for VASC. In addition, we set the learning rate as 0.0001, to ensure the convergence on all the datasets tested. The training processes were stopped if the training loss did not show obvious decrease within 50 epochs.</p><p id="p0085">Source codes implemented by keras (<ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras" id="ir025">https://github.com/fchollet/keras</ext-link>) can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/wang-research/VASC" id="ir030">https://github.com/wang-research/VASC</ext-link>.</p></sec></sec><sec id="s0075"><title>Benchmarking</title><p id="p0090">For each dataset, we considered four state-of-the-art dimension reduction methods &#x02013; PCA <xref rid="b0015" ref-type="bibr">[3]</xref>, t-SNE <xref rid="b0020" ref-type="bibr">[4]</xref>, ZIFA <xref rid="b0030" ref-type="bibr">[6]</xref>, and SIMLR <xref rid="b0035" ref-type="bibr">[7]</xref>. For all the methods, no gene filtering was used and the same log-2 transformation was applied. For PCA and t-SNE, we used the built-in python sklearn package functions. For the datasets with more than 500 cells, we firstly applied a PCA transformation with 500 dimensions before t-SNE. Perplexity, the key parameter of t-SNE, was set as 0.2 times the number of cells as suggested previously <xref rid="b0160" ref-type="bibr">[32]</xref>. For ZIFA, we downloaded the package and used the block_ZIFA module provided by Pierson and Yau <xref rid="b0030" ref-type="bibr">[6]</xref>, due to the large number of genes evaluated. For SIMLR, we used the R package described by Wang and colleagues <xref rid="b0035" ref-type="bibr">[7]</xref>. For benchmarking the dimension reduction performance, <italic>k</italic>-means was used to obtain the predicted cell types based on their 2D representations (see Section 2 of <xref rid="s0160" ref-type="sec">File S1</xref> for more details).</p></sec><sec id="s0080"><title>Performance assessment</title><p id="p0095">To measure the quality of visualization and low-dimensional representation, <italic>k</italic>-means clustering was applied to the 2D representations of all the aforementioned methods. Then the obtained clustering results were compared with the known cell types provided in the original references. The number of clusters, <italic>k</italic>, was set to number of known cell types. Four measures were used to assess the performances, including normalized mutual information (NMI) <xref rid="b0185" ref-type="bibr">[37]</xref>, adjusted rand index (ARI) <xref rid="b0190" ref-type="bibr">[38]</xref>, homogeneity <xref rid="b0195" ref-type="bibr">[39]</xref>, and completeness <xref rid="b0195" ref-type="bibr">[39]</xref>.</p><sec id="s0085"><title>NMI</title><p id="p0100">Suppose P is the predicted clustering results, and T is the known cell types (the same below), we denote the entropy of P and T as H(P) and H(T), respectively, and the mutual information between them as MI(P,T). NMI is computed as:<disp-formula id="e0020"><label>(4)</label><mml:math id="M31" altimg="si29.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">NMI</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">MI</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="s0090"><title>ARI</title><p id="p0105">Suppose <italic>n</italic> is the total number of samples, <inline-formula><mml:math id="M32" altimg="si30.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of samples appearing in the <italic>i</italic>-th cluster of <italic>P</italic>, <inline-formula><mml:math id="M33" altimg="si31.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of samples appearing in the <italic>j</italic>-th types of <italic>T</italic>, and <inline-formula><mml:math id="M34" altimg="si32.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ij</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of overlaps between the <italic>i</italic>-th cluster of <italic>P</italic> and the <italic>j</italic>-th type and T. ARI is computed as:<disp-formula id="e0025"><label>(5)</label><mml:math id="M35" altimg="si33.gif" overflow="scroll"><mml:mrow><mml:mtext>ARI</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ij</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ij</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mfrac><mml:mfenced open="[" close="]"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced open="[" close="]"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mfrac><mml:mfenced open="[" close="]"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="s0095"><title>Homogeneity</title><p id="p0110">The measure homogeneity expects that every cluster only contains samples from one cell type. Suppose <italic>H</italic>(<italic>T</italic>|<italic>P</italic>) is the cross-entropy of cell types given the cluster <italic>P</italic>, the homogeneity score (<italic>h</italic>) is computed by:<disp-formula id="e0030"><label>(6)</label><mml:math id="M36" altimg="si34.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mspace width="3.33333pt"/></mml:mrow></mml:math></disp-formula></p></sec><sec id="s0100"><title>Completeness</title><p id="p0115">The measure completeness (c) expects that samples from one cell type are assigned to the same cluster, and is computed as:<disp-formula id="e0035"><label>(7)</label><mml:math id="M37" altimg="si35.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:mspace width="3.33333pt"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="p0120">For all the measures including NMI, ARI, homogeneity, and completeness, larger values (up to 1) mean better performances.</p></sec></sec><sec id="s0105"><title>Analysis of the PBMC3k dataset</title><p id="p0125">We filtered cells with less than three detected genes (UMIs&#x0202f;&#x0003e;&#x0202f;3). Number of UMI counts was transformed to transcript-per-million (TPM)-like values by normalizing each cell through dividing total UMI counts and then multiplying by 10,000. Log2 transformation was applied after adding a pseudo-count 1 to obtain the gene expression matrix. Due to the serious dropout events present in this dataset, gene selection is used to reduce noises. We adopted the same procedure as previously reported <xref rid="b0200" ref-type="bibr">[40]</xref>, with 1158 genes that remained. VASC was then tested on this pre-processed gene expression matrix.</p></sec></sec><sec id="s0110"><title>Results</title><sec id="s0115"><title>Visualization and performance comparison</title><p id="p0130">We tested the visualization performance of VASC together with four state-of-the-art dimension reduction methods, including PCA <xref rid="b0015" ref-type="bibr">[3]</xref>, t-SNE <xref rid="b0020" ref-type="bibr">[4]</xref>, ZIFA <xref rid="b0030" ref-type="bibr">[6]</xref>, and SIMLR <xref rid="b0035" ref-type="bibr">[7]</xref>, on 20 datasets with different number of cells included and sequencing protocols used (the top 20 datasets in <xref rid="t0005" ref-type="table">Table 1</xref>). Firstly, we compared the 2D visualizations on six &#x0201c;golden&#x0201d; datasets (these datasets provide highly-confident cell labels), with the number of cells ranging from tens to thousands (<xref rid="f0010" ref-type="fig">Figure 2</xref>). Datasets reported by Goolam et al. <xref rid="b0110" ref-type="bibr">[22]</xref>, Biase et al. <xref rid="b0090" ref-type="bibr">[18]</xref>, and Yan et al. <xref rid="b0150" ref-type="bibr">[30]</xref>, respectively, were generated from studies on the embryonic development from zygote to blast cells. PCA, ZIFA, and VASC roughly re-established the developmental stages of different cell types (cells are expected to be arranged in the order of zygote, 2-cell, 4-cell, 8-cell, 16-cell, and blast cells) (<xref rid="f0010" ref-type="fig">Figure 2</xref>A&#x02013;C). However, t-SNE and SIMLR, both of which use neighbor-preserving embedding, showed poor performance on these datasets. In contrast, VASC further separated 16-cell and blast from 8-cell stages in the Goolam dataset. Moreover, compared to PCA and ZIFA, VASC better separated blast cells from 4-cell stages, and identified one zygote as a possible outlier in the Biase dataset, whereas 4-cell stage was better separated from zygote and 2-cell stages using VASC in the Yan dataset (<xref rid="f0010" ref-type="fig">Figure 2</xref>A&#x02013;C). These results indicate that VASC can better model the embryo developmental progression than PCA and ZIFA.<fig id="f0010"><label>Figure 2</label><caption><p><bold>Visualization of scRNA-seq datasets using different methods</bold></p><p>Each data point represents a cell. Different cell types are indicated in different colors and shapes. All datasets were run by PCA, t-SNE, ZIFA, SIMLR, and VASC respectively. Cell type information was retrieved from original studies. Shown in the figures are clustering output from the Goolam <xref rid="b0110" ref-type="bibr">[22]</xref> (<bold>A</bold>), Biase <xref rid="b0090" ref-type="bibr">[18]</xref> (<bold>B</bold>), Yan <xref rid="b0150" ref-type="bibr">[30]</xref> (<bold>C</bold>), Pollen <xref rid="b0135" ref-type="bibr">[27]</xref> (<bold>D</bold>), Kolodziejczyk <xref rid="b0120" ref-type="bibr">[24]</xref> (<bold>E</bold>), and Baron_human-1 <xref rid="b0085" ref-type="bibr">[17]</xref> (<bold>F</bold>) datasets. Visualization of other datasets is provided in the Section 4 of <xref rid="s0160" ref-type="sec">File S1</xref>. PCA, principal components analysis; t-SNE, t-distributed stochastic neighbor embedding; ZIFA, zero-inflated factor analysis; SIMLR, single-cell interpretation via multiple kernel learning.</p></caption><graphic xlink:href="gr2"/></fig></p><p id="p0135">Eleven different cell types were sequenced in the fourth dataset reported by Pollen and colleagues <xref rid="b0135" ref-type="bibr">[27]</xref>. In this case, PCA and ZIFA showed poor performance in classification (<xref rid="f0010" ref-type="fig">Figure 2</xref>D). In the SIMLR visualization, eleven compact clusters of cells were formed, but at least four clusters were composed of more than one cell type (the points from different cell types were stacked together for possible misleading visualization). This result was undesirable because the cells from different types should not compactly cluster together. Instead, Using VASC, eight compact clusters of cells were formed, each from the same cell type. The remaining three cell types, GW16, GW21, and GW21&#x0202f;+&#x0202f;3 (originally sampled from the germinal zone of human cortex at gestational week 16, 21, and cultured for another three weeks, respectively), were distributed in a more decentralized manner than the others. These cells, along with neural progenitor cells (NPCs), are all neural cells. Therefore, it seems reasonable that they are presented more closely using VASC.</p><p id="p0140">Kolodziejczyk et al. generated a dataset when examining embryonic stem cells grown under three different conditions: serum, 2i, and alternative 2i (a2i) <xref rid="b0120" ref-type="bibr">[24]</xref>. Moreover, there existed different experimental batches for every condition. As shown in <xref rid="f0010" ref-type="fig">Figure 2</xref>E, PCA separated the cells grown under the three different conditions but almost mixed the batches; ZIFA better separated the cells under different growth conditions and from different batches but incorrectly mixed one 2i cell batch (2i_2) with a2i cells; SIMLR separated most cell populations under different growth conditions and from different batches (except two batches of 2i cells), but incorrectly grouped the cells from 2i and a2i conditions. Only t-SNE and VASC separated the most cell populations, while preserving their proper relative positions.</p><p id="p0145">The dataset reported by Baron et al. <xref rid="b0085" ref-type="bibr">[17]</xref> included several sequencing subsets from four human donors and two mice. Visualization of the first donor with 1937 cells from 14 different cell types is shown in <xref rid="f0010" ref-type="fig">Figure 2</xref>F. On this dataset, PCA and ZIFA separated only few cell types, whereas both t-SNE and SIMLR showed better separation, although SIMLR produced more compact clusters. However, the putative clusters grouped using SIMLR contained mixtures of different cell types at various levels (for example, two kinds of stellate cells were completely mixed). Notably, VASC showed better separation of the different cell types. Furthermore, the cells from close cell lineages were clustered together. For instance, the alpha, beta, delta, gamma, and epsilon cells that are all within islets were grouped close to each other; beta cells, despite with the largest number (872 cells), were most compactly clustered by VASC. In addition, three types of immune cells, including macrophages (14 cells), mast (8 cells), and T_cells (2 cells), were grouped close to each other, whereas the Schwann cells (only 5 cells) were well separated (see the purple dots in the central region).</p><p id="p0150">Next, to quantitatively assessing the performance of these methods in dimension reduction and visualization, we compared the cell sub-populations in the reduced subspaces (the sub-populations were identified by <italic>k</italic>-means clustering <xref rid="b0205" ref-type="bibr">[41]</xref>) with the true cell type labels annotated in the original publications. Four different parameters were used, including normalized NMI <xref rid="b0185" ref-type="bibr">[37]</xref>, ARI <xref rid="b0190" ref-type="bibr">[38]</xref>, homogeneity <xref rid="b0135" ref-type="bibr">[27]</xref>, and completeness <xref rid="b0195" ref-type="bibr">[39]</xref>, to quantitatively assess the clustering performances. PCA, t-SNE, ZIFA, SIMLR, and VASC were used to systematically analyze 20 datasets, including Camp <xref rid="b0095" ref-type="bibr">[19]</xref>, Darmanis <xref rid="b0100" ref-type="bibr">[20]</xref>, Deng <xref rid="b0105" ref-type="bibr">[21]</xref>, Klein <xref rid="b0115" ref-type="bibr">[23]</xref>, Li <xref rid="b0125" ref-type="bibr">[25]</xref>, Patel <xref rid="b0130" ref-type="bibr">[26]</xref>, Usokin <xref rid="b0140" ref-type="bibr">[28]</xref>, Xin <xref rid="b0145" ref-type="bibr">[29]</xref>, Zeisel <xref rid="b0155" ref-type="bibr">[31]</xref>, besides the aforementioned databases. These comparisons showed that VASC outperformed the other methods in terms of NMI and ARI in most cases (best performances achieved on 15 and 17 out of the 20 datasets, respectively) (<xref rid="f0015" ref-type="fig">Figure 3</xref>A). Furthermore, VASC always ranked in the top two methods on all the tested datasets (<xref rid="f0015" ref-type="fig">Figure 3</xref>B) in terms of NMI and ARI, respectively. This suggests that VASC has broad compatibility with various kinds of scRNA-seq datasets (see the detailed results in the Section 4 of <xref rid="s0160" ref-type="sec">File S1</xref>).<fig id="f0015"><label>Figure 3</label><caption><p><bold>Performance comparison using different methods</bold></p><p><bold>A.</bold> The NMI and ARI values for each method on each dataset. Clustering was performed on 2-D representations of different algorithms and then the output was compared with true cell type labels for the 20 datasets indicated. Detailed dataset information is listed in <xref rid="t0005" ref-type="table">Table 1</xref>. <bold>B.</bold> The statistics of the ranks of the compared methods based on NMI and ARI values. For each dataset, NMI and ARI values given by different algorithms were ranked in the descending order, with rank 1 indicative the highest NMI or ARI values. The number of ranks achieved by these algorithms in the 20 datasets is then counted for distribution. NMI, normalized mutual information; ARI, adjusted rand index.</p></caption><graphic xlink:href="gr3"/></fig></p></sec><sec id="s0120"><title>Analysis of the model stability and parameter setting</title><p id="p0155">In this section, we analyzed the stability and parameter settings of VASC. Firstly, we analyzed the model fitting processes of VASC on two datasets, the Pollen and Biase datasets (with 301 and 56 cells, respectively). Loss function of the whole neural network decreased sharply during the first few epochs, and simultaneously, the NMI and ARI values increased sharply (<xref rid="f0020" ref-type="fig">Figure 4</xref>A and B). After the first 100 epochs, the loss curves quickly converged to a lower limit and the loss fluctuations of the dataset with more samples (Pollen) were smaller than those of the dataset with fewer samples (Biase). Based on these observations, VASC is set to stop when there is no obvious decrease in loss function within 50 epochs (see details in the Methods section).<fig id="f0020"><label>Figure 4</label><caption><p><bold>Analysis of the model stability and parameter settings of VASC</bold></p><p><bold>A.</bold> The iteration process using the Pollen dataset <xref rid="b0135" ref-type="bibr">[27]</xref>. The change of loss values of the whole network as shown in Equation <xref rid="e0010" ref-type="disp-formula">(2)</xref> versus iteration epochs is shown on the left and the right part is the change of NMI and ARI values versus iteration epochs is shown on the right. <bold>B.</bold> The iteration process using the Biase dataset <xref rid="b0090" ref-type="bibr">[18]</xref>. <bold>C.</bold> The stability of VASC. The boxplots were generated based on 20 repeated runs with (green) or without (orange) the ZI layer. Tests were performed on the Biase <xref rid="b0090" ref-type="bibr">[18]</xref>, Goolam <xref rid="b0110" ref-type="bibr">[22]</xref>, Pollen <xref rid="b0135" ref-type="bibr">[27]</xref>, and Yan <xref rid="b0150" ref-type="bibr">[30]</xref> datasets. <bold>D.</bold> The down-sampling test on cell numbers based on the Pollen dataset <xref rid="b0135" ref-type="bibr">[27]</xref>. VASC was run on 10%&#x02013;100% randomly-sampled cells of the original dataset. <bold>E.</bold> The down-sampling test on read numbers based on the Pollen dataset <xref rid="b0135" ref-type="bibr">[27]</xref>. <bold>F.</bold> The effects of the dimensions (ranging from 2 to 30) for the latent variables based on the Pollen dataset <xref rid="b0135" ref-type="bibr">[27]</xref>. ZI, zero-inflated.</p></caption><graphic xlink:href="gr4"/></fig></p><p id="p0160">Due to the randomness of the stochastic gradient descent method, the model initialization, and the <italic>k</italic>-means clustering, slightly different results could be generated at different runs. We thus analyzed the four datasets with the smallest sample sizes, including Biase (56 samples), Goolam (124), Pollen (301), and Yan (90), to test the stability of VASC by 20 repeated runs. As expected, the two datasets with relatively more cells (Goolam and Pollen) showed much higher consistent results than the other two datasets (<xref rid="f0020" ref-type="fig">Figure 4</xref>C). The NMI values of the Biase dataset were almost distributed between the two boundaries of the boxplots. Considering the relatively small number of cells (only 56 samples), this distribution may be caused by the different clustering output of one or two cells at the boundary between two cell types. A similar result was also observed for the Yan dataset. However, the Goolam and Pollen datasets with more cells did not show this pattern.</p><p id="p0165">Then, the down-sampling experiment based on the Pollen dataset was implemented to further test the effect of number of cells on the stability of VASC. The dataset was bootstrapped with 10%, 30%, 50%, 70%, 90%, and 100% cells, also with 20 repeated runs. Low average NMI and ARI values with high variations were observed when the number of samples was too small. However, comparable NMI and ARI values were achieved when the percentage of sampled cells was above 50% (<xref rid="f0020" ref-type="fig">Figure 4</xref>D). We then down-sampled original reads of the Pollen dataset similarly. For each cell, 5000, 10,000, 50,000, 100,000, 200,000, and 300,0000 unique reads were randomly selected for the analysis, following the same pre-preprocessing procedures. As shown in <xref rid="f0020" ref-type="fig">Figure 4</xref>E, low NMI and ARI values were observed only when the number of reads was very small.</p><p id="p0170">The ZI layer was incorporated into VASC to model the dropout event. We then evaluated its effectiveness. As shown in <xref rid="f0020" ref-type="fig">Figure 4</xref>C, the inclusion of ZI layer improved both the stability and the average performances of VASC on three of the four tested datasets.</p><p id="p0175">The data projection to a 2D subspace is suitable for visualization, but the subspace with higher dimension may explain more variations. To further test the effects of dimension number, we varied the dimensions of the final latent variables from 2 to 20, using the Pollen dataset. Results showed that the increase in the dimensions did not improve the identification of known cell populations and the subspaces with high dimensions may even cause worse performances in terms of NMI and ARI values (<xref rid="f0020" ref-type="fig">Figure 4</xref>F).</p></sec><sec id="s0125"><title>Case study: human pre-implantation embryos</title><p id="p0180">The scRNA-seq is very useful for studying the cell dynamics during pre-implantation embryo development. We applied VASC on a recently-published dataset of human pre-implantation embryos (the Petropoulus dataset), including 1529 cells with detailed annotations of developmental stages, inferred lineage, and inferred pseudo-time information (all annotations were obtained from original publication) <xref rid="b0160" ref-type="bibr">[32]</xref>. According to the 2D visualization analysis, VASC and t-SNE recovered the known developmental stages (form E3 to E7) more precisely, with the exception that the E3 cells were out of the trajectory by t-SNE. Both PCA and ZIFA generally recovered the stage trajectory, but the E6 and E7 cells were largely overlapped. SIMLR, which emphasized the modularity of cell populations, did not re-establish the basic pattern (<xref rid="f0025" ref-type="fig">Figure 5</xref>A&#x02013;E).<fig id="f0025"><label>Figure 5</label><caption><p><bold>Visualizations of Petropoulos dataset using different methods and various annotations</bold></p><p>The 2D visualization of the Petropoulos dataset using PCA (<bold>A</bold>), t-SNE (<bold>B</bold>), ZIFA (<bold>C</bold>), SIMLR (<bold>D</bold>), and VASC (<bold>E</bold>). Cells are annotated with the developmental stages <xref rid="b0090" ref-type="bibr">[18]</xref>. <bold>F.</bold> Cells are annotated as pre-lineage and other cells. <bold>G.</bold> TE cells are further annotated as mural and polar cells. <bold>H.</bold> Cells are annotated with the inferred pseudo time. All the annotations are based on the original study <xref rid="b0090" ref-type="bibr">[18]</xref>. TE, trophectoderm; PE, primitive endoderm; EPI, epiblast.</p></caption><graphic xlink:href="gr5"/></fig></p><p id="p0185">Compared to t-SNE, a sharper split in the grouping was observed in the E5 cells by VASC (<xref rid="f0025" ref-type="fig">Figure 5</xref>B and E). We thus investigated the impact of other annotations on the visualization. We re-annotated the cells with their inferred lineages instead of the developmental stages. Interestingly, we found that the sharp split learned by VASC was a good separation of the pre-lineage cells from the others (<xref rid="f0025" ref-type="fig">Figure 5</xref>F). The inner cell mass (ICM), including the primitive endoderm (PE) and epiblast (EPI), were split from trophectoderm (TE), and the boundary was almost perpendicular to the direction of the developmental stage (<xref rid="f0025" ref-type="fig">Figure 5</xref>F). Furthermore, the two sub-populations of the TE cells, mural and polar cells, were separated in the visualization as well (<xref rid="f0025" ref-type="fig">Figure 5</xref>G). Finally, the trajectory recovered by VASC was strongly coincided with the inferred pseudo time (<xref rid="f0025" ref-type="fig">Figure 5</xref>H).</p><p id="p0190">The candidate genes associated with the pre-implantation embryo development were identified by calculating the Spearman&#x02019;s correlations between the gene expression and the two features shown in the reduced subspace. Many known regulators and markers were found in the top-correlated genes, such as <italic>PGF</italic>, <italic>GCM1</italic>, <italic>CYP19A1</italic>, <italic>MUC15</italic>, <italic>CD24</italic>, <italic>CCR7</italic>, <italic>GREM2</italic>, <italic>CGA</italic>, <italic>GATA2</italic>, <italic>TDGF1</italic>, <italic>ESRG</italic>, <italic>GDF3</italic>, and <italic>DNMT3L</italic> mentioned in the original article <xref rid="b0160" ref-type="bibr">[32]</xref> (rank &#x02264;100 for either feature). Interestingly, the top-ranked genes were significantly enriched in metabolic processes, such as carbohydrate derivative metabolic process (37 genes, <italic>q</italic>&#x0202f;=&#x0202f;5.63E&#x0202f;&#x02212;&#x0202f;05 by DAVID 6.8 <xref rid="b0210" ref-type="bibr">[42]</xref>), oxidation&#x02013;reduction process (32 genes, <italic>q</italic>&#x0202f;=&#x0202f;4.87E&#x0202f;&#x02212;&#x0202f;05), and lipid metabolic process (32 genes, <italic>q</italic>&#x0202f;=&#x0202f;4.94E&#x0202f;&#x02212;&#x0202f;03). Several metabolic pathways have been recently shown to play essential roles in regulating the stemness and differentiation of stem cells <xref rid="b0215" ref-type="bibr">[43]</xref>. Interestingly, we have identified several candidate genes that are involved in different metabolic processes. These include <italic>CYP11A1</italic> (encoding a member of the cytochrome P450 superfamily of enzymes, the same superfamily of CYP19A1), <italic>NR2F2</italic> (encoding a member of the steroid thyroid hormone superfamily of nuclear receptors), <italic>PKM</italic> (encoding a pyruvate kinase, a key kinase in glycolysis), <italic>PPARG</italic> (encoding a member of the peroxisome proliferator-activated receptor subfamily of nuclear receptors), and <italic>IDH1</italic> (encoding an isocitrate dehydrogenase, a key enzyme for cytoplasmic NADPH production).</p></sec><sec id="s0130"><title>Application on a 10&#x000d7; Genomics dataset</title><p id="p0195">We tested VASC on a dataset called PBMC3k <xref rid="b0165" ref-type="bibr">[33]</xref> generated using a new scRNA-seq technology &#x02013; 10&#x000d7; Genomics, which can handle more cells but with a relatively high dropout rate. This dataset contains 2700 cells, each with only &#x0223c;2000 unique molecular identifiers (UMIs). The cells were labeled based on computational predictions and known markers. As shown in <xref rid="f0030" ref-type="fig">Figure 6</xref>A, VASC can clearly distinguish most cell types, such as B cells, CD4<sup>+</sup> T cells, CD8<sup>+</sup> T cells, and NK cells. Cells from common myeloid progenitors, such as dendritic cells, megakaryocytes, and monocytes, were separated from the cells derived from common lymphoid progenitors, like B cells, T cells, and NK cells. Then, we re-ran VASC on the population of monocytes, and consequently further classified them into FCGR3A<sup>+</sup> monocytes and CD14<sup>+</sup> monocytes (<xref rid="f0030" ref-type="fig">Figure 6</xref>B). Therefore, VASC could identify the major global variance structure in the first place, and then detect subtle differences, when it is restricted to a local cell sub-population. These results indicate that VASC could also perform well for the dataset with more cells and higher dropout rate.<fig id="f0030"><label>Figure 6</label><caption><p><bold>Application of VASC in the PBMC3k dataset</bold></p><p>The 2D visualization of VASC on all cells (<bold>A</bold>) and monocytes (<bold>B</bold>). The PBMC3k dataset was downloaded from 10&#x000d7; Genomics (<ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/datasets" id="ir005">https://support.10xgenomics.com/single-cell-gene-expression/datasets</ext-link>).</p></caption><graphic xlink:href="gr6"/></fig></p></sec></sec><sec id="s0135"><title>Discussion</title><p id="p0200">Dimension reduction (or low-dimensional representation) is fundamental to visualization and the downstream analysis of scRNA-seq data. In this study we report VASC, a method based on deep VAE, for dimension reduction and visualization of scRNA-seq data. We evaluate the performance of VASC by comparing with four other commonly-used methods, including PCA, t-SNE, ZIFA, and SIMLR. These methods are broadly divided into two categories. (1) PCA, ZIFA, and VASC aim at finding the representation that can best explain the variations of the original data; and (2) t-SNE and SIMLR try to find another embedded space that can preserve the neighborhood relationship of the samples in the original space. According to our data analysis, the former group of methods can better retain the basic shapes of the data distributions. ZIFA can be treated as a combination of the probabilistic PCA and the zero-inflated model. The major limitation of ZIFA is that it assumes a linear relationship between the hidden subspace and the observed data. Conversely, VASC can deal with complex non-linear patterns based on deep neural networks. Our data show that VASC has better performance than PCA and ZIFA, especially when the sample sizes are larger (<xref rid="f0010" ref-type="fig">Figure 2</xref>, <xref rid="f0015" ref-type="fig">Figure 3</xref>). The two embedding methods in the latter group, t-SNE and SIMLR, frequently change the topology of the original data space. t-SNE tends to &#x0201c;disperse&#x0201d; the cells in the embedded subspace. Compared to t-SNE, SIMLR adds penalties on the modularity of samples in the embedded subspace, which forces the diagonal-block structure of the learned cell&#x02013;cell similarity matrix, and tends to generate compact clusters. This penalty is very useful to identify the cell populations with distinct transcriptomes (for examples, the Pollen dataset). Nevertheless, it frequently fails, if the dataset is generated from studies on &#x0201c;continuous&#x0201d; cell developmental processes or cell lineages. Overall, performance evaluation using multiple datasets demonstrates that VASC is superior in most cases and exhibits broader dataset compatibility.</p><p id="p0205">One major application of scRNA-seq is to identify different cell types at a single cell level. According to the quantitative analyses shown in <xref rid="f0015" ref-type="fig">Figure 3</xref>, the first two dimensions are enough to capture the major differences between different cells in most cases (NMI &#x0003e;0.7 for 16 out of the 20 datasets tested by VASC). Although higher dimensions can explain more variations in the original datasets, additional variations not associated with cell type (for example, the fluctuations associated with cell cycle) may even reduce the separation of different cell types according to our data analysis. The determination of the optimal dimension is a tricky task if prior knowledge is limited. Usually, higher dimensions should be used when investigating more subtle differences, for example, the intra-cell type heterogeneity.</p><p id="p0210">There are two parameters (the mean vector and the co-variance matrix) in the variational distribution Q(<bold><italic>z</italic></bold>|<bold><italic>X</italic></bold>). When the sample size is small, it is better to fix the co-variance matrix. However, when the size is large enough (&#x0003e;1000 according to our preliminary data analysis), a co-variance matrix learnt from the data can generate better results. It is expected that more complex variational distribution families should be tested in the near future, as the sample size of scRNA-seq dataset is quickly increasing.</p><p id="p0215">We also find that the inclusion of ZI layer improves the representation of VASC in terms of recovering the known cell types. Compared to ZIFA, the Gumbel distribution used by the ZI layer does not generate zeroes strictly, which may additionally model the near-zero dropout events. ZIFA is unable to deal with near-zero events, which could be a limitation of ZIFA <xref rid="b0030" ref-type="bibr">[6]</xref>.</p><p id="p0220">The stochastic optimization algorithms, used in the VASC model learning, introduce variations in the dimension reduction. Repeated runs are thus recommended for more consensus performance, although such random effect is small if the sample size is over several hundreds. The running time is a common issue for deep models. For the large dataset with several thousands of cells, it costs several hours for the VASC model learning using a desktop-level computer with single GPU card, which may be acceptable for most scRNA-seq studies.</p></sec><sec id="s0140"><title>Conclusions</title><p id="p0225">In this study, a dimension reduction method, VASC, was developed for scRNA-seq data visualization and analysis. We systematically compared VASC with four state-of-the-art dimension reduction methods on 20 datasets. Results show that VASC achieves superior performance in most cases and is broadly suitable for different datasets with different data structures in the original space. Especially, VASC could make clearer separation of rare cell types than other methods according to our data analysis. The application on a dataset of the human pre-implantation embryo development shows that VASC can re-establish the cell dynamics in the reduced 2D-subspace and identify the associated marker genes.</p></sec><sec id="s0145"><title>Authors&#x02019; contributions</title><p id="p0230">DW and JG designed this study and developed the algorithm. DW made the detailed implementation and performed the data analysis. DW and JG wrote this manuscript. Both authors read and approved the final manuscript.</p></sec><sec id="s0150"><title>Competing interests</title><p id="p0235">The authors declare that they have no competing interests.</p></sec></body><back><ref-list id="bi005"><title>References</title><ref id="b0005"><label>1</label><element-citation publication-type="journal" id="h0005"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>E.</given-names></name><name><surname>Biezuner</surname><given-names>T.</given-names></name><name><surname>Linnarsson</surname><given-names>S.</given-names></name></person-group><article-title>Single-cell sequencing-based technologies will revolutionize whole-organism science</article-title><source>Nat Rev Genet</source><volume>14</volume><year>2013</year><fpage>618</fpage><lpage>630</lpage><pub-id pub-id-type="pmid">23897237</pub-id></element-citation></ref><ref id="b0010"><label>2</label><element-citation publication-type="journal" id="h0010"><person-group person-group-type="author"><name><surname>Stegle</surname><given-names>O.</given-names></name><name><surname>Teichmann</surname><given-names>S.A.</given-names></name><name><surname>Marioni</surname><given-names>J.C.</given-names></name></person-group><article-title>Computational and analytical challenges in single-cell transcriptomics</article-title><source>Nat Rev Genet</source><volume>16</volume><year>2015</year><fpage>133</fpage><lpage>145</lpage><pub-id pub-id-type="pmid">25628217</pub-id></element-citation></ref><ref id="b0015"><label>3</label><element-citation publication-type="journal" id="h0015"><person-group person-group-type="author"><name><surname>Wold</surname><given-names>S.</given-names></name><name><surname>Esbensen</surname><given-names>K.</given-names></name><name><surname>Geladi</surname><given-names>P.</given-names></name></person-group><article-title>Principal component analysis</article-title><source>Chemometr Intell Lab Syst</source><volume>2</volume><year>1987</year><fpage>37</fpage><lpage>52</lpage></element-citation></ref><ref id="b0020"><label>4</label><element-citation publication-type="journal" id="h0020"><person-group person-group-type="author"><name><surname>Lvd</surname><given-names>Maaten</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Visualizing data using t-SNE</article-title><source>J Mach Learn Res</source><volume>9</volume><year>2008</year><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="b0025"><label>5</label><element-citation publication-type="journal" id="h0025"><person-group person-group-type="author"><name><surname>Bacher</surname><given-names>R.</given-names></name><name><surname>Kendziorski</surname><given-names>C.</given-names></name></person-group><article-title>Design and computational analysis of single-cell RNA-sequencing experiments</article-title><source>Genome Biol</source><volume>17</volume><year>2016</year><fpage>63</fpage><pub-id pub-id-type="pmid">27052890</pub-id></element-citation></ref><ref id="b0030"><label>6</label><element-citation publication-type="journal" id="h0030"><person-group person-group-type="author"><name><surname>Pierson</surname><given-names>E.</given-names></name><name><surname>Yau</surname><given-names>C.</given-names></name></person-group><article-title>ZIFA: dimensionality reduction for zero-inflated single-cell gene expression analysis</article-title><source>Genome Biol</source><volume>16</volume><year>2015</year><fpage>241</fpage><pub-id pub-id-type="pmid">26527291</pub-id></element-citation></ref><ref id="b0035"><label>7</label><mixed-citation publication-type="other" id="h0035">Wang B, Ramazzotti D, De Sano L, Zhu J, Pierson E, Batzoglou S. SIMLR: a tool for large-scale single-cell analysis by multi-kernel learning. arXiv 2017, 1703.07844.</mixed-citation></ref><ref id="b0040"><label>8</label><element-citation publication-type="journal" id="h0040"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G.E.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.R.</given-names></name></person-group><article-title>Reducing the dimensionality of data with neural networks</article-title><source>Science</source><volume>313</volume><year>2006</year><fpage>504</fpage><lpage>507</lpage><pub-id pub-id-type="pmid">16873662</pub-id></element-citation></ref><ref id="b0045"><label>9</label><mixed-citation publication-type="other" id="h0045">Kingma DP, Welling M. Auto-encoding variational bayes. arXiv 2013, 1312.6114.</mixed-citation></ref><ref id="b0050"><label>10</label><element-citation publication-type="journal" id="h0050"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group><article-title>Efficient gradient-based inference through transformations between bayes nets and neural nets</article-title><source>Int Conf Mach Learn</source><year>2014</year><fpage>1782</fpage><lpage>1790</lpage></element-citation></ref><ref id="b0055"><label>11</label><mixed-citation publication-type="other" id="h0055">Doersch C. Tutorial on variational autoencoders. arXiv 2016,1606.05908.</mixed-citation></ref><ref id="b0060"><label>12</label><element-citation publication-type="journal" id="h0060"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N.</given-names></name><name><surname>Hinton</surname><given-names>G.E.</given-names></name><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group><article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title><source>J Mach Learn Res</source><volume>15</volume><year>2014</year><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="b0065"><label>13</label><element-citation publication-type="journal" id="h0065"><person-group person-group-type="author"><name><surname>Kharchenko</surname><given-names>P.V.</given-names></name><name><surname>Silberstein</surname><given-names>L.</given-names></name><name><surname>Scadden</surname><given-names>D.T.</given-names></name></person-group><article-title>Bayesian approach to single-cell differential expression analysis</article-title><source>Nat Methods</source><volume>11</volume><year>2014</year><fpage>740</fpage><lpage>742</lpage><pub-id pub-id-type="pmid">24836921</pub-id></element-citation></ref><ref id="b0070"><label>14</label><element-citation publication-type="book" id="h0070"><person-group person-group-type="author"><name><surname>Gumbel</surname><given-names>E.J.</given-names></name></person-group><chapter-title>Statistical theory of extreme values and some practical applications: a series of lectures</chapter-title><year>1954</year><publisher-name>US Government Print Office</publisher-name><publisher-loc>Washington</publisher-loc></element-citation></ref><ref id="b0075"><label>15</label><mixed-citation publication-type="other" id="h0075">Jang E, Gu S, Poole B. Categorical reparameterization with gumbel-softmax. arXiv 2016, 1611.01144.</mixed-citation></ref><ref id="b0080"><label>16</label><element-citation publication-type="journal" id="h0080"><person-group person-group-type="author"><name><surname>Tieleman</surname><given-names>T.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude</article-title><source>COURSERA: Neural Networks Mach Learn</source><volume>4</volume><year>2012</year><fpage>26</fpage><lpage>31</lpage></element-citation></ref><ref id="b0085"><label>17</label><element-citation publication-type="journal" id="h0085"><person-group person-group-type="author"><name><surname>Baron</surname><given-names>M.</given-names></name><name><surname>Veres</surname><given-names>A.</given-names></name><name><surname>Wolock</surname><given-names>S.L.</given-names></name><name><surname>Faust</surname><given-names>A.L.</given-names></name><name><surname>Gaujoux</surname><given-names>R.</given-names></name><name><surname>Vetere</surname><given-names>A.</given-names></name></person-group><article-title>A single-cell transcriptomic map of the human and mouse pancreas reveals inter-and intra-cell population structure</article-title><source>Cell Syst</source><volume>3</volume><year>2016</year><comment>346&#x02212;60.e4</comment></element-citation></ref><ref id="b0090"><label>18</label><element-citation publication-type="journal" id="h0090"><person-group person-group-type="author"><name><surname>Biase</surname><given-names>F.H.</given-names></name><name><surname>Cao</surname><given-names>X.</given-names></name><name><surname>Zhong</surname><given-names>S.</given-names></name></person-group><article-title>Cell fate inclination within 2-cell and 4-cell mouse embryos revealed by single-cell RNA sequencing</article-title><source>Genome Res</source><volume>24</volume><year>2014</year><fpage>1787</fpage><lpage>1796</lpage><pub-id pub-id-type="pmid">25096407</pub-id></element-citation></ref><ref id="b0095"><label>19</label><element-citation publication-type="journal" id="h0095"><person-group person-group-type="author"><name><surname>Camp</surname><given-names>J.G.</given-names></name><name><surname>Sekine</surname><given-names>K.</given-names></name><name><surname>Gerber</surname><given-names>T.</given-names></name><name><surname>Loeffler-Wirth</surname><given-names>H.</given-names></name></person-group><article-title>Multilineage communication regulates human liver bud development</article-title><source>Nature</source><volume>546</volume><year>2017</year><fpage>533</fpage><lpage>538</lpage><pub-id pub-id-type="pmid">28614297</pub-id></element-citation></ref><ref id="b0100"><label>20</label><element-citation publication-type="journal" id="h0100"><person-group person-group-type="author"><name><surname>Darmanis</surname><given-names>S.</given-names></name><name><surname>Sloan</surname><given-names>S.A.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Enge</surname><given-names>M.</given-names></name><name><surname>Caneda</surname><given-names>C.</given-names></name><name><surname>Shuer</surname><given-names>L.M.</given-names></name></person-group><article-title>A survey of human brain transcriptome diversity at the single cell level</article-title><source>Proc Natl Acad Sci U S A</source><volume>112</volume><year>2015</year><fpage>7285</fpage><lpage>7290</lpage><pub-id pub-id-type="pmid">26060301</pub-id></element-citation></ref><ref id="b0105"><label>21</label><element-citation publication-type="journal" id="h0105"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>Q.</given-names></name><name><surname>Ramsk&#x000f6;ld</surname><given-names>D.</given-names></name><name><surname>Reinius</surname><given-names>B.</given-names></name><name><surname>Sandberg</surname><given-names>R.</given-names></name></person-group><article-title>Single-cell RNA-seq reveals dynamic, random monoallelic gene expression in mammalian cells</article-title><source>Science</source><volume>343</volume><year>2014</year><fpage>193</fpage><lpage>196</lpage><pub-id pub-id-type="pmid">24408435</pub-id></element-citation></ref><ref id="b0110"><label>22</label><element-citation publication-type="journal" id="h0110"><person-group person-group-type="author"><name><surname>Goolam</surname><given-names>M.</given-names></name><name><surname>Scialdone</surname><given-names>A.</given-names></name><name><surname>Graham</surname><given-names>S.J.</given-names></name><name><surname>Macaulay</surname><given-names>I.C.</given-names></name><name><surname>Jedrusik</surname><given-names>A.</given-names></name><name><surname>Hupalowska</surname><given-names>A.</given-names></name></person-group><article-title>Heterogeneity in Oct4 and Sox2 targets biases cell fate in 4-cell mouse embryos</article-title><source>Cell</source><volume>165</volume><year>2016</year><fpage>61</fpage><lpage>74</lpage><pub-id pub-id-type="pmid">27015307</pub-id></element-citation></ref><ref id="b0115"><label>23</label><element-citation publication-type="journal" id="h0115"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A.M.</given-names></name><name><surname>Mazutis</surname><given-names>L.</given-names></name><name><surname>Akartuna</surname><given-names>I.</given-names></name><name><surname>Tallapragada</surname><given-names>N.</given-names></name><name><surname>Veres</surname><given-names>A.</given-names></name><name><surname>Li</surname><given-names>V.</given-names></name></person-group><article-title>Droplet barcoding for single-cell transcriptomics applied to embryonic stem cells</article-title><source>Cell</source><volume>161</volume><year>2015</year><fpage>1187</fpage><lpage>1201</lpage><pub-id pub-id-type="pmid">26000487</pub-id></element-citation></ref><ref id="b0120"><label>24</label><element-citation publication-type="journal" id="h0120"><person-group person-group-type="author"><name><surname>Kolodziejczyk</surname><given-names>A.A.</given-names></name><name><surname>Kim</surname><given-names>J.K.</given-names></name><name><surname>Tsang</surname><given-names>J.C.</given-names></name><name><surname>Ilicic</surname><given-names>T.</given-names></name><name><surname>Henriksson</surname><given-names>J.</given-names></name><name><surname>Natarajan</surname><given-names>K.N.</given-names></name></person-group><article-title>Single cell RNA-sequencing of pluripotent states unlocks modular transcriptional variation</article-title><source>Cell Stem Cell</source><volume>17</volume><year>2015</year><fpage>471</fpage><lpage>485</lpage><pub-id pub-id-type="pmid">26431182</pub-id></element-citation></ref><ref id="b0125"><label>25</label><element-citation publication-type="journal" id="h0125"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Courtois</surname><given-names>E.T.</given-names></name><name><surname>Sengupta</surname><given-names>D.</given-names></name><name><surname>Tan</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>K.H.</given-names></name><name><surname>Goh</surname><given-names>J.J.L.</given-names></name></person-group><article-title>Reference component analysis of single-cell transcriptomes elucidates cellular heterogeneity in human colorectal tumors</article-title><source>Nat Genet</source><volume>49</volume><year>2017</year><fpage>708</fpage><lpage>718</lpage><pub-id pub-id-type="pmid">28319088</pub-id></element-citation></ref><ref id="b0130"><label>26</label><element-citation publication-type="journal" id="h0130"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>A.P.</given-names></name><name><surname>Tirosh</surname><given-names>I.</given-names></name><name><surname>Trombetta</surname><given-names>J.J.</given-names></name><name><surname>Shalek</surname><given-names>A.K.</given-names></name><name><surname>Gillespie</surname><given-names>S.M.</given-names></name><name><surname>Wakimoto</surname><given-names>H.</given-names></name></person-group><article-title>Single-cell RNA-seq highlights intratumoral heterogeneity in primary glioblastoma</article-title><source>Science</source><volume>344</volume><year>2014</year><fpage>1396</fpage><lpage>1401</lpage><pub-id pub-id-type="pmid">24925914</pub-id></element-citation></ref><ref id="b0135"><label>27</label><element-citation publication-type="journal" id="h0135"><person-group person-group-type="author"><name><surname>Pollen</surname><given-names>A.A.</given-names></name><name><surname>Nowakowski</surname><given-names>T.J.</given-names></name><name><surname>Shuga</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Leyrat</surname><given-names>A.A.</given-names></name><name><surname>Lui</surname><given-names>J.H.</given-names></name></person-group><article-title>Low-coverage single-cell mRNA sequencing reveals cellular heterogeneity and activated signaling pathways in developing cerebral cortex</article-title><source>Nat Biotechnol</source><volume>32</volume><year>2014</year><fpage>1053</fpage><lpage>1058</lpage><pub-id pub-id-type="pmid">25086649</pub-id></element-citation></ref><ref id="b0140"><label>28</label><element-citation publication-type="journal" id="h0140"><person-group person-group-type="author"><name><surname>Usoskin</surname><given-names>D.</given-names></name><name><surname>Furlan</surname><given-names>A.</given-names></name><name><surname>Islam</surname><given-names>S.</given-names></name><name><surname>Abdo</surname><given-names>H.</given-names></name><name><surname>L&#x000f6;nnerberg</surname><given-names>P.</given-names></name><name><surname>Lou</surname><given-names>D.</given-names></name></person-group><article-title>Unbiased classification of sensory neuron types by large-scale single-cell RNA sequencing</article-title><source>Nat Neurosci</source><volume>18</volume><year>2015</year><fpage>145</fpage><lpage>153</lpage><pub-id pub-id-type="pmid">25420068</pub-id></element-citation></ref><ref id="b0145"><label>29</label><element-citation publication-type="journal" id="h0145"><person-group person-group-type="author"><name><surname>Xin</surname><given-names>Y.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Okamoto</surname><given-names>H.</given-names></name><name><surname>Ni</surname><given-names>M.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Adler</surname><given-names>C.</given-names></name></person-group><article-title>RNA sequencing of single human islet cells reveals type 2 diabetes genes</article-title><source>Cell Metab</source><volume>24</volume><year>2016</year><fpage>608</fpage><lpage>615</lpage><pub-id pub-id-type="pmid">27667665</pub-id></element-citation></ref><ref id="b0150"><label>30</label><element-citation publication-type="journal" id="h0150"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>M.</given-names></name><name><surname>Guo</surname><given-names>H.</given-names></name><name><surname>Yang</surname><given-names>L.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>R.</given-names></name></person-group><article-title>Single-cell RNA-Seq profiling of human preimplantation embryos and embryonic stem cells</article-title><source>Nat Struct Mol Biol</source><volume>20</volume><year>2013</year><fpage>1131</fpage><lpage>1139</lpage><pub-id pub-id-type="pmid">23934149</pub-id></element-citation></ref><ref id="b0155"><label>31</label><element-citation publication-type="journal" id="h0155"><person-group person-group-type="author"><name><surname>Zeisel</surname><given-names>A.</given-names></name><name><surname>Mu&#x000f1;oz-Manchado</surname><given-names>A.B.</given-names></name><name><surname>Codeluppi</surname><given-names>S.</given-names></name><name><surname>L&#x000f6;nnerberg</surname><given-names>P.</given-names></name><name><surname>La Manno</surname><given-names>G.</given-names></name><name><surname>Jur&#x000e9;us</surname><given-names>A.</given-names></name></person-group><article-title>Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq</article-title><source>Science</source><volume>347</volume><year>2015</year><fpage>1138</fpage><lpage>1142</lpage><pub-id pub-id-type="pmid">25700174</pub-id></element-citation></ref><ref id="b0160"><label>32</label><element-citation publication-type="journal" id="h0160"><person-group person-group-type="author"><name><surname>Petropoulos</surname><given-names>S.</given-names></name><name><surname>Edsg&#x000e4;rd</surname><given-names>D.</given-names></name><name><surname>Reinius</surname><given-names>B.</given-names></name><name><surname>Deng</surname><given-names>Q.</given-names></name><name><surname>Panula</surname><given-names>S.P.</given-names></name><name><surname>Codeluppi</surname><given-names>S.</given-names></name></person-group><article-title>Single-cell RNA-seq reveals lineage and X chromosome dynamics in human preimplantation embryos</article-title><source>Cell</source><volume>165</volume><year>2016</year><fpage>1012</fpage><lpage>1026</lpage><pub-id pub-id-type="pmid">27062923</pub-id></element-citation></ref><ref id="b0165"><label>33</label><element-citation publication-type="journal" id="h0165"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>G.X.</given-names></name><name><surname>Terry</surname><given-names>J.M.</given-names></name><name><surname>Belgrader</surname><given-names>P.</given-names></name><name><surname>Ryvkin</surname><given-names>P.</given-names></name><name><surname>Bent</surname><given-names>Z.W.</given-names></name><name><surname>Wilson</surname><given-names>R.</given-names></name></person-group><article-title>Massively parallel digital transcriptional profiling of single cells</article-title><source>Nat Commun</source><volume>8</volume><year>2017</year><fpage>14049</fpage><pub-id pub-id-type="pmid">28091601</pub-id></element-citation></ref><ref id="b0170"><label>34</label><element-citation publication-type="journal" id="h0170"><person-group person-group-type="author"><name><surname>McCarthy</surname><given-names>D.J.</given-names></name><name><surname>Campbell</surname><given-names>K.R.</given-names></name><name><surname>Lun</surname><given-names>A.T.</given-names></name><name><surname>Wills</surname><given-names>Q.F.</given-names></name></person-group><article-title>Scater: pre-processing, quality control, normalization and visualization of single-cell RNA-seq data in R</article-title><source>Bioinformatics</source><volume>33</volume><year>2017</year><fpage>1179</fpage><lpage>1186</lpage><pub-id pub-id-type="pmid">28088763</pub-id></element-citation></ref><ref id="b0175"><label>35</label><element-citation publication-type="journal" id="h0175"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>P.</given-names></name><name><surname>Larochelle</surname><given-names>H.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Manzagol</surname><given-names>P.A.</given-names></name></person-group><article-title>Extracting and composing robust features with denoising autoencoders</article-title><source>Proc 25th Int Conf Mach Learn</source><year>2008</year><fpage>1096</fpage><lpage>1103</lpage></element-citation></ref><ref id="b0180"><label>36</label><element-citation publication-type="journal" id="h0180"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Adv Neural Inform Process Syst</source><volume>25</volume><year>2012</year><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="b0185"><label>37</label><element-citation publication-type="journal" id="h0185"><person-group person-group-type="author"><name><surname>Strehl</surname><given-names>A.</given-names></name><name><surname>Ghosh</surname><given-names>J.</given-names></name></person-group><article-title>Cluster ensembles&#x02014;a knowledge reuse framework for combining multiple partitions</article-title><source>J Mach Learn Res</source><volume>3</volume><year>2002</year><fpage>583</fpage><lpage>617</lpage></element-citation></ref><ref id="b0190"><label>38</label><element-citation publication-type="journal" id="h0190"><person-group person-group-type="author"><name><surname>Hubert</surname><given-names>L.</given-names></name><name><surname>Arabie</surname><given-names>P.</given-names></name></person-group><article-title>Comparing partitions</article-title><source>J Classif</source><volume>2</volume><year>1985</year><fpage>193</fpage><lpage>218</lpage></element-citation></ref><ref id="b0195"><label>39</label><element-citation publication-type="journal" id="h0195"><person-group person-group-type="author"><name><surname>Vinh</surname><given-names>N.X.</given-names></name><name><surname>Epps</surname><given-names>J.</given-names></name><name><surname>Bailey</surname><given-names>J.</given-names></name></person-group><article-title>Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</article-title><source>J Mach Learn Res</source><volume>11</volume><year>2010</year><fpage>2837</fpage><lpage>2854</lpage></element-citation></ref><ref id="b0200"><label>40</label><element-citation publication-type="journal" id="h0200"><person-group person-group-type="author"><name><surname>Brennecke</surname><given-names>P.</given-names></name><name><surname>Anders</surname><given-names>S.</given-names></name><name><surname>Kim</surname><given-names>J.K.</given-names></name><name><surname>Kolodziejczyk</surname><given-names>A.A.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Proserpio</surname><given-names>V.</given-names></name></person-group><article-title>Accounting for technical noise in single-cell RNA-seq experiments</article-title><source>Nat Methods</source><volume>10</volume><year>2013</year><fpage>1093</fpage><lpage>1095</lpage><pub-id pub-id-type="pmid">24056876</pub-id></element-citation></ref><ref id="b0205"><label>41</label><element-citation publication-type="journal" id="h0205"><person-group person-group-type="author"><name><surname>Hartigan</surname><given-names>J.A.</given-names></name><name><surname>Wong</surname><given-names>M.A.</given-names></name></person-group><article-title>Algorithm AS 136: a k-means clustering algorithm</article-title><source>J R Stat Soc Ser C Appl Stat</source><volume>28</volume><year>1979</year><fpage>100</fpage><lpage>108</lpage></element-citation></ref><ref id="b0210"><label>42</label><element-citation publication-type="journal" id="h0210"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>D.W.</given-names></name><name><surname>Sherman</surname><given-names>B.T.</given-names></name><name><surname>Lempicki</surname><given-names>R.A.</given-names></name></person-group><article-title>Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources</article-title><source>Nat Protoc</source><volume>4</volume><year>2009</year><fpage>44</fpage><lpage>57</lpage><pub-id pub-id-type="pmid">19131956</pub-id></element-citation></ref><ref id="b0215"><label>43</label><element-citation publication-type="journal" id="h0215"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>K.</given-names></name><name><surname>Suda</surname><given-names>T.</given-names></name></person-group><article-title>Metabolic requirements for the maintenance of self-renewing stem cells</article-title><source>Nat Rev Mol Cell Biol</source><volume>15</volume><year>2014</year><fpage>243</fpage><lpage>256</lpage><pub-id pub-id-type="pmid">24651542</pub-id></element-citation></ref></ref-list><sec id="s0160" sec-type="supplementary-material"><title>Supplementary material</title><p id="p0250">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="m0005"><caption><title>Supplementary File S1</title><p><bold>Detailed model description and performance assessment of VASC</bold></p></caption><media xlink:href="mmc1.pdf"/></supplementary-material></p></sec><ack id="ak005"><title>Acknowledgments</title><p>We thank Xiangyu Li, Jianyang Zeng, Michael Zhang, and Jun Li for their helpful discussions. We address special thanks to the share of single-cell datasets by Hemberg group from the <funding-source id="gp015">Wellcome Trust</funding-source> Sanger Institute. This work is supported by the National Natural Science Foundation of China (Grant Nos. 61370035 and 31361163004) and Tsinghua University Initiative Scientific Research Program.</p></ack><fn-group><fn id="d31e706"><p id="np005">Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.</p></fn><fn id="s0155" fn-type="supplementary-material"><p id="p0245">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.gpb.2018.08.003" id="ir035">https://doi.org/10.1016/j.gpb.2018.08.003</ext-link>.</p></fn></fn-group></back></article>