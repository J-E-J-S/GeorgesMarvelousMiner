<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="letter"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Genome Biol</journal-id><journal-id journal-id-type="iso-abbrev">Genome Biol</journal-id><journal-title-group><journal-title>Genome Biology</journal-title></journal-title-group><issn pub-type="ppub">1474-7596</issn><issn pub-type="epub">1474-760X</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6737594</article-id><article-id pub-id-type="publisher-id">1794</article-id><article-id pub-id-type="doi">10.1186/s13059-019-1794-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Open Letter</subject></subj-group></article-categories><title-group><article-title>Reproducible biomedical benchmarking in the cloud: lessons from crowd-sourced data challenges</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ellrott</surname><given-names>Kyle</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Buchanan</surname><given-names>Alex</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Creason</surname><given-names>Allison</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Mason</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Schaffter</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Hoff</surname><given-names>Bruce</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Eddy</surname><given-names>James</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Chilton</surname><given-names>John M.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Stuart</surname><given-names>Joshua M.</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Saez-Rodriguez</surname><given-names>Julio</given-names></name><xref ref-type="aff" rid="Aff6">6</xref><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Stolovitzky</surname><given-names>Gustavo</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Boutros</surname><given-names>Paul C.</given-names></name><xref ref-type="aff" rid="Aff8">8</xref><xref ref-type="aff" rid="Aff9">9</xref><xref ref-type="aff" rid="Aff10">10</xref><xref ref-type="aff" rid="Aff11">11</xref><xref ref-type="aff" rid="Aff12">12</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1477-1888</contrib-id><name><surname>Guinney</surname><given-names>Justin</given-names></name><address><email>justin.guinney@sagebase.org</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff13">13</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9758 5690</institution-id><institution-id institution-id-type="GRID">grid.5288.7</institution-id><institution>Biomedical Engineering, </institution><institution>Oregon Health and Science University, </institution></institution-wrap>Portland, OR 97239 USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 6023 5303</institution-id><institution-id institution-id-type="GRID">grid.430406.5</institution-id><institution>Sage Bionetworks, </institution></institution-wrap>Seattle, WA USA </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.481554.9</institution-id><institution>IBM Research, </institution></institution-wrap>Yorktown Heights, NY USA </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2097 4281</institution-id><institution-id institution-id-type="GRID">grid.29857.31</institution-id><institution>Department of Biochemistry and Molecular Biology, </institution><institution>The Pennsylvania State University, </institution></institution-wrap>University Park, State College, PA USA </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0740 6917</institution-id><institution-id institution-id-type="GRID">grid.205975.c</institution-id><institution>University of California, Santa Cruz, </institution></institution-wrap>Santa Cruz, CA USA </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2190 4373</institution-id><institution-id institution-id-type="GRID">grid.7700.0</institution-id><institution>Institute for Computational Biomedicine, Heidelberg University, Faculty of Medicine and Heidelberg University Hospital, Bioquant, </institution></institution-wrap>Heidelberg, Germany </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0728 696X</institution-id><institution-id institution-id-type="GRID">grid.1957.a</institution-id><institution>Joint Research Center for Computational Biomedicine, </institution><institution>RWTH Aachen University, Faculty of Medicine, </institution></institution-wrap>Aachen, Germany </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0626 690X</institution-id><institution-id institution-id-type="GRID">grid.419890.d</institution-id><institution>Ontario Institute for Cancer Research, </institution></institution-wrap>Toronto, Canada </aff><aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2157 2938</institution-id><institution-id institution-id-type="GRID">grid.17063.33</institution-id><institution>Departments of Medical Biophysics and Pharmacology &#x00026; Toxicology, University of Toronto, </institution></institution-wrap>Toronto, Canada </aff><aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution>Departments of Human Genetics and Urology, </institution><institution>University of California, </institution></institution-wrap>Los Angeles, CA USA </aff><aff id="Aff11"><label>11</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution>Jonsson Comprehensive Cancer Centre, </institution><institution>University of California, </institution></institution-wrap>Los Angeles, CA USA </aff><aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution>Institute for Precision Health, </institution><institution>University of California, </institution></institution-wrap>Los Angeles, CA USA </aff><aff id="Aff13"><label>13</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122986657</institution-id><institution-id institution-id-type="GRID">grid.34477.33</institution-id><institution>Biomedical Informatics and Medical Education, </institution><institution>University of Washington, </institution></institution-wrap>Seattle, WA 98195 USA </aff></contrib-group><pub-date pub-type="epub"><day>10</day><month>9</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>10</day><month>9</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>20</volume><elocation-id>195</elocation-id><history><date date-type="received"><day>25</day><month>4</month><year>2019</year></date><date date-type="accepted"><day>13</day><month>8</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s). 2019</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Challenges are achieving broad acceptance for addressing many biomedical questions and enabling tool assessment. But ensuring that the methods evaluated are reproducible and reusable is complicated by the diversity of software architectures, input and output file formats, and computing environments. To mitigate these problems, some challenges have leveraged new virtualization and compute methods, requiring participants to submit cloud-ready software packages. We review recent data challenges with innovative approaches to model reproducibility and data sharing, and outline key lessons for improving quantitative biomedical data analysis through crowd-sourced benchmarking challenges.</p><sec><title>Electronic supplementary material</title><p>The online version of this article (10.1186/s13059-019-1794-0) contains supplementary material, which is available to authorized users.</p></sec></abstract><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000054</institution-id><institution>National Cancer Institute</institution></institution-wrap></funding-source><award-id>P30CA016042</award-id><award-id>R01CA180778</award-id><award-id>5U24CA209923</award-id><principal-award-recipient><name><surname>Guinney</surname><given-names>Justin</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The role of the <italic>algorithm</italic> in biomedical research has been growing steadily, propelled by technological advances in the high-throughput capture of molecular, cellular, and clinical states. The complexity and volume of diverse data types&#x02014;spanning omics, imaging, and clinical phenotyping&#x02014;require similarly complex pipelines and algorithms for processing and interpretation. Despite the central role of algorithms in supporting the biomedical research community, mechanisms for their distribution, evaluation, and comparison are lacking. Today, the predominant paradigm for algorithm assessment is self-reporting, a conflict of interest known as the &#x0201c;self-assessment trap&#x0201d; [<xref ref-type="bibr" rid="CR1">1</xref>]. By definition, self-assessment of an algorithm is highly biased and can mask critical problems such as overfitting, incomplete documentation, software portability, and poor generalizability. These issues collectively impede the successful utilization and translation of algorithms in the lab and the clinic.</p><p id="Par3">Crowd-sourced data challenges are an increasingly popular mechanism to address the aforementioned shortcomings of method development. Data challenges incentivize teams to work on complex problems, and provide a robust and unbiased framework for assessing the performance of resulting methods [<xref ref-type="bibr" rid="CR2">2</xref>]. The DREAM Challenges are an example of a data challenge community focused on the rigorous assessment of biomedical tools and algorithms, with over 50 completed challenges over the last decade [<xref ref-type="bibr" rid="CR3">3</xref>]. As DREAM has evolved with its communities, it has needed to confront a critical problem&#x02014;many current algorithmic problems cannot be easily evaluated using <italic>open data</italic>. Rather, concerns around data size and privacy are making it increasingly difficult to transfer datasets to participants for their evaluation. To resolve this problem, several alternative forms of data sharing have been explored, and a paradigm described as &#x0201c;model to data&#x0201d; (M2D) has emerged [<xref ref-type="bibr" rid="CR4">4</xref>] and Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). In M2D, the underlying dataset remains hidden from users; rather, models are moved to the data for execution and evaluation in protected compute environments. In addition to solving model reproducibility problems, model to data challenges enable assessment of models on future (i.e., prospective) data sets and facilitate <italic>continuous benchmarking</italic> as new models and data sets emerge.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Challenge cycle overview. For each challenge, participants can form teams of one or more individuals. Challenge teams work together to develop a model (depicted as open box), train their model on training data (purple cylinders) provided by the challenge organizers, containerize their model (closed box with outline), and submit their model to the challenge container repository. Submitted models are run on validation data (green cylinders) on a cloud computing system by the challenge organizers. Once predictions produced by the models are evaluated and scored, results are made available to the challenge teams. Teams can use this information to make improvements to their model and resubmit their optimized model</p></caption><graphic xlink:href="13059_2019_1794_Fig1_HTML" id="MO1"/></fig></p><p id="Par4">DREAM has now successfully completed several M2D challenges, demonstrating the feasibility and utility of this paradigm. Each M2D challenge has revealed unique logistical and technological hurdles associated with data storage and access, scalability of compute resources, modularity of pipelines and algorithms, and the complexity of training models in a cloud environment. These challenges have also revealed important lessons on how to leverage cloud and virtualization technologies, how to utilize protected and sensitive data, and how to engage communities in solving complex biomedical problems. Here, we review five M2D challenges covering a broad range of scientific questions and data types. We highlight key lessons on benchmarking, challenge execution, model reproducibility, and data sharing. These lessons provide concrete steps for optimizing future cloud-based biomedical data challenges and also serve as a roadmap for creating a distributed benchmarking ecosystem that connects algorithms to data.</p></sec><sec id="Sec2"><title>M2D challenges overview</title><p id="Par5">The M2D challenges examined here address a common problem: how to facilitate the training and evaluation of algorithms on hidden data at scale using cloud resources. This problem is addressed in different ways, depending on the unique technical and scientific constraints of each challenge. The variety of approaches is summarized in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> across five areas: (i) cloud environment, (ii) compute requirement, (iii) data generation method, (iv) data type, and (v) form of submitted model (algorithm). Here, we briefly introduce each of the challenges before describing the lessons learned with respect to implementation of the M2D paradigm.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Challenge features. Challenges used cloud computing services for running and evaluating models including Google Cloud Platform, Openstack, Amazon Web Services, and IBM Cloud. Models were designed to run using either CPUs or GPUs. The type of data used in running and evaluation of models was either real data (obtained from patients or cell lines) or simulated using a computer algorithm. Challenges used genomic data, such as DNA sequencing, RNA sequencing, and gene expression; clinical phenotypes; and/or images. Models could be submitted to a challenge in the form of a galaxy workflow, docker image, or CWL (Common Workflow Language) workflow</p></caption><graphic xlink:href="13059_2019_1794_Fig2_HTML" id="MO2"/></fig></p><sec id="Sec3"><title>Digital Mammography Challenge</title><p id="Par6">The Digital Mammography (DM) DREAM Challenge was a data challenge designed to develop and assess algorithms for improved breast cancer detection [<xref ref-type="bibr" rid="CR5">5</xref>]. The DM Challenge encouraged the use of deep learning methods applied to a large image repository of screening mammograms, with the goal of reducing the ~&#x02009;10% false-positive rate of screening mammography [<xref ref-type="bibr" rid="CR6">6</xref>]. The Challenge asked participants to train and validate models that identify women with breast cancer using a hidden data cohort of screening images and limited demographic information.</p><p id="Par7">The Challenge utilized multiple independent data cohorts for training and validation (see Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>), with Kaiser Permanente Washington contributing the primary challenge cohort. The condition of use for all images dictated that the <italic>images could not be distributed directly to participants</italic>, thereby requiring the M2D paradigm whereby participants submitted containerized models to challenge organizers. Participants were able to submit three containerized pipelines for handling data pre-processing, model training, and model prediction which were then run by the challenge organizers within protected cloud environments (see Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). Given the large data sets and deep learning requirements, computational resources available to participants included access to GPUs and large storage capacity. The Challenge resulted in 57 teams submitting 310 models during the 7&#x000a0;months of the Challenge. These models established the first-ever benchmarks of deep learning methods for detecting cancer from screening mammograms, with results to be published in a forthcoming manuscript.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Challenge data characteristics</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Challenge</th><th>Data types</th><th>Data cohorts</th><th><italic>N</italic> samples</th><th>Size</th><th>Open</th></tr></thead><tbody><tr><td rowspan="6">Digital Mammography</td><td rowspan="6">Human clinical Imaging</td><td>Kaiser Permanente</td><td>80k patients (640k images)</td><td>13&#x02009;TB</td><td>No</td></tr><tr><td>MSSM</td><td>1k (15k)</td><td>.3&#x02009;TB</td><td>No</td></tr><tr><td>Karolinska</td><td>69k (663k)</td><td>13.2&#x02009;TB</td><td>No</td></tr><tr><td>UCSF</td><td>42k (500k)</td><td>10&#x02009;TB</td><td>No</td></tr><tr><td>CRUK</td><td>7&#x02009;k</td><td/><td>No</td></tr><tr><td>Total</td><td>200k (1818k)</td><td>36.5&#x02009;TB</td><td/></tr><tr><td rowspan="6">Multiple Myeloma</td><td rowspan="6">Human clinical; gene expr; DNAseq; Cytogenetics</td><td>MMRF</td><td>797</td><td>11&#x02009;GB</td><td>Yes</td></tr><tr><td>PUBLIC</td><td>1444</td><td>1&#x02009;GB</td><td>Yes</td></tr><tr><td>DFCI</td><td>294</td><td>76&#x02009;GB</td><td>No</td></tr><tr><td>UAMS</td><td>463</td><td>6&#x02009;GB</td><td>No</td></tr><tr><td>M2Gen</td><td>105</td><td>41&#x02009;GB</td><td>No</td></tr><tr><td>Total</td><td>3103</td><td>135&#x02009;GB</td><td/></tr><tr><td>SMC-Het</td><td/><td>All</td><td>76</td><td>22&#x02009;GB</td><td>No</td></tr><tr><td rowspan="3">SMC-RNA</td><td rowspan="3">Simulated; Human clinical; RNA-seq</td><td>Training</td><td>31</td><td>290&#x02009;GB</td><td>Yes</td></tr><tr><td>Test</td><td>20</td><td>197&#x02009;GB</td><td>Yes</td></tr><tr><td>Real</td><td>32</td><td>265&#x02009;GB</td><td>No</td></tr></tbody></table><table-wrap-foot><p>Data cohorts describe the source of the data used in the challenge. <italic>MSSM</italic> Mount Sinai School of Medicine, <italic>UCSF</italic> University of California San Francisco, <italic>CRUK</italic> Cancer Research UK, <italic>MMRF</italic> Multiple Myeloma Research Foundation, <italic>DFCI</italic> Dana-Farber Cancer Institute, <italic>UAMS</italic> University of Arkansas for Medical Sciences, <italic>Training</italic> synthetically generated data provided to participants, <italic>Test</italic> synthetically generated data held-out data, <italic>Real</italic> cell lines spiked in with known constructs. The number of samples in digital mammography includes the number of patients and the number of images in parentheses. Open indicates whether the data was publicly available to participants</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of models and teams for challenges</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Challenge</th><th>Cloud platforms</th><th>Model format</th><th># of models</th><th># of teams</th></tr></thead><tbody><tr><td>Digital Mammography</td><td>AWS, IBM Softlayer</td><td>Docker</td><td>310</td><td>57 teams</td></tr><tr><td>Multiple Myeloma</td><td>AWS</td><td>Docker</td><td>180</td><td>71</td></tr><tr><td>SMC-Het</td><td>ISB-CGC (Google)</td><td>Galaxy, Docker</td><td>58</td><td>31</td></tr><tr><td>SMC-RNA</td><td>ISG-CGC (Google)</td><td>CWL, Docker</td><td>141</td><td>16</td></tr><tr><td>Proteogenomic</td><td>AWS</td><td>Docker</td><td>449</td><td>68</td></tr></tbody></table><table-wrap-foot><p>Number of participants from each challenge, as well as model types and submission counts</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec4"><title>Multiple Myeloma Challenge</title><p id="Par8">Multiple myeloma (MM) is a cancer of the plasma cells in the bone marrow, and therapeutic strategies and clinical course depend on a complex interplay of clinical and molecular features. Risk-based therapy is becoming standard of care, creating an urgent need for precise risk stratification model to assist in therapeutic decision-making. The MM DREAM Challenge aimed to accelerate the development and evaluation of such risk models. Previous MM risk models using clinical, genomic, and transcriptomic data have been published [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>], yet no objective and systematic assessment of these models has been conducted and none of these has yet been adopted for routine clinical use.</p><p id="Par9">The MM Challenge was structured to provide participants access to large and robust data sets for model training, while utilizing unpublished and proprietary data for unbiased model validation. Validation data sets were acquired from commercial and academic entities on the condition that the data sets could not be directly shared with challenge participants. Consequently, teams were required to submit fully trained and Dockerized models that could be applied to these validation data sets, which included combinations of clinical, genomic, and transcriptomic data. Models were then scored according to their ability to predict disease-free survival in multiple patient cohorts. Well-regarded published models based on gene expression or genomic variants were used as state-of-the-art benchmarks, while simpler models based on age and MM stage were used to provide a lower bound on expected performance. The 427 models submitted by 73 teams were compared against these benchmarks and against one another, with the best-performing ones significantly outperforming existing models and identifying novel gene candidates for follow-up studies.</p></sec><sec id="Sec5"><title>SMC-Het: ICGC-TCGA Tumor Heterogeneity Challenge</title><p id="Par10">Subclonal reconstruction is the quantification and genotyping of each individual cell population within a tumor. SMC-Het was a global effort to improve methods in this field, including evaluation of the use of somatic variants to identify the different subclones in the sample, assign mutations to these different subpopulations, and reconstruct the evolutionary tree of these subpopulations. To accomplish this, the organizers of this DREAM Challenge created simulated tumors with known tumor evolutionary histories, accepted Docker containers from participants, and scored the methods on new simulated tumors. The methods were able to be rescored as improvements were made to the tumor heterogeneity simulator itself [<xref ref-type="bibr" rid="CR9">9</xref>].</p><p id="Par11">Participants were provided custom Google Cloud VM images running Galaxy and Planemo to allow them to develop analysis pipelines. Contestants were given examples of the input data, consisting of somatic variant VCF and copy number alteration files, along with the result files. These files were small enough so that they could be packaged on the VM image along with the development software. A copy of the evaluation and scoring code was also packaged as a Galaxy tool. This allowed users to quickly cycle between developing tools and evaluating their results on a set of training files. Once contestants were ready to submit, a submission system was built directly into the VM, accessible via a command-line utility or a website running on the VM. This utility would package the participants Galaxy tools and workflow, as well as extract Docker container images from the VM, and copy them all to Synapse Challenge Platform, before creating a submission entry in the evaluation queue. By the challenge&#x02019;s close, the organizers received 76 entries from 31 teams.</p></sec><sec id="Sec6"><title>SMC-RNA: ICGC-TCGA RNA-Seq Challenge</title><p id="Par12">The transcribed genome serves a multitude of functions within a cell including carrying the information to encode proteins and serving as regulatory components. Coding and noncoding RNA have been demonstrated to play an important role in cancer. Dysregulation of RNA expression and formation of chimeric fusion proteins are both common features in tumor cells. Next-generation sequencing can both quantify RNA abundance and define its structure, allowing simultaneous identification and quantitation of chimeric transcript and protein products not present in normal cells, which can be used as diagnostic markers (e.g., TMPRSS2-ERG in prostate cancer) or drug targets (e.g., BCR-ABL in CML). The SMC-RNA DREAM Challenge was an effort to improve standardization, reproducibility, and accuracy of RNA-Seq methods. Participants were provided Illumina-based RNA sequencing from simulated tumor samples and evaluated on their ability to quantify isoform abundance and to detect chimeric fusion transcripts.</p><p id="Par13">The SMC-RNA Challenge provided participants the flexibility to choose their development environment through either the ISB Cancer Genomics Cloud or Seven Bridges Cancer Genomics Cloud. For participants who used ISB-CGC, the challenge provided access to training data on a Google storage bucket as well as custom Google VM images to use for their development environment. On SBG-CGC, training data was made accessible on a public project that users could clone and use in conjunction with the Seven Bridges Software Development Kit. Training data, which consisted of Illumina-based sequence FASTQ files, was synthetically generated in the same way as testing data. In order to standardize the submissions and evaluation of the methods, participants were required to define a CWL workflow for their tool and package their runtime environment using a Docker container. ISB-CGC participants were responsible for writing their own tool definition and workflow in CWL. The submission process consisted of pushing their Docker container to a public repository and submitting a merged CWL workflow (which references the Docker image) to Synapse. On SBG-CGC, participants were able to utilize the Rabix tool and workflow editors to both describe the tool and string together multiple tools into a workflow. For submission, participants shared a successfully completed task. The evaluation framework consisted of two steps: running submitted methods on test data using ISB-CGC and scoring their performance. The organizers received 76 submissions from 14 teams for fusion detection and 65 from 8 teams for isoform quantification.</p></sec><sec id="Sec7"><title>Proteogenomic Challenge</title><p id="Par14">The NCI-CPTAC DREAM Proteogenomics Challenge (Proteogenomics Challenge) aimed to use the community to develop computational tools to predict the proteome and phospho-proteome from genomics and transcriptomics as a means to understand the association between genome, transcriptome, and proteome in tumors. Measuring the proteome is very challenging, but recent rapid technology developments in mass spectrometry are enabling increasing deep and accurate proteomics analysis. The characterization and analyses of alterations in the proteome, such as phosphorylation, provide additional insight into the functionality of proteins and their deregulation in cancer. Collectively, (phospho) proteomic has the promise to shed light into the complexities of cancer and may improve development of both biomarkers and therapeutics. This challenge asked participants to find new methods for imputing missing values in proteomic data, predict protein abundances, and identify phosphorylation events from genomic data.</p><p id="Par15">This Proteogenomics Challenge used public and novel proteogenomic data to answer fundamental questions about how different levels of biological signal relate to one another. The challenge was built using a collection of tumor/normal pairs, with matched genomic, transcriptomic, and proteomic characterization for breast and ovarian cancer, large part of which had not yet been released to the public. Data was provided by the CPTAC (National Cancer Institute&#x02019;s Clinical Proteomic Tumor Analysis Consortium). Since the novel data could not be directly shared with the challenge participants, teams were required to submit fully trained and Dockerized models that could be applied to this data. The challenge attracted methods from 68 teams with 449 submissions over the three sub-challenges.</p></sec></sec><sec id="Sec8"><title>Lessons learned</title><sec id="Sec9"><title>Increased demands on participant to construct reproducible models</title><p id="Par16">In traditional challenge formats, participants download test data sets, run their method, and upload the outputs of their models to challenge organizers. While simple and convenient to participants, this format does not take advantage of the considerable strengths associated with M2D that includes the ability (i) to easily disseminate models to the public, (ii) to perform post hoc experiments and new analyses after the closure of the challenge, (iii) to evaluate performance in newly obtained data sets, and (iv) to develop and experiment with ensemble models. Naturally, there is a trade-off with the additional complexity and overhead required to host&#x02014;and participate in&#x02014;a M2D challenge compared to a <italic>traditional</italic> data challenge. However, while there is an increased <italic>upfront</italic> burden on participants which may negatively impact participation, this is offset by the greater flexibility and rigor that M2D bring to challenges. However, as familiarity with virtualization and workflow technologies continues to grow&#x02014;and as the technology itself matures&#x02014;we expect that these burdens on participants will substantially decrease.</p></sec><sec id="Sec10"><title>Importance of designing challenges in conjunction with data contributors</title><p id="Par17">Every benchmarking challenge relies on input datasets, and obtaining unpublished validation data requires close collaboration with researchers generating the data. There may be a number of concerns around access and security of that data. Among these is the desire of data contributors to have the first opportunity to publish key scientific results from their data. This can at times conflict with the need to keep datasets private to ensure an unbiased benchmarking challenge. Additionally, challenge validation data may be composed of multiple cohorts each originating from a separate data contributor, as was the case in the Multiple Myeloma Challenge. In such cases, these data contributors may view each other as competitors, and additional care must be taken to ensure such validation data is protected. To ensure the trust of data contributors, we developed guidelines regarding permissible summary statistics or sample characteristics participants could return and audited these accordingly. To further protect validation data in both the Digital Mammography and Multiple Myeloma challenges, we applied a strict size limit to output logs. To drive method development, participants need easy access to training data with clear information about the &#x0201c;truth.&#x0201d; In many cases, the most viable method is to develop synthetic models to generate training data. For example, in the case of the SMC-RNA Challenge, several rounds were scored using synthetic FASTQ files that could be provided to participants with minimal concerns around data privacy.</p></sec><sec id="Sec11"><title>Develop robust strategies for generating training data</title><p id="Par18">The selection of training and debugging data is a complex issue, and each challenge has had to adopt customized approaches depending on data availability. For some challenge data, there were no privacy issues and training data&#x02014;a subset of the full data set&#x02014;could be shared directly with participants, as was done for the Proteomics Challenge. Others challenges have used simulated data to bypass these issues&#x02014;as in the SMC-RNA Challenge. While simulated datasets may not completely recapitulate the underlying biology, they can provide a baseline on known and expected qualities of the data and can assist in developing robust computational pipelines. For the DM Challenge, none of the primary challenge data could be disseminated to participants. To help with model training, challenge participants could submit Dockerized containers that were permitted to train models using a subset of the imaging data. Limited feedback was returned to participants from method logging, but this required careful scrutiny by challenge organizers to ensure no sensitive data was leaked through the returned log files. Many teams in the DM Challenge utilized public datasets for training seed models and then used the private challenge data for further optimization.</p></sec><sec id="Sec12"><title>Monitoring, rapid correction, and feedback to participants</title><p id="Par19">A public-facing challenge is a complex interaction that involves providing documentation to users, accepting work products, and making sure outputs are compatible and that novel methods from external parties will function correctly within a pre-set evaluation system. Each of these steps can contain novel software-development, algorithmic, or scientific work. Consequently, challenge procedures need to be put in place that will mitigate common failures that include (1) carefully documenting the input data format and requirements for the model output format, (2) providing a small, representative data set which participants can download and test with their code prior to submission, (3) providing a mechanism for rapid assessment and feedback of execution errors using a reduced size dataset, and (4) performing upfront validation prior to initiating computational expensive and long-running jobs. When running computational models in the cloud, we are asking participants to give up the close, interactive exploration of data they might normally pursue when tinkering with novel algorithmic approaches and to troubleshoot potential defects in their code. In the event that an algorithm fails to execute, providing log files back to the participants may assist in diagnosing and fixing errors. <italic>However, this has the potential to leak data or sensitive information and must be tightly controlled</italic>. Consequently, if log files must be returned to participants, we recommend using simulated or &#x0201c;open&#x0201d; data for testing and troubleshooting models.</p></sec><sec id="Sec13"><title>Estimating and managing computational resources</title><p id="Par20">For many challenges, computational methods can have non-trivial run times and resource requirements (see Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). For example in the SMC-RNA Challenge, methods can average 4&#x02009;h per tumor. When doing the final computational runs, every method submitted needs to be run against every testing set. This can quickly lead to thousands of computational jobs that cost several thousand dollars, all of which is now run at the cost of the challenge organizers. In a number of different challenges, runtime caps had to be put into place to eliminate methods that took multiple days to complete. In the case of the SMC-Het Challenge, methods were limited to a budget of $7/tumor. A high memory machine cost $0.60 an hour, which equated to ~&#x02009;12&#x02009;h of compute time for memory-intensive algorithms. In some challenges, preemptable machines were used for evaluation, because of their lower costs. But these types of VMs work better for short running methods, that can complete before the cloud provider preempt the system. Efforts such as the Digital Mammography challenge, in which both model evaluation and <italic>training</italic> are performed in the cloud, require significantly increased compute resources. In this case, we limited compute budgets to 2&#x000a0;weeks per team per round for model training, with four rounds in the challenge. The high-end GPU servers cost several dollars per hour to rent from cloud providers. Not knowing in advance how many participants would join, we faced the risk of running out of computational resources. From this perspective, it is far less risky to ask participants to provide their own computation but, of course, this is only feasible when data contributors agree to let participants download training data. In short, when organizing a challenge, care must be taken to only commit to run the training phase when it is truly necessary for business reasons, such as sensitivity of training data.
<fig id="Fig3"><label>Fig. 3</label><caption><p><bold>a</bold>) Distribution of model run times across M2D Challenges. <bold>b</bold>) Comparison between CPU and disk usage among the M2D Challenges. CPU time is in the total wall time for running a single entry against all test samples used for benchmarking. Disk usage is the size of the testing set in GB. The diagonal line represents the point at which the cost of download egress fees and the cost of compute are equivalent. Below the line a M2D approach is theoretically cheaper</p></caption><graphic xlink:href="13059_2019_1794_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec14"><title>Increased flexibility to evolve and adapt a challenge over time</title><p id="Par21">During the active phase of the challenge, and even <italic>post</italic> analysis, there is a great deal of additional thought and analysis that goes into the evaluation data and the evaluation criteria. In some cases, there are evaluations that need to be made to the dataset, based on characteristics found during the challenge. Fixing these systems during the running of the challenge is inevitable, but every disruption disincentivizes participants from continuing work on the challenge and may limit the moral authority of the challenge to drive community-evolution. In previous challenges, if there was an issue with the testing data, it was impossible to adjust it and send back to users for new analysis. But with portable code, it becomes possible to modify the testing set, rerun methods, and evaluate. The SMC-Het Challenge faced the problem that there were no well-accepted standards for the scoring of complex phylogenetic relationships in cancer. This created a need for development of new methods for model simulation and scoring [<xref ref-type="bibr" rid="CR10">10</xref>], and these greatly increase the risk of unexpected errors, edge-cases or performance degradations. Because the participants submitted reproducible code, their methods could be reevaluated using newly generated models and evaluation methods.</p></sec><sec id="Sec15"><title>Model distribution and re-use</title><p id="Par22">Docker containers have a very modular format for distribution, and there exist several different repositories that allow for users to download the software image with a single command. However, this is only one component of distribution; there is also a need for systems that document how to invoke the tool, with descriptions of command-line formatting, tunable parameters and expected outputs. If these descriptions are machine parseable, they can be deployed with workflow engines that manage large collections of tasks. In the case of SMC-Het, the chain of commands was documented using the standards from the Galaxy Project [<xref ref-type="bibr" rid="CR11">11</xref>]. For the SMC-RNA Challenge, these descriptions were made using the Common Workflow Language (CWL) [doi:10.6084/m9.figshare.3115156.v2]. These systems allow for automated deployment, and are used as part of the evaluation framework deployed by challenge organizers. Because of this, two of the winning methods from the SMC-RNA Fusion calling challenge have been integrated into the NCI&#x02019;s Genomic Data Commons [<xref ref-type="bibr" rid="CR12">12</xref>] (GDC) standard analysis pipeline, and are now being applied to a number of datasets including TARGET, CPTAC, MMRF and TCGA.</p></sec></sec><sec id="Sec16"><title>Future of data challenges and cloud-centric analysis</title><p id="Par23">The purpose and scope of data challenges are quickly evolving in response to a rapidly maturing compute ecosystem, the growing popularity of challenges to solve complex problems, and the use of challenges to demonstrate and advertise technical competencies. Most importantly, challenges provide a robust and unbiased mechanism for assessing the best approach to solving quantitative problems. This is increasingly important in a world where algorithms are playing critical roles in biomedical decision making. The ability to objectively track the performance of algorithms over time - across a wide array of data cohorts - can play an important role in establishing confidence that algorithms are achieving their purported goals. Below, we outline some of the innovative and exciting directions for future data challenges, and biomedical analysis more broadly.</p><sec id="Sec17"><title>Bridging the translation gap</title><p id="Par24">One key bar algorithm developers need to pass to induce their tool or algorithm to be broadly adopted is <italic>believability</italic>: does the algorithm achieve its purported claims. In this regard, a bottleneck in most of biomedicine is not the lack of algorithms, but instead the lack of <italic>validated and verified</italic> algorithms. This lack of validation is a major contributor to the failure of tools to move beyond the research setting into a context that can more directly impact human health (i.e., the translational gap). Data challenges solve this problem by developing benchmarks and objective standards for tool evaluation. Challenges reveal the strengths and weaknesses of competing approaches to solving domain-specific problems, and in doing so, can accelerate the selection and adoption for tools to use in the lab and the clinic. Utilizing the M2D approach, the ability to capture methods and replay them in a controlled environment provides the opportunity to close the gap to direct patient care.</p></sec><sec id="Sec18"><title>Distributed benchmarking ecosystem</title><p id="Par25">Some of the most highly impactful biomedical data is not readily shareable due to concerns around privacy, personal health information, or intellectual property risks. Well-known examples of such data include clinical trial data, electronic healthcare records (EHR), and genetic data. The inability to access these critical datasets further contributes to the translational gap. We can imagine, and are developing toward, a frictionless benchmarking ecosystem whereby algorithms are regularly distributed to private clouds and protected data repositories for evaluation on hidden data. Such a system would enable real-time assessment of an algorithm&#x02019;s performance, and allow this performance to be tracked over time as new data becomes available. Moreover, by distributing an algorithm over many such repositories, differences in performance as a result of collection biases or population differences could be assessed, and be used to determine an algorithm&#x02019;s generalizability. Indeed, DREAM has already begun piloting such approaches with the recently launched EHR DREAM Challenge [<xref ref-type="bibr" rid="CR13">13</xref>], which will allow participants to develop and assess predictive clinical algorithms across multiple healthcare systems&#x02019; data repositories. We intend to use this Challenge to demonstrate the feasibility and value of a secure and distributed benchmarking system.</p></sec><sec id="Sec19"><title>Enabling a cloud-centric future for biomedical research</title><p id="Par26">As the rapid expansion of data generation continues, research projects will be increasingly reliant on distributed cloud-based systems for data processing and analysis. Solutions that involve a single lab distributing a package of tools and documentation for running on a single dataset or running a low throughput web server will not scale. Without standards for packaging and documenting how to invoke tools, the frictional cost of transferring software slows down the movement of methods into new cloud resources. Analytical methods need to be packaged using modern cloud-based solutions so that new methods can be quickly moved to new data and deployed by new groups. M2D encapsulates this shifting paradigm, where algorithms are brought to data in a systematic and scalable way. As this paradigm becomes more widely implemented&#x02014;not only for data challenges but as the predominant architecture for biomedical and genomic data hosting and <italic>data commons</italic>&#x02014;we envision a future in which the barriers between algorithms and data are substantially reduced, thereby accelerating biomedical insights and applications.</p></sec><sec id="Sec20"><title>Conclusion</title><p id="Par27">As the role of algorithms and software tools within the biomedical sciences grows, there is a concomitant need to rigorously evaluate and benchmark their performance. By utilizing cloud-based infrastructure and virtualization software, this is achievable like never before. The data challenges described herein are proof-of-concepts successfully demonstrating how large, complex, and sensitive biomedical data can be used to address scientific questions and benchmark methods. These challenges have also presented an alternative paradigm with respect to data access, algorithm reproducibility, community participation, and objective evaluation. As cloud platforms expand their services at ever cheaper costs, and as biomedical institutions improve federated and integrated capabilities across sites, data challenges and algorithm benchmarking are likely to become important fixtures in the biomedical landscape.</p></sec></sec><sec sec-type="supplementary-material"><title>Additional file</title><sec id="Sec21"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="13059_2019_1794_MOESM1_ESM.docx"><label>Additional file 1:</label><caption><p>Review history. (DOCX 18 kb)</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Review history</title><p>The review history is available as Additional&#x000a0;file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>KE and JG conceived of the study and authored the manuscript. AB and AC contributed to the analysis and generation of figures. BH, JE, JMC, and TY provided technical expertise in enabling model-to-data-based challenges. MM, TS, JMS, JS-R, PCB, and GS led the various challenges described in this paper and provided insights about their methods. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This work was supported by the NIH/NCI to UCLA under award number P30CA016042. This includes funding from UC Santa Cruz from NCI ITCR (grant no R01CA180778). Sage Bionetworks was funded by NIH grant 5U24CA209923.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>In the case of SMC-Het, the chain of commands was documented using the standards from the Galaxy Project [<xref ref-type="bibr" rid="CR11">11</xref>]. For the SMC-RNA Challenge, these descriptions were made using the Common Workflow Language (CWL) [doi:10.6084/m9.figshare.3115156.v2]. The workflows for the SMC-RNA Challenge can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/smc-rna-challenge/">https://github.com/smc-rna-challenge/</ext-link> [<xref ref-type="bibr" rid="CR14">14</xref>].</p></notes><notes><title>Ethics approval and consent to participate</title><p id="Par28">NA.</p></notes><notes><title>Consent for publication</title><p id="Par29">NA.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par30">The authors declare that they have no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norel</surname><given-names>R</given-names></name><name><surname>Rice</surname><given-names>JJ</given-names></name><name><surname>Stolovitzky</surname><given-names>G</given-names></name></person-group><article-title>The self-assessment trap: can we all be better than average?</article-title><source>Mol Syst Biol</source><year>2011</year><volume>7</volume><fpage>537</fpage><pub-id pub-id-type="doi">10.1038/msb.2011.70</pub-id><pub-id pub-id-type="pmid">21988833</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bender</surname><given-names>E</given-names></name></person-group><article-title>Challenges: crowdsourced solutions</article-title><source>Nature.</source><year>2016</year><volume>533</volume><fpage>S62</fpage><lpage>S64</lpage><pub-id pub-id-type="doi">10.1038/533S62a</pub-id><pub-id pub-id-type="pmid">27167394</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saez-Rodriguez</surname><given-names>J</given-names></name><name><surname>Costello</surname><given-names>JC</given-names></name><name><surname>Friend</surname><given-names>SH</given-names></name><name><surname>Kellen</surname><given-names>MR</given-names></name><name><surname>Mangravite</surname><given-names>L</given-names></name><name><surname>Meyer</surname><given-names>P</given-names></name><etal/></person-group><article-title>Crowdsourcing biomedical research: leveraging communities as innovation engines</article-title><source>Nat Rev Genet</source><year>2016</year><volume>17</volume><fpage>470</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1038/nrg.2016.69</pub-id><pub-id pub-id-type="pmid">27418159</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guinney</surname><given-names>J</given-names></name><name><surname>Saez-Rodriguez</surname><given-names>J</given-names></name></person-group><article-title>Alternative models for sharing confidential biomedical data</article-title><source>Nat Biotechnol</source><year>2018</year><volume>36</volume><fpage>391</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1038/nbt.4128</pub-id><pub-id pub-id-type="pmid">29734317</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trister</surname><given-names>Andrew D.</given-names></name><name><surname>Buist</surname><given-names>Diana S. M.</given-names></name><name><surname>Lee</surname><given-names>Christoph I.</given-names></name></person-group><article-title>Will Machine Learning Tip the Balance in Breast Cancer Screening?</article-title><source>JAMA Oncology</source><year>2017</year><volume>3</volume><issue>11</issue><fpage>1463</fpage><pub-id pub-id-type="doi">10.1001/jamaoncol.2017.0473</pub-id><pub-id pub-id-type="pmid">28472204</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>BL</given-names></name><name><surname>Arao</surname><given-names>RF</given-names></name><name><surname>Miglioretti</surname><given-names>DL</given-names></name><name><surname>Henderson</surname><given-names>LM</given-names></name><name><surname>Buist</surname><given-names>DSM</given-names></name><name><surname>Onega</surname><given-names>T</given-names></name><etal/></person-group><article-title>National performance benchmarks for modern diagnostic digital mammography: update from the Breast Cancer Surveillance Consortium</article-title><source>Radiology.</source><year>2017</year><volume>283</volume><fpage>59</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1148/radiol.2017161519</pub-id><pub-id pub-id-type="pmid">28244803</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaughnessy</surname><given-names>JD</given-names><suffix>Jr</suffix></name><name><surname>Zhan</surname><given-names>F</given-names></name><name><surname>Burington</surname><given-names>BE</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Colla</surname><given-names>S</given-names></name><name><surname>Hanamura</surname><given-names>I</given-names></name><etal/></person-group><article-title>A validated gene expression model of high-risk multiple myeloma is defined by deregulated expression of genes mapping to chromosome 1</article-title><source>Blood.</source><year>2007</year><volume>109</volume><fpage>2276</fpage><lpage>2284</lpage><pub-id pub-id-type="doi">10.1182/blood-2006-07-038430</pub-id><pub-id pub-id-type="pmid">17105813</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuiper</surname><given-names>R</given-names></name><name><surname>Broyl</surname><given-names>A</given-names></name><name><surname>de Knegt</surname><given-names>Y</given-names></name><name><surname>van Vliet</surname><given-names>MH</given-names></name><name><surname>van Beers</surname><given-names>EH</given-names></name><name><surname>van der Holt</surname><given-names>B</given-names></name><etal/></person-group><article-title>A gene expression signature for high-risk multiple myeloma</article-title><source>Leukemia.</source><year>2012</year><volume>26</volume><fpage>2406</fpage><lpage>2413</lpage><pub-id pub-id-type="doi">10.1038/leu.2012.127</pub-id><pub-id pub-id-type="pmid">22722715</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Salcedo A, Tarabichi M, Espiritu SMG, Deshwar AG, David M, Wilson NM, et al. Creating standards for evaluating tumour subclonal reconstruction. bioRxiv. 2018:310425 [cited 2018 Jul 23]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2018/07/15/310425">https://www.biorxiv.org/content/early/2018/07/15/310425</ext-link>.</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Boutros PC, Salcedo A, Tarabichi M, Espiritu SMG, Deshwar AG, David M, et al. Creating standards for evaluating tumour subclonal reconstruction. bioRxiv. 2018:310425 [cited 2018 Jul 12]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2018/04/30/310425">https://www.biorxiv.org/content/early/2018/04/30/310425</ext-link>.</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afgan</surname><given-names>E</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name><name><surname>van den Beek</surname><given-names>M</given-names></name><name><surname>Blankenberg</surname><given-names>D</given-names></name><name><surname>Bouvier</surname><given-names>D</given-names></name><name><surname>&#x0010c;ech</surname><given-names>M</given-names></name><etal/></person-group><article-title>The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update</article-title><source>Nucleic Acids Res</source><year>2016</year><volume>44</volume><fpage>W3</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1093/nar/gkw343</pub-id><pub-id pub-id-type="pmid">27137889</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>MA</given-names></name><name><surname>Ferretti</surname><given-names>V</given-names></name><name><surname>Grossman</surname><given-names>RL</given-names></name><name><surname>Staudt</surname><given-names>LM</given-names></name></person-group><article-title>The NCI Genomic Data Commons as an engine for precision medicine</article-title><source>Blood.</source><year>2017</year><volume>130</volume><fpage>453</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1182/blood-2017-03-735654</pub-id><pub-id pub-id-type="pmid">28600341</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">EHR DREAM Challenge [Internet]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn18405991/wiki/589657">https://www.synapse.org/#!Synapse:syn18405991/wiki/589657</ext-link></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Ellrott K, Buchanan A, Creason A, Mason M, Schaffter T, Hoff B, Eddy J, Chilton JM, Yu T, Stuart JM, et al, Reproducible biomedical benchmarking in the cloud: lessons from crowd-sourced data challenges. Source code. Github <ext-link ext-link-type="uri" xlink:href="https://github.com/smc-rna-challenge/">https://github.com/smc-rna-challenge/</ext-link>.</mixed-citation></ref></ref-list></back></article>