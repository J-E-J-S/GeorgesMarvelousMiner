<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Molecules</journal-id><journal-id journal-id-type="iso-abbrev">Molecules</journal-id><journal-id journal-id-type="publisher-id">molecules</journal-id><journal-title-group><journal-title>Molecules : A Journal of Synthetic Chemistry and Natural Product Chemistry</journal-title></journal-title-group><issn pub-type="epub">1420-3049</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6149962</article-id><article-id pub-id-type="doi">10.3390/molecules22122116</article-id><article-id pub-id-type="publisher-id">molecules-22-02116</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An Interface for Biomedical Big Data Processing on the Tianhe-2 Supercomputer</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Xi</given-names></name><xref ref-type="aff" rid="af1-molecules-22-02116">1</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Chengkun</given-names></name><xref ref-type="aff" rid="af1-molecules-22-02116">1</xref><xref rid="c1-molecules-22-02116" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Kai</given-names></name><xref ref-type="aff" rid="af1-molecules-22-02116">1</xref></contrib><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Lin</given-names></name><xref ref-type="aff" rid="af2-molecules-22-02116">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Yong</given-names></name><xref ref-type="aff" rid="af2-molecules-22-02116">2</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Shengkang</given-names></name><xref ref-type="aff" rid="af2-molecules-22-02116">2</xref></contrib><contrib contrib-type="author"><name><surname>Guo</surname><given-names>Guixin</given-names></name><xref ref-type="aff" rid="af3-molecules-22-02116">3</xref></contrib><contrib contrib-type="author"><name><surname>Du</surname><given-names>YunFei</given-names></name><xref ref-type="aff" rid="af4-molecules-22-02116">4</xref><xref rid="c1-molecules-22-02116" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-molecules-22-02116"><label>1</label>College of Computer, National University of Defense Technology, Changsha 410073, China; <email>yangxi1016@nudt.edu.cn</email> (X.Y.); <email>kailu@nudt.edu.cn</email> (K.L.)</aff><aff id="af2-molecules-22-02116"><label>2</label>Beijing Genomics Institute (BGI) Shenzhen, Shenzhen 518083, China; <email>fangl@genomics.cn</email> (L.F.); <email>zhangyong2@genomics.cn</email> (Y.Z.); <email>lishengkang@genomics.cn</email> (S.L.)</aff><aff id="af3-molecules-22-02116"><label>3</label>National Supercomputing Center of Guangzhou, Guangzhou 510006, China; <email>guixin.guo@nscc-gz.cn</email></aff><aff id="af4-molecules-22-02116"><label>4</label>School of Data and Computer Science, Sun Yat-Sen University, Guangzhou 510000, China; <email>duyunfei@mail.sysu.edu.cn</email></aff><author-notes><corresp id="c1-molecules-22-02116"><label>*</label>Correspondence: <email>chengkun_wu@nudt.edu.cn</email> (C.W.); <email>duyunfei@mail.sysu.edu.cn</email> (Y.D.); Tel.: +86-135-4964-2841 (C.W.); Fax: +86-0731-8451-1369 (C.W.)</corresp></author-notes><pub-date pub-type="epub"><day>01</day><month>12</month><year>2017</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2017</year></pub-date><volume>22</volume><issue>12</issue><elocation-id>2116</elocation-id><history><date date-type="received"><day>25</day><month>10</month><year>2017</year></date><date date-type="accepted"><day>29</day><month>11</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2017 by the authors.</copyright-statement><copyright-year>2017</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Big data, cloud computing, and high-performance computing (HPC) are at the verge of convergence. Cloud computing is already playing an active part in big data processing with the help of big data frameworks like Hadoop and Spark. The recent upsurge of high-performance computing in China provides extra possibilities and capacity to address the challenges associated with big data. In this paper, we propose Orion&#x02014;a big data interface on the Tianhe-2 supercomputer&#x02014;to enable big data applications to run on Tianhe-2 via a single command or a shell script. Orion supports multiple users, and each user can launch multiple tasks. It minimizes the effort needed to initiate big data applications on the Tianhe-2 supercomputer via automated configuration. Orion follows the &#x0201c;allocate-when-needed&#x0201d; paradigm, and it avoids the idle occupation of computational resources. We tested the utility and performance of Orion using a big genomic dataset and achieved a satisfactory performance on Tianhe-2 with very few modifications to existing applications that were implemented in Hadoop/Spark. In summary, Orion provides a practical and economical interface for big data processing on Tianhe-2. </p></abstract><kwd-group><kwd>big data</kwd><kwd>Tianhe-2</kwd><kwd>Hadoop</kwd><kwd>Spark</kwd><kwd>genomics big data</kwd></kwd-group></article-meta></front><body><sec id="sec1-molecules-22-02116"><title>1. Introduction</title><p>In recent years, the so-called &#x0201c;big data&#x0201d; trend has become increasingly prominent due to the proliferation of large scientific instruments such as particle colliders and astronomical telescopes, as well as high-throughput analysis devices like Next-Generation sequencers and pervasive sensors. Many scientific discoveries and insights are now driven by massive datasets, and this is considered as the fourth paradigm of science besides experiments, theories, and computational methods [<xref rid="B1-molecules-22-02116" ref-type="bibr">1</xref>]. The volume, velocity, and variety of data generation comprise the major challenges of big data. Biomedical studies emerge as a perfect example. For instance, the European Bioinformatics Institute (EBI, one of the largest biological data holders) stores over 20 petabytes of data, which recently has tended to double every year [<xref rid="B2-molecules-22-02116" ref-type="bibr">2</xref>]. The heterogeneity of the data sources, including genomics, proteomics, metabolomics, microarray data, literature, etc., makes it even more complex. </p><p>Timely analysis of those huge datasets is usually of great importance for scientific discoveries, economic decisions, and other activities. To address this challenge, advanced computing technologies are essential. </p><p>The first solution is to utilize open source frameworks that were designed to run on commodity-level servers, clusters, computing grids, or cloud environments. </p><p>Hadoop, following the MapReduce model, is one of the most representative programming frameworks for big data processing [<xref rid="B3-molecules-22-02116" ref-type="bibr">3</xref>]. It is an open source Java-based programming framework that supports the processing and storage of extremely large data sets in a distributed computing or cloud computing environment. The framework makes it possible to run big data applications on systems constructed by connecting many commodity nodes. It specifically employs a number of mechanisms to enable applications to continue operating even in case of node failures, including the Hadoop Distributed File System (HDFS) [<xref rid="B4-molecules-22-02116" ref-type="bibr">4</xref>]. It was designed to run on distributed systems, which are commonly built as a cluster of commodity hardware, which is relatively cheap compared to supercomputers built using high-end elements. Three major components of Hadoop include a distributed filesystem, a processing module, and a job manager. The HDFS manages distributed storage, and the processing core adopts the MapReduce paradigm. The MapReduce model &#x0201c;maps&#x0201d; the input data into subgroups (by grouping and sorting) and applies multiple maps functions in parallel on each subgroup of data. The &#x0201c;reduce&#x0201d; operation produces an output file for each reduce task. The MapReduce module hides all details of parallel processing, data transfers and recovery from errors. One disadvantage is that many applications exhibit computational and data access patterns that cannot be expressed as a MapReduce model. Moreover, Hadoop writes all intermediate data to disk, which involves a lot of slow IO operations.</p><p>The Hadoop framework is widely used in molecular research. For instance, since traditional methods are incapable of dealing with the large-scale data of high-throughput sequencing data and the DNA MSA (Multiple sequence alignment) issue, Zou, Q. [<xref rid="B5-molecules-22-02116" ref-type="bibr">5</xref>,<xref rid="B6-molecules-22-02116" ref-type="bibr">6</xref>] addressed the problem by using the Hadoop platform to exploit parallelism. Traditionally, chemists need to upload small molecule files and collect the virtual screening results manually, which is a laborious and tedious procedure. To alleviate this situation, Zhao, J. [<xref rid="B7-molecules-22-02116" ref-type="bibr">7</xref>] developed a Hadoop-based application for large-scale data storage, which uses HDFS to store and manage small molecule files and docking results. By using the Hadoop programming framework for parallel docking, result files was preprocessed and the automation of the virtual screening molecular docking was achieved. Zhang, Y. [<xref rid="B8-molecules-22-02116" ref-type="bibr">8</xref>] proposed a novel storage solution based on Hadoop, introducing HBase as a distributed database to maintain the properties of massive molecules and docking results. In addition, HDFS was also utilized as a molecule source files storage system. Niu, J. Ellingson [<xref rid="B9-molecules-22-02116" ref-type="bibr">9</xref>] created AutoDockCloud, which implemented the MapReduce paradigm for distributed computing by using the Hadoop framework, demonstrating a speed-up of 450&#x000d7; on a commercial cloud service.</p><p>To further improve the data processing efficiency and the capability to address more complex tasks, the in-memory big data framework Spark has become increasingly popular [<xref rid="B10-molecules-22-02116" ref-type="bibr">10</xref>,<xref rid="B11-molecules-22-02116" ref-type="bibr">11</xref>]. Spark addresses several aspects of performance issues to accelerate big data processing. Firstly, it maximizes the utilization of the main memory of each node to avoid repeated IO requests for intermediate results; secondly, it employs a directed acyclic graph (DAG) mechanism for task scheduling, which is more efficient and flexible. Spark has attracted a great deal of attention, as it is more efficient than Hadoop (up to 100&#x000d7; speedup) and it provides support for Java, Python, Scala and R. It also provides a stack of libraries including SQL, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for stream processing.</p><p>Numerous big data applications were developed using Hadoop and Spark [<xref rid="B12-molecules-22-02116" ref-type="bibr">12</xref>,<xref rid="B13-molecules-22-02116" ref-type="bibr">13</xref>]. To run Hadoop and Spark applications, you need a Hadoop/Spark cluster, whose scale can be from a few nodes to thousands. Big companies like Baidu, Tencent, and Alibaba usually maintains several dedicated large-scale clusters for such purposes. However, not everyone can have their own cluster. Cloud computing could be an alternative choice. Mainstream cloud computing services like Alibaba Cloud and Tencent Cloud do provide interfaces that can help you set up temporary Hadoop/Spark clusters for big data processing. Nevertheless, to use such services, you need to have a data &#x0201c;highway&#x0201d; to transfer your big data onto the cloud.</p><p>High-performance computing (HPC) like Tianhe-2 [<xref rid="B14-molecules-22-02116" ref-type="bibr">14</xref>,<xref rid="B15-molecules-22-02116" ref-type="bibr">15</xref>,<xref rid="B16-molecules-22-02116" ref-type="bibr">16</xref>] represents high-end computing infrastructures that have traditionally been used to solve compute-intensive scientific and engineering problems. Although the computing power, stability, scalability, and storage capacity of HPC systems are enormous, they are usually expensive to build and deploy. In addition, profound experience in parallel programming (e.g., OpenMP, MPI) and knowledge about the system architecture are required in order to harness the power of HPC systems. Consequently, only a small number of engineers can utilize HPC to handle their big data problems. </p><p>Although the software stack on Tianhe-2 is designed and optimized for compute-intensive tasks [<xref rid="B17-molecules-22-02116" ref-type="bibr">17</xref>,<xref rid="B18-molecules-22-02116" ref-type="bibr">18</xref>,<xref rid="B19-molecules-22-02116" ref-type="bibr">19</xref>], its high-performance architecture does provide the possibility and capability of efficient big data processing. The motivation of this paper is to implement an easy-to-use interface on Tianhe-2 for Hadoop/Spark applications in a dynamic and scalable way. This is a non-trivial task for the following two reasons: firstly, the resource requirements and configuration settings vary for different users and different applications; for convenience, users might want to have full control over the configuration process. However, it is infeasible (or very costly) for the supercomputing center to perform a new configuration from scratch every time, and if given too much control, users might unintentionally (or maliciously) breach the security protocols; secondly, users are charged based on the amount of resources they occupy, so they would like to release spare resources whenever possible. That would require changes in the configuration, and it could be a tedious process to perform different configurations over and over again. Frequently, users might want to run existing big data applications built for cloud computing on the Tianhe-2 supercomputer without major modifications to the program. In addition, users often want to develop new applications in their familiar framework (Hadoop/Spark) that can run readily on Tianhe-2. </p><p>In this paper, we first emphasize the need to enhance big data support on modern supercomputers, and we analyze the architecture of Tianhe-2 for a practicable and efficient solution. We propose Orion, which takes requests for big data processing, allocates the desired amount of resources, and automatically sets up and configures a recyclable Hadoop/Spark cluster using the HPC system software stack. The benefits are two-fold: firstly, developers avoid re-inventing the wheel by utilizing existing Hadoop/Spark applications; secondly, the Tianhe-2 software stack requires no major changes. We tested Orion using genomics big data processing as a case study, which comes from one of the major users of Tianhe-2: the Beijing Genomics Institute (BGI). The main contributions of this paper can be listed as follows:<list list-type="order"><list-item><p>We analyze the need for and possibility of convenient big data support on modern supercomputers</p></list-item><list-item><p>We propose Orion, which is an easy-to-use interface for users who wants to run Hadoop/Spark applications on the Tianhe-2 supercomputer</p></list-item><list-item><p>We test Orion on Tianhe-2 against an in-house Hadoop cluster using the genomics big data example and testify the utility of Orion</p></list-item></list></p></sec><sec id="sec2-molecules-22-02116"><title>2. Results and Discussion</title><sec id="sec2dot1-molecules-22-02116"><title>2.1. Automated Deployment of Big Data Applications</title><p>We developed the Orion big data interface to facilitate big data applications on Tianhe-2. In order to more efficiently solve multiple tasks, one user can launch multiple Orion instances to maintain several big data analytics &#x0201c;clusters&#x0201d; (composed of Tianhe-2 compute nodes) at the same time, so that each Orion instance can deal with a dedicated task. The maximum number of Orion instances is limited by the number of available compute nodes in the Tianhe-2 system.</p><p>The parallel processing mode is illustrated in <xref ref-type="fig" rid="molecules-22-02116-f001">Figure 1</xref>.</p></sec><sec id="sec2dot2-molecules-22-02116"><title>2.2. Testing Orion: Genomics Big Data as a Case Study</title><p>In recent years, a deluge of genomic sequence data has been observed due to the constantly decreasing cost of sequencing. We are now entering the precision medicine era, in which medical discoveries will largely depend on the processing and analysis of large genomic data sets. In this paper, we collaborated with one major player in the field of genomics&#x02014;the Beijing Genomics Institute (BGI)-Shenzhen&#x02014;to test the utility of Orion in genomics big data processing. </p><p>The key question is, can they readily run their existing applications on Tianhe-2? To answer this question, we tested the four major components that are essential for a genomics analysis pipeline, SOAPGaea, which is their primary analysis software (using the Hadoop framework). Those four components include FASTQ filtering, read alignment, duplication removal, and quality control. Four components were tested on a sample dataset of 300 GB (in BAM format). We carried out the test using the following three different scenarios. Details of the three scenarios are listed in <xref rid="molecules-22-02116-t001" ref-type="table">Table 1</xref>. Note that in all three scenarios, some RAM must be reserved for system needs. </p><p>The performance of each component under three different scenarios is listed in <xref rid="molecules-22-02116-t002" ref-type="table">Table 2</xref>. Overall, we can see a major performance boost in Orion-1 and Orion-2 compared to the BGI setting (the shortest processing time is presented in bold and italic format). In Orion-1, although the number of running cores was fewer (6 cores, 10 cores in BGI), the amount of available RAM was far more than that of BGI (8 GB compared to 3 GB). In Orion-2, the number of running cores on each node was greater than that of BGI. Note that the CPUs utilized in BGI and Tianhe-2 are of different specifications, and so it is not a completely fair comparison. Without loss of generality, the performance comparison here is mainly to demonstrate that by using Orion, users can easily run their big data application on Tianhe-2 and they can gain better performance if using the same amount of compute nodes (as Tianhe-2 nodes have more powerful CPUs and more RAM). The cost of leasing Tianhe-2 nodes is quite low (about 2.4 RMB per node-hour). In addition, using Orion to run Hadoop applications is as simple as typing a command. Therefore, it is completely feasible to consider Tianhe-2 as a solution for BGI&#x02019;s genomics big data analytics. </p><p>In <xref rid="molecules-22-02116-t002" ref-type="table">Table 2</xref>, we can also observe that the performance boost in Orion-1 and Orion-2 is actually not linear. This is because different components in SOAPGaea have different computing patterns. Some are sensitive to the amount of RAM, while others are sensitive to the number of cores.</p><p>The above tests demonstrate how BGI&#x02019;s Hadoop applications can run easily on Tianhe-2 with the help of Orion. Below, we would like to demonstrate how it can be applied to BGI&#x02019;s Spark applications. In <xref rid="molecules-22-02116-t003" ref-type="table">Table 3</xref>, we tested Orion on one specific module named GaeaDuplicate Spark, which can be decomposed into three steps: read in &#x02192; compute &#x02192; write out. This module spends a lot of time in I/O. The input data is a 300 GB dataset in BAM format. We tested this module with three scenarios as well. </p><p>Note that in Orion-A and Orion-B we only allowed the cores to use up to 70 percent of the node RAM to ensure node stability. The testing results are listed in <xref rid="molecules-22-02116-t004" ref-type="table">Table 4</xref>.</p><p>As we can see, as the number of available nodes increases, the computation time is vastly reduced. The time needed for I/O constructs the barrier of further reducing the processing time. Here, the GaeaDuplicate module will be integrated into a running pipeline; that is, there would be one I/O operation in, many computation steps, and one I/O operation out. Therefore, the percentage of I/O time will be reduced, and the overall processing time can be greatly decreased if we introduce more compute nodes when launching Orion. </p><p>In all, we have demonstrated that Orion can facilitate existing big data applications without requiring the users to modify their programs, while also providing a performance boost. The performance boost can be attributed to multiple reasons. Firstly, Tianhe-2 is extremely powerful as a whole system, and each node is also more powerful compared to commodity ones; secondly, the amount of RAM available on Tianhe-2 is more abundant, which fits well for big data applications. There are aspects where the performance can be further improved, like utilizing RDMA (Remote Direct Memory Access) [<xref rid="B16-molecules-22-02116" ref-type="bibr">16</xref>]. However, that is beyond the scope of this paper, which focuses on providing an easy-to-use interface for big data users to run their applications readily on Tianhe-2. </p><p>Currently, there are many molecular big data applications available or under development, using standard frameworks like Hadoop and Spark. To take full advantage of current efforts on cloud, we aim not to re-invent the wheel on the HPC platform but to develop an interface support at the platform level. In this manner, it is possible to boost current cloud applications by fully utilizing the computational and data access performance of HPC systems. </p></sec><sec id="sec2dot3-molecules-22-02116"><title>2.3. Portability of the Interface</title><p>Besides Tianhe-2, a number of Tianhe supercomputing systems have been deployed all over China in recent years, hosted by national or local supercomputing centers in different areas of the country. For instance, Tianhe-1 and Tianhe-1A [<xref rid="B14-molecules-22-02116" ref-type="bibr">14</xref>] are hosted by the National Supercomputing Center of Changsha and Tianjin, respectively. </p><p>Different Tianhe HPC systems have different specific hardware architectures. However, the software environments are similar in terms of the job management system and the operating system, and it is relatively straight-forward to deploy Orion on different Tianhe HPC systems. Several recently delivered Tianhe HPC systems are already pre-configured with the Orion interface, all of which went through many iterations of functional and pressure tests. Consequently, users only need to modify some configuration parameters (like paths, partition names) to use Orion.</p></sec></sec><sec id="sec3-molecules-22-02116"><title>3. Materials and Methods</title><p>Orion is implemented via Shell scripts. It employs the scheduling system of Tianhe-2 to allocate required computing resources and utilizes the parallel filesystem of Tianhe-2 directly rather than HDFS.</p><p>An overview of the Orion implementation is depicted in <xref ref-type="fig" rid="molecules-22-02116-f002">Figure 2</xref>. Generally speaking, Orion involves the following four components:</p><p>(1)&#x000a0;Initialization</p><p>Users need to specify the framework type (Hadoop or Spark) and the desired number of nodes. Orion parses the user input and then automatically allocates resources by calling the Tianhe-2 scheduling system to configure a virtual Hadoop/Spark cluster using allocated compute nodes.</p><p>(2)&#x000a0;Job submission</p><p>Orion can either read an analytic job from a user job Shell script or accept a user job interactively using the command line.</p><p>(3)&#x000a0;Job execution</p><p>Orion executes the submitted job and monitors the status of the execution in real time. Once the task terminates unexpectedly, exception handling will be started.</p><p>(4)&#x000a0;Finalization</p><p>Orion collects the generated results and stores them in the globally shared filesystem. Orion will then stop all compute daemons and release allocated compute nodes.</p><sec id="sec3dot1-molecules-22-02116"><title>3.1. Features of Orion</title><p>In summary, Orion has the following features:<list list-type="bullet"><list-item><p>It supports Hadoop and Spark configuration automatically; the user only needs to specify the framework type and the number of nodes.</p></list-item><list-item><p>We use the original scheduling system of Tianhe-2 and do not use YARN for node management.</p></list-item><list-item><p>We use the H2FS filesystem of Tianhe-2 instead of HDFS.</p></list-item><list-item><p>We not only configure the framework work environment, but also enable related environments like NoSQL databases support (Redis, Mongo DB), OpenMP, and MPI.</p></list-item><list-item><p>The Tianhe-2 monitoring subsystem can track the status of the temporarily created big data analytics cluster dynamically.</p></list-item><list-item><p>The temporary data analytics cluster can be created and recycled any time the users wish, so that users will not be charged if their applications are not actually running.</p></list-item></list></p></sec><sec id="sec3dot2-molecules-22-02116"><title>3.2. Demonstration of Orion Usage (Tianhe-2 in the NSCC Guangzhou as an Example)</title><p>To use Orion and run big data applications (Hadoop/Spark) on Tianhe-2, the procedure is described in details as follows:<list list-type="order"><list-item><p>Log in to your user account on Tianhe-2 and you will be given a command line terminal. Then, you can see the following:<list list-type="alpha-lower"><list-item><p>The installation directory
<array orientation="portrait"><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1"><italic>/WORK/app/share/Orion</italic></td></tr></tbody></array>
</p></list-item><list-item><p>The initial situation in this directory is as the <xref ref-type="fig" rid="molecules-22-02116-f003">Figure 3</xref>.</p><p>Orion_start.sh is used for cluster configuration and start. Orion_clean.sh is the task run after the completion of the calculation or the release of resources upon an error, and cleans up the directory. Script directory is used to store task scripts.</p></list-item></list></p></list-item><list-item><p>Switch to the Orion installation directory and execute a command like:
<array orientation="portrait"><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1"><italic>./Orion_start.sh -t hadoop -p work -N 6 -s &#x02018;pwd&#x02019;/script/wordcount.sh</italic></td></tr></tbody></array></p><p>Here, the &#x0201c;-t&#x0201d; option can either be &#x0201c;hadoop&#x0201d; or &#x0201c;spark&#x0201d;, which specifies the big data framework you are using; the &#x0201c;-p&#x0201d; option specifies the resource pool on Tianhe-2 you want to use; the &#x0201c;-N&#x0201d; option specifies the number of compute nodes to use; the &#x0201c;-s&#x0201d; option specifies the job Shell script to be executed. The first three options are obligatory. The last one is optional. An example job script &#x0201c;wordcount.sh&#x0201d; is given in <xref ref-type="fig" rid="molecules-22-02116-f004">Figure 4</xref>.</p><p>The above command will invoke the initialization process and wait for the resources to be allocated. If the required resources cannot be allocated after a long time, Orion will prompt the user to select a different resource pool or to use less nodes.</p><p>$ {HADOOP_HOME} -points to the directory where Hadoop is installed. In general, the contents of the command can be modified. You can also specify a different output directory.</p></list-item><list-item><p>Once the task is submitted, it will be automatically queued to wait for resources. If the designated resource requirements cannot be fulfilled, Orion will give a message and you will need to switch to a partition with more idle resources or reduce the number of nodes.</p></list-item><list-item><p>If the application for resource is successful, Orion will continue to configure and launch a Hadoop/Spark cluster using the allocated nodes.</p></list-item><list-item><p>Orion executes the designated script and waits for the execution to finish. You can continue to use the configured cluster to run jobs interactively.</p></list-item><list-item><p>Use a command like the following to terminate the cluster and release occupied resources:
<array orientation="portrait"><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1"><italic>./Orion_clean.sh -j 1362049</italic></td></tr></tbody></array>
</p><p>Here the &#x0201c;-j&#x0201d; option tells Orion to clean up the corresponding job, and the job ID is given in Step 2.</p></list-item></list></p><p>Tips:</p><p>Regarding the configuration file changes:</p><p>If you want to modify the configuration file, you can switch to the installation directory under the magpie/conf directory, and modify the corresponding file. The changes will take effect when you create a new virtual cluster. You can also manually log in to the master node of the existing cluster and restart YARN and JobHistoryServer as described above.</p></sec></sec><sec id="sec4-molecules-22-02116"><title>4. Conclusions </title><p>In this paper, we proposed Orion, the big data analytics platform on Tianhe-2, which enables users to submit their current big data applications to Tianhe-2 without many changes and to gain a performance boost at the same time. As Orion manages all details about communicating with the supercomputer, users with little knowledge about Tianhe-2 can also develop new Hadoop/Spark applications that can run on Tianhe-2 via Orion. We tested Orion using genomics big data as a case study. Through Orion, BGI&#x02019;s current applications can run on Tianhe-2 readily with a major performance boost, which can be attributed to the hardware advantages and the sufficient amount of computational resources of Tianhe-2. In summary, Orion provides an easy-to-use and scalable interface to users who want to carry out big data analytics on the Tianhe-2 supercomputer, and this interface is also available for other Tianhe HPC facilities. The use is straightforward, and the performance is satisfactory. </p><p>For future work, we will introduce more optimization to further improve the big data performance on Tianhe-2. </p></sec></body><back><ack><title>Acknowledgments</title><p>This work is funded by the National Key Research and Development Program of China 2016YFB0200401 and the National Science Foundation of China 31501073. </p></ack><fn-group><fn><p><bold>Sample Availability:</bold> Samples of the compounds are not available from the authors.</p></fn></fn-group><notes><title>Author Contributions</title><p>Chengkun Wu and Yunfei Du conceived the idea of the project and designed the methods; Xi Yang and Kai Lu implemented the code of Orion; Lin Fang, Shengkang Li and Yong Zhang prepared the SOAPGaea program and the dataset for testing and performed the test of SOAPGaea on the BGI&#x02019;s cluster; Xi Yang, Chengkun Wu and Guixin Guo performed the tests of Orion on Tianhe-2. Yunfei Du drafted the paper, Xi Yang and Chengkun Wu revised the manuscript. </p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-molecules-22-02116"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolle</surname><given-names>K.M.</given-names></name><name><surname>Tansley</surname><given-names>D.S.W.</given-names></name><name><surname>Hey</surname><given-names>A.J.G.</given-names></name></person-group><article-title>The fourth paradigm: Data-intensive scientific discovery</article-title><source>Proc. IEEE</source><year>2011</year><volume>99</volume><fpage>1334</fpage><lpage>1337</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2011.2155130</pub-id></element-citation></ref><ref id="B2-molecules-22-02116"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marx</surname><given-names>V.</given-names></name></person-group><article-title>Biology: The big challenges of big data</article-title><source>Nature</source><year>2013</year><volume>498</volume><fpage>255</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1038/498255a</pub-id><?supplied-pmid 23765498?><pub-id pub-id-type="pmid">23765498</pub-id></element-citation></ref><ref id="B3-molecules-22-02116"><label>3.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zikopoulos</surname><given-names>P.</given-names></name><name><surname>Eaton</surname><given-names>C.</given-names></name></person-group><source>Understanding Big Data: Analytics for Enterprise Class Hadoop and Streaming Data</source><publisher-name>McGraw-Hill Osborne Media</publisher-name><publisher-loc>Columbus, OH, USA</publisher-loc><year>1989</year></element-citation></ref><ref id="B4-molecules-22-02116"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shvachko</surname><given-names>K.</given-names></name><name><surname>Kuang</surname><given-names>H.</given-names></name><name><surname>Radia</surname><given-names>S.</given-names></name><name><surname>Chansler</surname><given-names>R.</given-names></name></person-group><article-title>The Hadoop distributed file system</article-title><source>Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)</source><conf-loc>Incline Village, NV, USA</conf-loc><conf-date>3&#x02013;7 May 2010</conf-date><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="B5-molecules-22-02116"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>Q.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Jiang</surname><given-names>W.</given-names></name><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>G.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name></person-group><article-title>Survey of MapReduce frame operation in bioinformatics</article-title><source>Brief. Bioinform.</source><year>2017</year><volume>15</volume><fpage>637</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1093/bib/bbs088</pub-id></element-citation></ref><ref id="B6-molecules-22-02116"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>Q.</given-names></name><name><surname>Hu</surname><given-names>Q.</given-names></name><name><surname>Guo</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>G.</given-names></name></person-group><article-title>HAlign&#x0202f;: Fast multiple similar DNA/RNA sequence alignment based on the centre star strategy</article-title><source>Bioinformatics</source><year>2017</year><volume>31</volume><fpage>2475</fpage><lpage>2481</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btv177</pub-id><?supplied-pmid 25812743?><pub-id pub-id-type="pmid">25812743</pub-id></element-citation></ref><ref id="B7-molecules-22-02116"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>R.</given-names></name></person-group><article-title>Hadoop MapReduce framework to implement molecular docking of large-scale virtual screening</article-title><source>Proceedings of the Services Computing Conference (APSCC), 2012 IEEE Asia-Pacific</source><conf-loc>Guilin, China</conf-loc><conf-date>6&#x02013;8 December 2012</conf-date><fpage>350</fpage><lpage>353</lpage></element-citation></ref><ref id="B8-molecules-22-02116"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>R.</given-names></name><name><surname>Chen</surname><given-names>Q.</given-names></name><name><surname>Gao</surname><given-names>X.</given-names></name><name><surname>Hu</surname><given-names>R.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>G.</given-names></name></person-group><article-title>A hadoop-based massive molecular data storage solution for virtual screening</article-title><source>Proceedings of the 2012 Seventh China Grid Annual Conference</source><conf-loc>Beijing, China</conf-loc><conf-date>20&#x02013;23 September 2012</conf-date></element-citation></ref><ref id="B9-molecules-22-02116"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Niu</surname><given-names>J.</given-names></name><name><surname>Bai</surname><given-names>S.</given-names></name><name><surname>Khosravi</surname><given-names>E.</given-names></name><name><surname>Park</surname><given-names>S.</given-names></name></person-group><article-title>A Hadoop approach to advanced sampling algorithms in molecular dynamics simulation on cloud computing</article-title><source>Proceedings of the 2013 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source><conf-loc>Shanghai, China</conf-loc><conf-date>18&#x02013;21 December 2013</conf-date><fpage>452</fpage><lpage>455</lpage></element-citation></ref><ref id="B10-molecules-22-02116"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Efficient distributed data clustering on spark</article-title><source>Proceedings of the 2015 IEEE International Conference on Cluster Computing (CLUSTER)</source><conf-loc>Chicago, IL, USA</conf-loc><conf-date>8&#x02013;11 September 2015</conf-date><fpage>504</fpage><lpage>505</lpage></element-citation></ref><ref id="B11-molecules-22-02116"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X.</given-names></name><name><surname>Liao</surname><given-names>X.</given-names></name><name><surname>Lu</surname><given-names>K.</given-names></name><name><surname>Hu</surname><given-names>Q.</given-names></name><name><surname>Song</surname><given-names>J.</given-names></name><name><surname>Su</surname><given-names>J.</given-names></name></person-group><article-title>The TianHe-1A supercomputer: Its hardware and software</article-title><source>Comput. Sci.</source><year>2011</year><volume>26</volume><fpage>344</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1007/s02011-011-1137-8</pub-id></element-citation></ref><ref id="B12-molecules-22-02116"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fascio</surname><given-names>V.</given-names></name><name><surname>W&#x000fc;thrich</surname><given-names>R.</given-names></name><name><surname>Bleuler</surname><given-names>H.</given-names></name></person-group><article-title>Spark assisted chemical engraving in the light of electrochemistry</article-title><source>Electrochim. Acta</source><year>2004</year><volume>49</volume><fpage>3997</fpage><lpage>4003</lpage><pub-id pub-id-type="doi">10.1016/j.electacta.2003.12.062</pub-id></element-citation></ref><ref id="B13-molecules-22-02116"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zaharia</surname><given-names>M.</given-names></name><name><surname>Chowdhury</surname><given-names>M.</given-names></name><name><surname>Franklin</surname><given-names>M.J.</given-names></name><name><surname>Shenker</surname><given-names>S.</given-names></name><name><surname>Stoica</surname><given-names>I.</given-names></name></person-group><article-title>Spark: Cluster computing with working sets</article-title><source>Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>22&#x02013;25 June 2010</conf-date><fpage>10</fpage></element-citation></ref><ref id="B14-molecules-22-02116"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>X.</given-names></name><name><surname>Xiao</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>C.</given-names></name><name><surname>Lu</surname><given-names>Y.</given-names></name></person-group><article-title>MilkyWay-2 supercomputer: System and application</article-title><source>Front. Comput. Sci.</source><year>2014</year><volume>8</volume><fpage>345</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1007/s11704-014-3501-3</pub-id></element-citation></ref><ref id="B15-molecules-22-02116"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cheptsov</surname><given-names>A.</given-names></name></person-group><article-title>HPC in big data age: An evaluation report for java-based data-intensive applications implemented with Hadoop and OpenMPI</article-title><source>Proceedings of the 21st European MPI Users&#x02019; Group Meeting</source><conf-loc>Kyoto, Japan</conf-loc><conf-date>9&#x02013;12 September 2014</conf-date><fpage>175</fpage></element-citation></ref><ref id="B16-molecules-22-02116"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Islam</surname><given-names>N.S.</given-names></name><name><surname>Shankar</surname><given-names>D.</given-names></name><name><surname>Lu</surname><given-names>X.</given-names></name><name><surname>Panda</surname><given-names>D.K.</given-names></name></person-group><article-title>Accelerating I/O performance of big data analytics on HPC clusters through RDMA-based key-value store</article-title><source>Proceedings of the 2015 44th International Conference on Parallel Processing (ICPP)</source><conf-loc>Beijing, China</conf-loc><conf-date>1&#x02013;4 September 2015</conf-date><fpage>280</fpage><lpage>289</lpage></element-citation></ref><ref id="B17-molecules-22-02116"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>Y.</given-names></name><name><surname>Liao</surname><given-names>X.</given-names></name><name><surname>Peng</surname><given-names>S.</given-names></name><name><surname>Lu</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>B.</given-names></name><name><surname>Wu</surname><given-names>C.</given-names></name></person-group><article-title>Large-scale neo-heterogeneous programming and optimization of SNP detection on Tianhe-2</article-title><source>Proceedings of the International Conference on High Performance Computing</source><conf-loc>Bangalore, India</conf-loc><conf-date>15&#x02013;20 November 2015</conf-date><fpage>74</fpage><lpage>86</lpage></element-citation></ref><ref id="B18-molecules-22-02116"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>C.</given-names></name><name><surname>Schwartz</surname><given-names>J.M.</given-names></name><name><surname>Brabant</surname><given-names>G.</given-names></name><name><surname>Peng</surname><given-names>S.</given-names></name><name><surname>Nenadic</surname><given-names>G.</given-names></name></person-group><article-title>Constructing a molecular interaction network for thyroid cancer via large-scale text mining of gene and pathway events</article-title><source>BMC Syst. Biol.</source><year>2015</year><volume>9</volume><issue>Suppl 6</issue><elocation-id>S5</elocation-id><pub-id pub-id-type="doi">10.1186/1752-0509-9-S6-S1</pub-id><?supplied-pmid 26678566?><pub-id pub-id-type="pmid">26678566</pub-id></element-citation></ref><ref id="B19-molecules-22-02116"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Q.</given-names></name><name><surname>Peng</surname><given-names>S.</given-names></name><name><surname>Lu</surname><given-names>Y.</given-names></name><name><surname>Zhu</surname><given-names>W.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>mD3DOCKxb: A deep parallel optimized software for molecular docking with Intel Xeon Phi coprocessors</article-title><source>Proceedings of the IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</source><conf-loc>Shenzhen, China</conf-loc><conf-date>4&#x02013;7 May 2015</conf-date><fpage>725</fpage><lpage>728</lpage></element-citation></ref></ref-list></back><floats-group><fig id="molecules-22-02116-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The parallel processing mode for big data analytics.</p></caption><graphic xlink:href="molecules-22-02116-g001"/></fig><fig id="molecules-22-02116-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>An overview of the Orion architecture for big data analytics.</p></caption><graphic xlink:href="molecules-22-02116-g002"/></fig><fig id="molecules-22-02116-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The initial interface of the installation directory.</p></caption><graphic xlink:href="molecules-22-02116-g003"/></fig><fig id="molecules-22-02116-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>An example job script.</p></caption><graphic xlink:href="molecules-22-02116-g004"/></fig><table-wrap id="molecules-22-02116-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-22-02116-t001_Table 1</object-id><label>Table 1</label><caption><p>Hardware and software settings of the Hadoop test. BGI: Beijing Genomics Institute; HDFS: Hadoop Distributed File System.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Scenario Name</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hardware Setting</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Analysis Setting</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BGI</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A Hadoop cluster with 18 nodes. Each node is equipped with 24 cores, 32 GB RAM. The storage uses HDFS, and the total capacity is 12 TB.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 master + 17 slaves.Each core is allocated 3 GB RAM, a maximum of 10 cores were used on each node.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Orion-1</td><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">A Hadoop cluster initiated and maintained by Orion on Tianhe-2 with 18 nodes. Each node is equipped with 24 cores, 64 GB RAM. Use the Tianhe-2 parallel filesystem directly.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 master + 17 slaves.Each core is allocated 8 GB of RAM, a maximum of 6 cores were used on each node.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Orion-2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 master + 17 slaves.Each core is allocated 3 GB of RAM, a maximum of 16 cores were used on each node.</td></tr></tbody></table></table-wrap><table-wrap id="molecules-22-02116-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-22-02116-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance comparison of four components of SOAPGaea.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SOAPGaea Components</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">BGI</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Orion-1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Orion-2</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">FASTQ Filtering</td><td align="center" valign="middle" rowspan="1" colspan="1">24 m 43 s</td><td align="center" valign="middle" rowspan="1" colspan="1">12 m 50 s</td><td align="center" valign="middle" rowspan="1" colspan="1">10 m 23 s</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Read Alignment</td><td align="center" valign="middle" rowspan="1" colspan="1">1 h 35 m 56 s</td><td align="center" valign="middle" rowspan="1" colspan="1">48 m 48 s</td><td align="center" valign="middle" rowspan="1" colspan="1">49 m 49 s</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Duplication Removal</td><td align="center" valign="middle" rowspan="1" colspan="1">28 m 21s</td><td align="center" valign="middle" rowspan="1" colspan="1">15 m 38 s</td><td align="center" valign="middle" rowspan="1" colspan="1">9 m 43 s</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Quality Control</td><td align="center" valign="middle" rowspan="1" colspan="1">1 h 30 m 2 s</td><td align="center" valign="middle" rowspan="1" colspan="1">1 h 39 m</td><td align="center" valign="middle" rowspan="1" colspan="1">46 m 38 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total processing time</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 h 59 m 2 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2 h 56 m 16 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 h 56 m 33 s</td></tr></tbody></table></table-wrap><table-wrap id="molecules-22-02116-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-22-02116-t003_Table 3</object-id><label>Table 3</label><caption><p>Hardware and software settings of the Spark test.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Scenario Name</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hardware Setting</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Analysis Setting</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">BGI</td><td align="left" valign="middle" rowspan="1" colspan="1">A Spark cluster with 18 nodes. Each node is equipped with 24 cores, 32 GB RAM. The storage uses HDFS and the total capacity is 12 TB.</td><td align="left" valign="middle" rowspan="1" colspan="1">1 master + 17 slaves. Each core is allocated 3 GB RAM, a maximum of 10 cores were used on each node.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Orion-A</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A Spark cluster initiated and maintained by Orion on Tianhe-2 with 100 nodes. Each node is equipped with 24 cores, 64 GB RAM. Uses the Tianhe-2 parallel filesystem directly.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 master + 99 slaves. 24 cores, a maximum total of 44 GB RAM cores were used on each node.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Orion-B</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A Spark cluster initiated and maintained by Orion on Tianhe-2 with 250 nodes. Each node is equipped with 24 cores, 64 GB RAM. Uses the Tianhe-2 parallel filesystem directly.</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 master + 249 slaves. 24 cores, a maximum total of 44 GB RAM cores were used on each node.</td></tr></tbody></table></table-wrap><table-wrap id="molecules-22-02116-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-22-02116-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance decomposition for GaeaDuplicate Spark in different settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GaeaDuplicate_Spark</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Read In</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Compute</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Write Out</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">BGI</td><td align="center" valign="middle" rowspan="1" colspan="1">17 m</td><td align="center" valign="middle" rowspan="1" colspan="1">1.1 h</td><td align="center" valign="middle" rowspan="1" colspan="1">40 m</td><td align="center" valign="middle" rowspan="1" colspan="1">2 h</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Orion-A</td><td align="center" valign="middle" rowspan="1" colspan="1">25 m</td><td align="center" valign="middle" rowspan="1" colspan="1">14 m</td><td align="center" valign="middle" rowspan="1" colspan="1">40 m</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3 h</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Orion-B</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1 h</td></tr></tbody></table></table-wrap></floats-group></article>