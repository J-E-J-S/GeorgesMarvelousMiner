<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Genes (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Genes (Basel)</journal-id><journal-id journal-id-type="publisher-id">genes</journal-id><journal-title-group><journal-title>Genes</journal-title></journal-title-group><issn pub-type="epub">2073-4425</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6769581</article-id><article-id pub-id-type="doi">10.3390/genes10090652</article-id><article-id pub-id-type="publisher-id">genes-10-00652</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Sparse Convolutional Denoising Autoencoders for Genotype Imputation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0483-303X</contrib-id><name><surname>Chen</surname><given-names>Junjie</given-names></name></contrib><contrib contrib-type="author"><name><surname>Shi</surname><given-names>Xinghua</given-names></name><xref rid="c1-genes-10-00652" ref-type="corresp">*</xref><xref ref-type="author-notes" rid="fn1-genes-10-00652">&#x02020;</xref></contrib></contrib-group><aff id="af1-genes-10-00652">Department of Bioinformatics and Genomics, College of Computing and Informatics, University of North Carolina at Charlotte, 9201 University City Blvd, Charlotte, NC 28223, USA</aff><author-notes><corresp id="c1-genes-10-00652"><label>*</label>Correspondence: <email>x.shi@uncc.edu</email></corresp><fn id="fn1-genes-10-00652"><label>&#x02020;</label><p>Current Address: Department of Computer &#x00026; Information Sciences, Temple University, 925 N. 12th Street, Philadelphia, PA 19122, USA.</p></fn></author-notes><pub-date pub-type="epub"><day>28</day><month>8</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2019</year></pub-date><volume>10</volume><issue>9</issue><elocation-id>652</elocation-id><history><date date-type="received"><day>26</day><month>6</month><year>2019</year></date><date date-type="accepted"><day>24</day><month>8</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 by the authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Genotype imputation, where missing genotypes can be computationally imputed, is an essential tool in genomic analysis ranging from genome wide associations to phenotype prediction. Traditional genotype imputation methods are typically based on haplotype-clustering algorithms, hidden Markov models (HMMs), and statistical inference. Deep learning-based methods have been recently reported to suitably address the missing data problems in various fields. To explore the performance of deep learning for genotype imputation, in this study, we propose a deep model called a sparse convolutional denoising autoencoder (SCDA) to impute missing genotypes. We constructed the SCDA model using a convolutional layer that can extract various correlation or linkage patterns in the genotype data and applying a sparse weight matrix resulted from the <italic>L</italic><sub>1</sub> regularization to handle high dimensional data. We comprehensively evaluated the performance of the SCDA model in different scenarios for genotype imputation on the yeast and human genotype data, respectively. Our results showed that SCDA has strong robustness and significantly outperforms popular reference-free imputation methods. This study thus points to another novel application of deep learning models for missing data imputation in genomic studies.</p></abstract><kwd-group><kwd>genotype imputation</kwd><kwd>convolutional neural network</kwd><kwd>autoencoder</kwd><kwd>sparse model</kwd><kwd>deep learning</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-genes-10-00652"><title>1. Introduction</title><p>Genotype imputation is a critical step in many types of genomic analysis, ranging from genome wide association studies (GWAS) to phenotype prediction. Missing values in genotype data are common and could result from many reasons such as low call rates, deviations from Hardy&#x02013;Weinberg equilibrium, and the abundance of rare or low frequent variants in samples [<xref rid="B1-genes-10-00652" ref-type="bibr">1</xref>,<xref rid="B2-genes-10-00652" ref-type="bibr">2</xref>]. Genotype imputation works by computationally inferring missing values in a genotype profile, typically using the correlation or linkage information from untyped variants and nearby markers that are genotyped [<xref rid="B3-genes-10-00652" ref-type="bibr">3</xref>,<xref rid="B4-genes-10-00652" ref-type="bibr">4</xref>]. Genetic variants whose genotypes are imputed are mostly single nucleotide polymorphisms (SNPs), although other types of genetic variants such as small insertions and deletions and large structural variants can be imputed as long as they are in linkage disequilibrium (LD) with typed variants [<xref rid="B5-genes-10-00652" ref-type="bibr">5</xref>,<xref rid="B6-genes-10-00652" ref-type="bibr">6</xref>].</p><p>Existing imputation methods can be generally classified into two categories based on whether a reference panel is required, as summarized in <xref rid="genes-10-00652-t001" ref-type="table">Table 1</xref>. Methods in the first category require a reference panel, which contains haplotype information from many samples usually from the same or similar population background. These reference-based imputation methods usually apply a haplotype-clustering algorithm [<xref rid="B7-genes-10-00652" ref-type="bibr">7</xref>] and a hidden Markov model (HMM) [<xref rid="B8-genes-10-00652" ref-type="bibr">8</xref>] to impute missing SNPs using known haplotypes as a reference [<xref rid="B9-genes-10-00652" ref-type="bibr">9</xref>], such as those from the HapMap Project [<xref rid="B10-genes-10-00652" ref-type="bibr">10</xref>] or the 1000 Genomes Project [<xref rid="B5-genes-10-00652" ref-type="bibr">5</xref>,<xref rid="B6-genes-10-00652" ref-type="bibr">6</xref>] for human genomes. Particularly, fastPHASE [<xref rid="B7-genes-10-00652" ref-type="bibr">7</xref>] uses a localized haplotype-clustering model, in which reference haplotypes are grouped into clusters at each SNP for imputing missing genotypes at that locus. IMPUTE [<xref rid="B8-genes-10-00652" ref-type="bibr">8</xref>] is based on an extension of the HMM algorithm originally developed as part of the importance sampling scheme for simulating coalescent trees, modelling LD, and estimating recombination rates. IMPUTE2 [<xref rid="B11-genes-10-00652" ref-type="bibr">11</xref>] is a flexible and scalable extension of the original IMPUTE algorithm [<xref rid="B8-genes-10-00652" ref-type="bibr">8</xref>]. It uses an adaptive haplotype selection approach to impute untyped SNPs in a linear time (<italic>O</italic>(<italic>N</italic>)) compared with the quadratic time (<italic>O</italic>(<italic>N</italic><sup>2</sup>)) carried out in IMPUTE [<xref rid="B8-genes-10-00652" ref-type="bibr">8</xref>]. MACH [<xref rid="B12-genes-10-00652" ref-type="bibr">12</xref>] uses an HMM model that works by successively updating the phase of each individual&#x02019;s genotypes conditional on the current haplotype estimates of all other individuals. Minimac4 [<xref rid="B13-genes-10-00652" ref-type="bibr">13</xref>] is the latest version in a series of genotype imputation software-preceded by Minimac3 (2015) [<xref rid="B14-genes-10-00652" ref-type="bibr">14</xref>], Minimac2 (2014) [<xref rid="B15-genes-10-00652" ref-type="bibr">15</xref>], minimac (2012) [<xref rid="B16-genes-10-00652" ref-type="bibr">16</xref>], and MaCH (2010) [<xref rid="B12-genes-10-00652" ref-type="bibr">12</xref>]. Minimac4 [<xref rid="B13-genes-10-00652" ref-type="bibr">13</xref>] is a lower memory demanding and a more computationally efficient implementation of the original MACH algorithms with comparable imputation quality. BEAGLE [<xref rid="B17-genes-10-00652" ref-type="bibr">17</xref>,<xref rid="B18-genes-10-00652" ref-type="bibr">18</xref>] is another common imputation tool based on a graphical model of a set of haplotypes. It works iteratively by fitting the model to the current set of estimated haplotypes and then resampling new estimated haplotypes for each individual using a fitted model. The probabilities of missing genotypes are calculated from the model that is fitted at the final iteration. Additionally, SNP tagging-based approaches such as PLINK [<xref rid="B19-genes-10-00652" ref-type="bibr">19</xref>], SNPMSTAT [<xref rid="B20-genes-10-00652" ref-type="bibr">20</xref>], and TUNA [<xref rid="B21-genes-10-00652" ref-type="bibr">21</xref>] carry out genotype imputation using LD information on tag SNPs [<xref rid="B22-genes-10-00652" ref-type="bibr">22</xref>,<xref rid="B23-genes-10-00652" ref-type="bibr">23</xref>]. Specifically, for each SNP to be imputed, the reference haplotypes are used to search for a small set of tag SNPs in the flanking region that forms a local haplotype background in high LD with the target SNP to be imputed. All of the methods aforementioned rely on a well-defined haplotype-reference panel and would not work for those species without a high-resolution reference panel.</p><p>Instead, a suite of methods for missing data imputation that do not require a reference panel are developed based on statistical inference of unobserved data, which can be utilized for genotype imputation. These methods use information such as the row average approach [<xref rid="B24-genes-10-00652" ref-type="bibr">24</xref>], distance or similarity based methods like the k-nearest neighbors (KNN) algorithm [<xref rid="B25-genes-10-00652" ref-type="bibr">25</xref>], singular value decomposition (SVD) [<xref rid="B26-genes-10-00652" ref-type="bibr">26</xref>], and prediction models based on classification or regression [<xref rid="B26-genes-10-00652" ref-type="bibr">26</xref>,<xref rid="B29-genes-10-00652" ref-type="bibr">29</xref>]. Specifically, as one of the simplest methods, the row average approach [<xref rid="B24-genes-10-00652" ref-type="bibr">24</xref>] imputes missing values using an average of all the non-missing values or the most frequent value in the same column with missing values. Distance or similarity based methods usually exploit data that are similar or close to the missing data to make inference of missing values. The commonly used similarity measurements include Pearson correlation, Euclidean distance, variance minimization, and cosine distance. For example, the KNN imputation algorithm [<xref rid="B25-genes-10-00652" ref-type="bibr">25</xref>] finds the <italic>k</italic>-nearest neighbors that have values in the missing value positions, and uses a weighted average of the values from <italic>k</italic>-nearest neighbors to estimate missing values. Evidence has shown that log-transformation can sufficiently reduce the effect of outliers on similarity measurements [<xref rid="B30-genes-10-00652" ref-type="bibr">30</xref>] and can thus be performed before imputing missing data using distance or similarity based methods. An SVD-based imputation method [<xref rid="B26-genes-10-00652" ref-type="bibr">26</xref>] obtains the <italic>k</italic> most significant eigenvectors to impute the missing values using a low-rank SVD approximation estimated by an expectation maximization (EM) algorithm. Prediction-model-based imputation methods [<xref rid="B26-genes-10-00652" ref-type="bibr">26</xref>,<xref rid="B29-genes-10-00652" ref-type="bibr">29</xref>] create a predictive model to estimate values that will substitute missing data in a machine learning fashion. In this case, the input data are divided into two sets: one set with no missing values for training and the other set with missing values for testing. The predictive models can be trained using popular regression or classification models such as logistic regression [<xref rid="B27-genes-10-00652" ref-type="bibr">27</xref>,<xref rid="B31-genes-10-00652" ref-type="bibr">31</xref>] and random forest [<xref rid="B28-genes-10-00652" ref-type="bibr">28</xref>,<xref rid="B32-genes-10-00652" ref-type="bibr">32</xref>].</p><p>Recently, deep learning [<xref rid="B33-genes-10-00652" ref-type="bibr">33</xref>] has shown great potential in numerous applications including image processing [<xref rid="B34-genes-10-00652" ref-type="bibr">34</xref>,<xref rid="B35-genes-10-00652" ref-type="bibr">35</xref>], voice recognition [<xref rid="B36-genes-10-00652" ref-type="bibr">36</xref>,<xref rid="B37-genes-10-00652" ref-type="bibr">37</xref>], natural language processing [<xref rid="B38-genes-10-00652" ref-type="bibr">38</xref>,<xref rid="B39-genes-10-00652" ref-type="bibr">39</xref>], and particularly bioinformatics [<xref rid="B40-genes-10-00652" ref-type="bibr">40</xref>]. Applications of deep learning in Bioinformatics include variant calling [<xref rid="B41-genes-10-00652" ref-type="bibr">41</xref>], functional annotation [<xref rid="B42-genes-10-00652" ref-type="bibr">42</xref>,<xref rid="B43-genes-10-00652" ref-type="bibr">43</xref>], protein structure recognition and prediction [<xref rid="B44-genes-10-00652" ref-type="bibr">44</xref>,<xref rid="B45-genes-10-00652" ref-type="bibr">45</xref>,<xref rid="B46-genes-10-00652" ref-type="bibr">46</xref>,<xref rid="B47-genes-10-00652" ref-type="bibr">47</xref>,<xref rid="B48-genes-10-00652" ref-type="bibr">48</xref>,<xref rid="B49-genes-10-00652" ref-type="bibr">49</xref>,<xref rid="B50-genes-10-00652" ref-type="bibr">50</xref>], gene expression inference [<xref rid="B51-genes-10-00652" ref-type="bibr">51</xref>], molecular function recognition [<xref rid="B52-genes-10-00652" ref-type="bibr">52</xref>], prediction of methylation states [<xref rid="B53-genes-10-00652" ref-type="bibr">53</xref>], and high-throughput chromosome conformation capture (HiC) data enhancement [<xref rid="B54-genes-10-00652" ref-type="bibr">54</xref>]. Deep learning-based methods, especially autoencoders, have been reported to work well to address the missing data problems in various fields [<xref rid="B55-genes-10-00652" ref-type="bibr">55</xref>,<xref rid="B56-genes-10-00652" ref-type="bibr">56</xref>]. For instance, autoencoders have been applied to impute missing data in electronic health records [<xref rid="B55-genes-10-00652" ref-type="bibr">55</xref>] and human immunodeficiency virus (HIV) data [<xref rid="B57-genes-10-00652" ref-type="bibr">57</xref>]. Another example is a multiple-layer perceptron-based denoising autoencoder method for imputing DNA methylation data with comparable performance with the SVD approach [<xref rid="B58-genes-10-00652" ref-type="bibr">58</xref>]. However, the commonly used autoencoder architectures are based on fully connected layers in which each neuron is connected to every neuron in a previous layer, and each connection has its own weight. Learning on this fully connected architecture is very expensive in terms of computational time and space. Furthermore, fully connected autoencoders ignore the underlying structure or relationship in genomic data such as the LD structure in genotype profiles. Therefore, the limitations of the current practice of deep learning methodology in genomic analysis leave a vast room for model improvement, especially for those models based on the autoencoder framework. One particular technique to encode data relatedness or correlation is to use convolutional networks. A convolutional network can learn the underlying structure and relationship in genotype data by leveraging a convolutional kernel that is capable of learning various local patterns in a filter window. To handle high dimensional genomics data where the feature size is significantly larger than the sample size, we can introduce model sparsity by incorporating regularization on the weight matrix of a deep learning model.</p><p>Hence, in this study, we propose a novel deep learning model, called sparse convolutional denoising autoencoder (SCDA), for genotype imputation that does not need to compare with a reference panel. Specifically, the SCDA model utilizes convolutional layers to take account of local data correlations in the general autoencoder framework, and incorporates model sparsity to handle high dimensional genomic data using an <italic>L</italic><sub>1</sub> regularization on each convolutional kernel. To comprehensively evaluate the performance of the SCDA model for genotype imputation, we simulated different missing data scenarios on a yeast genotype dataset and a human leukocyte antigen (HLA) genotype dataset, respectively. Our results showed that SCDA achieved higher imputation accuracy than three existing reference panel-free imputation methods, and demonstrated the strong robustness of this SCDA model on different missing data scenarios. SCDA&#x02019;s nice performance for genotype imputation benefits from using convolutional layers to extract linkage patterns in the genotype data and a sparse weight matrix resulted from the <italic>L</italic><sub>1</sub> regularization to handle high dimensional data. This study thus demonstrates a new application of deep learning models to impute missing data in genomic studies.</p></sec><sec id="sec2-genes-10-00652"><title>2. Materials and Methods</title><sec id="sec2dot1-genes-10-00652"><title>2.1. Dataset</title><p>To evaluate our proposed SCDA method, we used a comprehensively assayed yeast genotype dataset [<xref rid="B59-genes-10-00652" ref-type="bibr">59</xref>] and a human genotype dataset from the most extensive catalog of human genetic variation from the 1000 Genomes Project [<xref rid="B5-genes-10-00652" ref-type="bibr">5</xref>,<xref rid="B6-genes-10-00652" ref-type="bibr">6</xref>]. The yeast data represents a scenario that the genetic background is simple and the genotypes are highly correlated. The human data represent a more realistic and complex scenario where the genotypes are sampled from diverse human populations. The yeast genotype dataset contains the genotype profile of 28,820 unique genetic variants, which was obtained by sequencing 4390 observations from a cross between two strains of yeast: a widely used laboratory strain (BY) and an isolate from a vineyard (RM). The original data fields in the yeast genotype profile were encoded as -1 for BY and 1 for RM. As the loss function in our SCDA model requires non-negative data fields, we replaced all -1 values with 2 in data preprocessing. For the human genotype data, we chose to impute genotypes of the human leukocyte antigen (HLA) [<xref rid="B60-genes-10-00652" ref-type="bibr">60</xref>]. As the HLA region contains a gene complex encoding the major histocompatibility complex (MHC) proteins in humans, HLA represents a region where genotypes are diverse, heterogeneous, and complicated. We extracted the HLA genotype data consisting of 27,209 unique genetic variants in 2504 individuals across five super populations worldwide sequenced from the 1000 Genome Project [<xref rid="B5-genes-10-00652" ref-type="bibr">5</xref>], including Americans (AMR), Southern Asians (SAS), East Asians (EAS), Europeans (EUR), and Africans (AFR). The EAS super population consists of 617 individuals from six populations, including Chinese Dai in Xishuangbanna (CDX), Han Chinese in Beijing (CHB), Chinese in Denver (CHD), Southern Han Chinese (CHS), Japanese in Tokyo (JPT), and Kinh in Ho Chi Minh City (KHV). The human genotypes are encoded as 1 for the original genotype of &#x02018;0|0&#x02019;, 2 for &#x02018;0|1&#x02019; or &#x02018;1|0&#x02019;, and 3 for &#x02018;1|1&#x02019;, respectively, converting from the original variant call format (VCF) file of the 1000 Genomes Project. We can see that both the yeast and human genotype datasets are highly dimensional with the feature dimension (<italic>p</italic> = 28,820 for yeast genotypes, <italic>p</italic> = 27,209 for HLA genotypes) significantly larger than the sample size (<italic>n</italic> = 4390 for yeast, <italic>n</italic> =2504 for HLA). With this type of highly dimensional dataset, sparse models that use regularization to impose sparsity work well to address the problem of the curse of dimensionality [<xref rid="B61-genes-10-00652" ref-type="bibr">61</xref>].</p><p>In order to assess the performance of our SCDA method in different missing data scenarios, we generated three sets of synthetic datasets by randomly masking 5%, 10%, and 20% of the original genotypes to zeros in the original yeast and human HLA datasets, respectively. For each of these synthetic datasets, we split the data into three separate datasets containing 65%, 15%, and 20% of the synthetic data for training, validation, and testing, respectively. <xref ref-type="fig" rid="genes-10-00652-f001">Figure 1</xref> visualizes data vectors of two samples with 5% missing values randomly selected from the yeast genotype dataset (<xref ref-type="fig" rid="genes-10-00652-f001">Figure 1</xref>a) and HLA genotype data (<xref ref-type="fig" rid="genes-10-00652-f001">Figure 1</xref>b). Different colors represent different genotypes and missing values are denoted in white color. Consecutive color blocks indicate that genotypes are highly correlated among nearby genetic markers, resulting from LD and linkage patterns. As expected, the correlations among yeast genotypes are much stronger than those among HLA genotypes. Hence, compared with the yeast data, the HLA data are more heterogenous and complicated, thus imputation is more difficult. Given the highly correlated and structured genotype data, methods like our SCDA method that take account of these local patterns will work well to impute missing genotypes.</p></sec><sec id="sec2dot2-genes-10-00652"><title>2.2. Autoencoders</title><p>Autoencoders [<xref rid="B62-genes-10-00652" ref-type="bibr">62</xref>] are unsupervised artificial neural networks that are designed to learn efficient data encoding or representation to reconstruct the original input data. As shown in <xref ref-type="fig" rid="genes-10-00652-f002">Figure 2</xref>a, an autoencoder consists of two parts: an encoder and a decoder, which can be defined as <italic>f</italic> and <italic>g</italic>, respectively. The encoder takes an input vector <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and maps it to a hidden representation <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> through a mapping function in Equation (1).
<disp-formula id="FD1-genes-10-00652"><label>(1)</label><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif">&#x003a6;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm5"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> weight matrix, <inline-formula><mml:math id="mm7"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> is a bias vector, and <inline-formula><mml:math id="mm8"><mml:mrow><mml:mi mathvariant="sans-serif">&#x003a6;</mml:mi></mml:mrow></mml:math></inline-formula> is an activation function such as a sigmoid [<xref rid="B63-genes-10-00652" ref-type="bibr">63</xref>] or rectified linear units (ReLU) [<xref rid="B64-genes-10-00652" ref-type="bibr">64</xref>]. The hidden representation, <inline-formula><mml:math id="mm9"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula>, is also called a latent representation. The decoder takes a hidden representation <inline-formula><mml:math id="mm10"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula> to map it to a reconstructed vector <italic>z</italic>
<inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> using Equation (2).
<disp-formula id="FD2-genes-10-00652"><label>(2)</label><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="sans-serif">&#x003a6;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm14"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> weight matrix; <inline-formula><mml:math id="mm16"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is a bias vector; and <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="sans-serif">&#x003a6;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is an activation function, the same as <inline-formula><mml:math id="mm18"><mml:mrow><mml:mi mathvariant="sans-serif">&#x003a6;</mml:mi></mml:mrow></mml:math></inline-formula>. The parameters <inline-formula><mml:math id="mm19"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> of an autoencoder will be optimized to minimize the average reconstruction error, as shown in Equation (3).
<disp-formula id="FD3-genes-10-00652"><label>(3)</label><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>*</mml:mo><mml:mo>=</mml:mo><mml:mi>argmin</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>argmin</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>*</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are parameters to be learned on data.</p><p>The aim of an autoencoder is to reconstruct <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> such that <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02248;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> by minimizing the loss function <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> can be defined as the widely used mean squared error for continuous data or cross-entropy for discrete data. In genotype imputation, we minimized the cross-entropy loss between the input <inline-formula><mml:math id="mm27"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and reconstructed <inline-formula><mml:math id="mm28"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> as defined in Equation (3), as genotype values are discrete.
<disp-formula id="FD4-genes-10-00652"><label>(4)</label><mml:math id="mm29"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3-genes-10-00652"><title>2.3. Denoising Autoencoders</title><p>A denoising autoencoder [<xref rid="B65-genes-10-00652" ref-type="bibr">65</xref>] is an extension of a standard autoencoder, which takes corrupted input data with missing values, and can thus be applied for data imputation. A denoising autoencoder reconstructs the output from the corrupted input data by allowing the encoder to extract the most important features and learn a robust representation of the input data, as shown in <xref ref-type="fig" rid="genes-10-00652-f002">Figure 2</xref>b. When the corrupted input includes missing data values, denoising autoencoders can solve the imputation problem by predicting the values of missing data points by reconstructing the input data [<xref rid="B65-genes-10-00652" ref-type="bibr">65</xref>]. As denoising autoencoders fit well with solving missing data problems, in this project, we utilized denoising autoencoders for genotype imputation. For example, in a synthetic dataset where the yeast data was first corrupted by randomly masking 5% of the original values to zeros, we can train a denoising autoencoder to predict values of those data points masked as zeros.</p></sec><sec id="sec2dot4-genes-10-00652"><title>2.4. Sparse Convolutional Networks</title><p>However, it is computationally expensive to use solely autoencoders for data imputation, especially when the input data are of large scale, high dimension, and with local structures. Autoencoders are primarily multiple-layer perceptron neural networks with dense layer-wise connections, which can be very expensive to learn in terms of computational time and space. Furthermore, fully connected autoencoders ignore data relationships such as LD and linkage structures in genotype data (<xref ref-type="fig" rid="genes-10-00652-f001">Figure 1</xref>). To further take advantage of the characteristics of genotype data, we leverage convolution networks to learn the underlying structures and relationships of genotype data. As genotype values are discrete, we use convolution operations in a discrete space defined below.
<disp-formula id="FD5-genes-10-00652"><label>(5)</label><mml:math id="mm32"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the output of the <inline-formula><mml:math id="mm34"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> marker in the input vector <italic>I</italic>. <inline-formula><mml:math id="mm35"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> is the convolutional filter and <inline-formula><mml:math id="mm36"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is an odd number representing the convolutional filter size. In this study, we experiment with odd filter sizes ranging from 3 to 19. The convolution operation in Equation (5) is performed for every location of the input vector <inline-formula><mml:math id="mm37"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, and thus for each genetic marker.</p><p>Every convolutional layer is composed of <italic>n</italic> convolutional filters, each with a depth of <italic>D</italic>, where <italic>D</italic> is the input depth. A convolution among an input <inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and a set of <italic>n</italic> convolutional filters <inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> produces a set of <italic>n</italic> activation maps or, equivalently, a volume of activation maps with a depth of <italic>n</italic>:<disp-formula id="FD6-genes-10-00652"><label>(6)</label><mml:math id="mm40"><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm41"><mml:mrow><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> is a non-linear activation function and <inline-formula><mml:math id="mm42"><mml:mrow><mml:mo>&#x02297;</mml:mo></mml:mrow></mml:math></inline-formula> is a convolution symbol of Equation (5). Here, <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias and <italic>m</italic> represents the <italic>m</italic>th feature map.</p><p>Overfitting is a critical problem for analyzing highly dimensional data such as genotype data. To prevent overfitting and improve model performance, we introduce the <italic>L</italic><sub>1</sub> normalization, defined in Equation (7), to all convolutional filters. The <italic>L</italic><sub>1</sub> norm regularization works by applying penalties on layer weights during the optimization process of a model. These penalties are incorporated in the loss function on which the model will optimize. The <italic>L</italic><sub>1</sub> norm will penalize or shrink small weights to zeros to improve the robustness of a model.
<disp-formula id="FD7-genes-10-00652"><label>(7)</label><mml:math id="mm44"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif">&#x003bb;</mml:mi><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm45"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the <italic>L</italic><sub>1</sub> norm of a weight matrix, <inline-formula><mml:math id="mm46"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the <italic>m</italic>th convolution filter weight matrix in the layer, and <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bb;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is a hyperparameter for controlling the model shrinkage or sparsity. The larger <inline-formula><mml:math id="mm48"><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> is, the sparser the trained model will become.</p></sec><sec id="sec2dot5-genes-10-00652"><title>2.5. Sparse Convolutional Denoising Autoencoders</title><p>To leverage the advantages of denoising autoencoders and convolutional networks, we propose a sparse convolutional denoising autoencoders (SCDA) model for genotype imputation. As the highly correlated LD and linkage patterns are key characteristics in genotype data, we use convolutional networks to incorporate these patterns from input data. Each convolutional kernel generates a feature map from the input, and in this process, LD patterns in the filtering window of the convolutional layer can be incorporated. Moreover, we introduce an <italic>L</italic><sub>1</sub> regularization to every convolutional kernel to induce sparsity in the SCDA model.</p><p>The proposed network architecture of the SCDA model is shown in <xref ref-type="fig" rid="genes-10-00652-f003">Figure 3</xref>. There are seven layers in the model, including one input layer and six convolution layers. The input layer takes corrupted data. Each convolution layer is regularized by an <italic>L</italic><sub>1</sub> penalty. The number of convolutional kernels in SCDA model is 32, 64, 128, 128, 64, and 1. The hyperparameter of the <italic>L</italic><sub>1</sub> norm is set at <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In the SCDA model, maxpooling and upsampling are used in the model architecture. Maxpooling is down-sample processing to reduce dimensionality, which applies a max filter to non-overlapping subregions of the previous layer. Maxpooling thus reduces the computational cost by reducing the number of parameters and provides basic translation invariance to the internal representation. Upsampling is an opposite processing to maxpooling. Upsampling is used to increase the dimensionality by repeating data along the axis. The filter size of maxpooling and upsampling of SCDA is fixed as 2 [<xref rid="B66-genes-10-00652" ref-type="bibr">66</xref>].</p><p>To prevent overfitting, we implemented a commonly-used technique called dropout in the SCDA model. Dropout works by removing neurons and their connected edges either at the hidden or visible layers in a neural network. One simple dropout strategy is that each neuron is kept in the network with a retention probability <italic>p</italic> independent of any other neurons. In this study, we use this dropout strategy to empirically set the dropout probability at 25% [<xref rid="B67-genes-10-00652" ref-type="bibr">67</xref>].</p><p>The whole SCDA model was built using TensorFlow v1.13.1 [<xref rid="B68-genes-10-00652" ref-type="bibr">68</xref>] in python3.6, and trained and tested on one NVIDIA GeForce GTX-1080Ti GPU. The batch size is set at 32 and the maximum number of epochs is 1000.</p></sec></sec><sec sec-type="results" id="sec3-genes-10-00652"><title>3. Results</title><sec id="sec3dot1-genes-10-00652"><title>3.1. Optimization of the SCDA Architecture</title><p>One of the most critical steps in building a deep learning model is to optimize the model architecture and tune its hyperparameters. These hyperparameters in most deep learning-based approaches are tuned empirically [<xref rid="B69-genes-10-00652" ref-type="bibr">69</xref>], although automatic machine learning methods have been recently proposed [<xref rid="B70-genes-10-00652" ref-type="bibr">70</xref>]. In this study, we empirically tune the hyperparameters of the SCDA model to achieve an optimized SCDA architecture for genotype imputation. Specifically, we first investigated the number of convolutional layers and the convolutional kernel size, as these two hyperparameters usually have large effects on the final performance of a model. One important concern in genotype imputation is to capture the local linkage or correlation patterns that can be incorporated in convolutional layers in a model. The number of convolutional layers in a deep neural network determines the degree of complexity of the relationship among different linkage patterns that a model can learn. The convolutional kernel size determines the local patterns that convolutional layers can capture. We tested two SCDA architectures with five layers and seven layers, and evaluated their performance for genotype imputation respectively. All convolution kernels had the same filter sizes, ranging from 3 to 19. The other hyperparameters were set as canonical values. For an SCDA model with seven layers, the number of kernels of the six convolution layers was 32, 64, 128, 128, 64, and 1. For an SCDA model with five layers, the number of kernels of the four convolution layers was 32, 64, 32, and 1. Dropout filter size was fixed at 25%, and maxpooling and upsampling filter sizes were set at 2.</p><p>These optimizations were performed on the yeast genotype dataset that was randomly corrupted at a 10% missing level. As shown in <xref ref-type="fig" rid="genes-10-00652-f004">Figure 4</xref>, the seven-layered architecture with the filter size of five achieved the best performance with an accuracy of 0.9986, followed by the five-layered architecture with the filter size of five, with an accuracy of 0.9983. We observed that the filter size of five is the best convolutional filter size in both five-layered and seven-layered SCDA architectures.</p><p>Another important hyperparameter in the SCDA model is the number of convolutional kernels. The more kernels in a layer, the more patterns a convolutional network can capture. However, the number of kernels chosen also depends on the complexity of data. The number of convolutional kernels at a later layer is expected to be larger than that of the previous layer, as the number of possible combinations grows. Here, we increased the number of convolutional kernels by doubling it in the encoder, and reducing it to half in the decoder. We evaluated three combinations (16, 32, 64, 64, 32, 1), (32, 64, 128, 128, 64, 1), and (48, 96, 192, 192, 96, 1). The results in <xref ref-type="fig" rid="genes-10-00652-f005">Figure 5</xref> show that the combination with (32, 64, 128, 128, 64, 1) achieved the best performance.</p><p>Hence, we finalized our SCDA model for genotype imputation as a network infrastructure with seven layers with the number of kernels (32, 64, 128, 128, 64, 1) and a fixed filter size of five. The kernel visualization of first convolution layer of the SCDA model with seven layers is shown in <xref ref-type="fig" rid="genes-10-00652-f006">Figure 6</xref>. We observed that more than half of the weight values are zeros (denoted as grey squares), indicating the sparsity of our model.</p></sec><sec id="sec3dot2-genes-10-00652"><title>3.2. The Performance and Robustness of the SCDA Model</title><p>To evaluate the performance and robustness of our SCDA model, applied the model to the yeast data and the HLA data to impute genotypes in three different missing levels at 5%, 10%, and 20%, respectively. We trained the SCDA models for 10 times by randomly splitting the data into training, validation, and testing datasets with 65%, 15%, and 20% of the data at each missing level. The imputation performance of our SCDA model is shown in <xref rid="genes-10-00652-t002" ref-type="table">Table 2</xref>. For yeast genotype imputation, SCDA has an average accuracy of 0.9978, 0.9977, and 0.9975, respectively, with a standard deviation of 7.0 &#x000d7; 10<sup>&#x02212;5</sup>, 3.9 &#x000d7; 10<sup>&#x02212;5</sup>, and 7.0 &#x000d7; 10<sup>&#x02212;5</sup>, respectively, at missing scenarios with 5%, 10%, and 20% missing values, respectively. Hence, the SCDA achieved comparable performance for the three missing scenarios, which indicates that SCDA works for imputing noisy data where a large proportion of the data is missing (e.g., 20% of missing values). Nonetheless, as expected, its performance is higher when the missing level is lower and the missing data is easier to impute. The low standard deviations indicate that the SCDA model is robust in these scenarios.</p><p>As previously reported, populations have a strong effect on human genotype imputation [<xref rid="B71-genes-10-00652" ref-type="bibr">71</xref>]. Therefore, we tested our proposed SCDA model on HLA genotypes of the EAS super population and the entire five super populations worldwide, respectively. For the HLA genotype imputation of the EAS super population, SCDA has an average accuracy of 0.9975, 0.9952, and 0.9942, respectively, with a standard deviation of 6.0 &#x000d7; 10<sup>&#x02212;5</sup>, 1.4 &#x000d7; 10<sup>&#x02212;4</sup>, and 4.2 &#x000d7; 10<sup>&#x02212;4</sup>, respectively, at three missing scenarios with 5%, 10% and 20% missing values, respectively. For the HLA genotype imputation on the entire super populations, SCDA has an average accuracy of 0.9973, 0.9949, and 0.9896, respectively, with a standard deviation of 1.9 &#x000d7; 10<sup>&#x02212;</sup>4, 7.5 &#x000d7; 10<sup>&#x02212;</sup>5 and 1.5 &#x000d7; 10<sup>&#x02212;4</sup>, respectively, at three missing scenarios with 5%, 10%, and 20% missing values, respectively. As expected, the performance of genotype imputation on a single EAS population is better than that on the entire human population including five super populations. This is because of the fact that human genotypes worldwide are more heterogeneous and complicated across diverse populations, and it is more difficult to capture local LD patterns in genotype imputation.</p><p>Compared with the yeast data, the correlation patterns in HLA genotypes are more dispersed and heterogeneous (<xref ref-type="fig" rid="genes-10-00652-f001">Figure 1</xref>), and thus the imputation of human HLA genotypes is more difficult than that of yeast genotypes. SCDA achieved an overall average accuracy of 0.9977 for yeast genotypes, which is better than the overall average accuracy of 0.9942 for HLA genotypes of EAS and an overall average accuracy of 0.9939 for entire HLA genotypes.</p></sec><sec id="sec3dot3-genes-10-00652"><title>3.3. Comparison with Other Methods</title><p>We compared our reference-free genotype imputation methods, SCDA, to other reference-free imputation methods on these datasets. Particularly, we chose three commonly used statistical inference algorithms for data imputation including row average [<xref rid="B24-genes-10-00652" ref-type="bibr">24</xref>], KNN [<xref rid="B25-genes-10-00652" ref-type="bibr">25</xref>], and SVD [<xref rid="B26-genes-10-00652" ref-type="bibr">26</xref>] for comparison. The average accuracy and standard deviation of each imputation method were calculated by performing the method 10 times at every missing scenario (5%, 10%, and 20%). The total average and standard deviation were calculated by averaging the results of three missing scenarios.</p><p>The imputation results on the yeast data summarized in <xref rid="genes-10-00652-t003" ref-type="table">Table 3</xref> show that the SCDA model achieves a total average accuracy of 0.9977, significantly outperforming the other imputation methods in comparison. KNN and SVD have comparable performance, which performed much better than the simplest strategy of row average imputation. SCDA also has the lowest standard deviation, which means that it is the most robust method compared with the other three imputation methods. The results on imputing HLA genotypes, described in <xref rid="genes-10-00652-t004" ref-type="table">Table 4</xref> and <xref rid="genes-10-00652-t005" ref-type="table">Table 5</xref>, show that the SCDA model outperforms the other three imputation methods in comparison as well, achieving the total average accuracy of 0.9942 and 0.9939 for the EAS super population and the entire human population, respectively.</p><p>We noticed that the methods in comparison, namely the average, KNN, and SVD methods, all achieved much better imputation performance on HLA genotypes than yeast genotypes, as they did not take into account the LD or correlation structures most obvious in the yeast data. Our SCDA model achieved comparable performance on the yeast and HLA genotypes, which indicates SCDA has strong robustness in imputing genotypes from homologous or heterogeneous population backgrounds.</p><p>These imputation results on the yeast and HLA genotypes were also visualized in violin plots in <xref ref-type="fig" rid="genes-10-00652-f007">Figure 7</xref> and <xref ref-type="fig" rid="genes-10-00652-f008">Figure 8</xref>. Higher median point indicates higher performance, and tighter distribution indicates greater robustness. We observed that SCDA has the highest median points and tightest distributions on both yeast and HLA data, which demonstrates that our SCDA model achieved the state-of-the-art performance compared with other methods for data imputation.</p></sec></sec><sec sec-type="discussion" id="sec4-genes-10-00652"><title>4. Discussion</title><p>In summary, we presented a novel deep learning model called SCDA for genotype imputation based on sparse convolutional denoising autoencoders. This SCDA model achieves state-of-the-art imputation accuracy compared with popular reference-free imputation methods. Additionally, the SCDA model is robust in different levels of missing data and heterogeneity of genotype data, making it a competing method for genotype imputation. The nice performance of our SCDA model benefits from its multiple convolutional layers that can extract hidden data patterns and its sparse architecture due to the added <italic>L</italic><sub>1</sub> regularization on the weight matrix.</p><p>In future, we will apply the SCDA model to more complex datasets with untyped genotypes in real scenarios. As the SCDA is based on a deep learning architecture, it is a computationally demanding process to train the model and many hyperparameters are set empirically. We will adopt more efficient training mechanisms and explore more comprehensive and automatic hyperparameter learning. The SCDA model also suffers from the common weakness of deep learning models in that it is hard to explain the prediction mechanisms. We will add prior domain knowledge and provide network visualization in the future to mitigate this limitation of SCDA towards explainable artificial intelligence.</p><p>All in all, this study demonstrates another new application of deep learning to the problem of missing data imputation in genomic studies. Although we originally designed the SCDA model for genotype imputation, our SCDA model can be applied to infer missing values in any data matrix including high-dimensional matrices and tensors. The genotype values we use here are discrete values, but the SCDA model can be extended to impute other kinds of missing values, including quantitative values in gene expression or DNA methylation data. The current SCDA architecture can be extended to multi-task imputation to infer missing values in multiple data sets. This deep learning architecture will thus be of great use in data integration such as omics data integration and imaging genomics.</p></sec></body><back><ack><title>Acknowledgments</title><p>We are grateful for Conor Nodzak&#x02019;s help with obtaining the HLA data from the 1000 Genomes Project.</p></ack><notes><title>Author Contributions</title><p>Conceptualization, J.C. and X.S.; software and implementation, J.C.; writing&#x02014;review and editing, J.C. and X.S.</p></notes><notes><title>Funding</title><p>This research was partially funded by National Science Foundation, USA, to X.S. (Award Number: 1750632).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-genes-10-00652"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wigginton</surname><given-names>J.E.</given-names></name><name><surname>Cutler</surname><given-names>D.J.</given-names></name><name><surname>Abecasis</surname><given-names>G.R.</given-names></name></person-group><article-title>A note on exact tests of Hardy-Weinberg equilibrium</article-title><source>Am. J. Hum. Genet.</source><year>2005</year><volume>76</volume><fpage>887</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1086/429864</pub-id><pub-id pub-id-type="pmid">15789306</pub-id></element-citation></ref><ref id="B2-genes-10-00652"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>Y.F.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Papasian</surname><given-names>C.J.</given-names></name><name><surname>Deng</surname><given-names>H.W.</given-names></name></person-group><article-title>Analyses and comparison of accuracy of different genotype imputation methods</article-title><source>PLoS ONE</source><year>2008</year><volume>3</volume><elocation-id>e3551</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0003551</pub-id><?supplied-pmid 18958166?><pub-id pub-id-type="pmid">18958166</pub-id></element-citation></ref><ref id="B3-genes-10-00652"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>S.R.</given-names></name><name><surname>Browning</surname><given-names>B.L.</given-names></name></person-group><article-title>Rapid and accurate haplotype phasing and missing-data inference for whole-genome association studies by use of localized haplotype clustering</article-title><source>Am. J. Hum. Genet.</source><year>2007</year><volume>81</volume><fpage>1084</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1086/521987</pub-id><?supplied-pmid 17924348?><pub-id pub-id-type="pmid">17924348</pub-id></element-citation></ref><ref id="B4-genes-10-00652"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimmel</surname><given-names>G.</given-names></name><name><surname>Shamir</surname><given-names>R.</given-names></name></person-group><article-title>GERBIL: Genotype resolution and block identification using likelihood</article-title><source>Proc. Natl. Acad. Sci.</source><year>2005</year><volume>102</volume><fpage>158</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1073/pnas.0404730102</pub-id><?supplied-pmid 15615859?><pub-id pub-id-type="pmid">15615859</pub-id></element-citation></ref><ref id="B5-genes-10-00652"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Consortium</surname><given-names>G.P.</given-names></name></person-group><article-title>A global reference for human genetic variation</article-title><source>Nature</source><year>2015</year><volume>526</volume><fpage>68</fpage><pub-id pub-id-type="doi">10.1038/nature15393</pub-id><pub-id pub-id-type="pmid">26432245</pub-id></element-citation></ref><ref id="B6-genes-10-00652"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sudmant</surname><given-names>P.H.</given-names></name><name><surname>Rausch</surname><given-names>T.</given-names></name><name><surname>Gardner</surname><given-names>E.J.</given-names></name><name><surname>Handsaker</surname><given-names>R.E.</given-names></name><name><surname>Abyzov</surname><given-names>A.</given-names></name><name><surname>Huddleston</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Ye</surname><given-names>K.</given-names></name><name><surname>Jun</surname><given-names>G.</given-names></name><name><surname>Fritz</surname><given-names>M.H.Y.</given-names></name></person-group><article-title>An integrated map of structural variation in 2,504 human genomes</article-title><source>Nature</source><year>2015</year><volume>526</volume><fpage>75</fpage><pub-id pub-id-type="doi">10.1038/nature15394</pub-id><?supplied-pmid 26432246?><pub-id pub-id-type="pmid">26432246</pub-id></element-citation></ref><ref id="B7-genes-10-00652"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheet</surname><given-names>P.</given-names></name><name><surname>Stephens</surname><given-names>M.</given-names></name></person-group><article-title>A fast and flexible statistical model for large-scale population genotype data: Applications to inferring missing genotypes and haplotypic phase</article-title><source>Am. J. Hum. Genet.</source><year>2006</year><volume>78</volume><fpage>629</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1086/502802</pub-id><pub-id pub-id-type="pmid">16532393</pub-id></element-citation></ref><ref id="B8-genes-10-00652"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchini</surname><given-names>J.</given-names></name><name><surname>Howie</surname><given-names>B.</given-names></name><name><surname>Myers</surname><given-names>S.</given-names></name><name><surname>McVean</surname><given-names>G.</given-names></name><name><surname>Donnelly</surname><given-names>P.</given-names></name></person-group><article-title>A new multipoint method for genome-wide association studies by imputation of genotypes</article-title><source>Nat. Genet.</source><year>2007</year><volume>39</volume><fpage>906</fpage><pub-id pub-id-type="doi">10.1038/ng2088</pub-id><pub-id pub-id-type="pmid">17572673</pub-id></element-citation></ref><ref id="B9-genes-10-00652"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchini</surname><given-names>J.</given-names></name><name><surname>Howie</surname><given-names>B.</given-names></name></person-group><article-title>Genotype imputation for genome-wide association studies</article-title><source>Nat. Rev. Genet.</source><year>2010</year><volume>11</volume><fpage>499</fpage><pub-id pub-id-type="doi">10.1038/nrg2796</pub-id><?supplied-pmid 20517342?><pub-id pub-id-type="pmid">20517342</pub-id></element-citation></ref><ref id="B10-genes-10-00652"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Consortium</surname><given-names>I.H.</given-names></name></person-group><article-title>The international HapMap project</article-title><source>Nature</source><year>2003</year><volume>426</volume><fpage>789</fpage><pub-id pub-id-type="doi">10.1038/nature02168</pub-id><?supplied-pmid 14685227?><pub-id pub-id-type="pmid">14685227</pub-id></element-citation></ref><ref id="B11-genes-10-00652"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howie</surname><given-names>B.N.</given-names></name><name><surname>Donnelly</surname><given-names>P.</given-names></name><name><surname>Marchini</surname><given-names>J.</given-names></name></person-group><article-title>A flexible and accurate genotype imputation method for the next generation of genome-wide association studies</article-title><source>PLoS Genet.</source><year>2009</year><volume>5</volume><elocation-id>e1000529</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pgen.1000529</pub-id><?supplied-pmid 19543373?><pub-id pub-id-type="pmid">19543373</pub-id></element-citation></ref><ref id="B12-genes-10-00652"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Willer</surname><given-names>C.J.</given-names></name><name><surname>Ding</surname><given-names>J.</given-names></name><name><surname>Scheet</surname><given-names>P.</given-names></name><name><surname>Abecasis</surname><given-names>G.R.</given-names></name></person-group><article-title>MaCH: Using sequence and genotype data to estimate haplotypes and unobserved genotypes</article-title><source>Genet. Epidemiol.</source><year>2010</year><volume>34</volume><fpage>816</fpage><lpage>834</lpage><pub-id pub-id-type="doi">10.1002/gepi.20533</pub-id><?supplied-pmid 21058334?><pub-id pub-id-type="pmid">21058334</pub-id></element-citation></ref><ref id="B13-genes-10-00652"><label>13.</label><element-citation publication-type="web"><article-title>Minimac4</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://genome.sph.umich.edu/wiki/Minimac4">https://genome.sph.umich.edu/wiki/Minimac4</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2019-05-18">(accessed on 18 May 2019)</date-in-citation></element-citation></ref><ref id="B14-genes-10-00652"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>S.</given-names></name><name><surname>Forer</surname><given-names>L.</given-names></name><name><surname>Sch&#x000f6;nherr</surname><given-names>S.</given-names></name><name><surname>Sidore</surname><given-names>C.</given-names></name><name><surname>Locke</surname><given-names>A.E.</given-names></name><name><surname>Kwong</surname><given-names>A.</given-names></name><name><surname>Vrieze</surname><given-names>S.I.</given-names></name><name><surname>Chew</surname><given-names>E.Y.</given-names></name><name><surname>Levy</surname><given-names>S.</given-names></name><name><surname>McGue</surname><given-names>M.</given-names></name></person-group><article-title>Next-generation genotype imputation service and methods</article-title><source>Nat. Genet.</source><year>2016</year><volume>48</volume><fpage>1284</fpage><pub-id pub-id-type="doi">10.1038/ng.3656</pub-id><?supplied-pmid 27571263?><pub-id pub-id-type="pmid">27571263</pub-id></element-citation></ref><ref id="B15-genes-10-00652"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuchsberger</surname><given-names>C.</given-names></name><name><surname>Abecasis</surname><given-names>G.R.</given-names></name><name><surname>Hinds</surname><given-names>D.A.</given-names></name></person-group><article-title>minimac2: Faster genotype imputation</article-title><source>Bioinformatics</source><year>2014</year><volume>31</volume><fpage>782</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu704</pub-id><?supplied-pmid 25338720?><pub-id pub-id-type="pmid">25338720</pub-id></element-citation></ref><ref id="B16-genes-10-00652"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howie</surname><given-names>B.</given-names></name><name><surname>Fuchsberger</surname><given-names>C.</given-names></name><name><surname>Stephens</surname><given-names>M.</given-names></name><name><surname>Marchini</surname><given-names>J.</given-names></name><name><surname>Abecasis</surname><given-names>G.R.</given-names></name></person-group><article-title>Fast and accurate genotype imputation in genome-wide association studies through pre-phasing</article-title><source>Nat. Genet.</source><year>2012</year><volume>44</volume><fpage>955</fpage><pub-id pub-id-type="doi">10.1038/ng.2354</pub-id><pub-id pub-id-type="pmid">22820512</pub-id></element-citation></ref><ref id="B17-genes-10-00652"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>B.L.</given-names></name><name><surname>Browning</surname><given-names>S.R.</given-names></name></person-group><article-title>A unified approach to genotype imputation and haplotype-phase inference for large data sets of trios and unrelated individuals</article-title><source>Am. J. Hum. Genet.</source><year>2009</year><volume>84</volume><fpage>210</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/j.ajhg.2009.01.005</pub-id><?supplied-pmid 19200528?><pub-id pub-id-type="pmid">19200528</pub-id></element-citation></ref><ref id="B18-genes-10-00652"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>B.L.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Browning</surname><given-names>S.R.</given-names></name></person-group><article-title>A one-penny imputed genome from next-generation reference panels</article-title><source>Am. J. Hum. Genet.</source><year>2018</year><volume>103</volume><fpage>338</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.ajhg.2018.07.015</pub-id><pub-id pub-id-type="pmid">30100085</pub-id></element-citation></ref><ref id="B19-genes-10-00652"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purcell</surname><given-names>S.</given-names></name><name><surname>Neale</surname><given-names>B.</given-names></name><name><surname>Todd-Brown</surname><given-names>K.</given-names></name><name><surname>Thomas</surname><given-names>L.</given-names></name><name><surname>Ferreira</surname><given-names>M.A.</given-names></name><name><surname>Bender</surname><given-names>D.</given-names></name><name><surname>Maller</surname><given-names>J.</given-names></name><name><surname>Sklar</surname><given-names>P.</given-names></name><name><surname>De Bakker</surname><given-names>P.I.</given-names></name><name><surname>Daly</surname><given-names>M.J.</given-names></name></person-group><article-title>PLINK: A tool set for whole-genome association and population-based linkage analyses</article-title><source>Am. J. Hum. Genet.</source><year>2007</year><volume>81</volume><fpage>559</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1086/519795</pub-id><pub-id pub-id-type="pmid">17701901</pub-id></element-citation></ref><ref id="B20-genes-10-00652"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>D.</given-names></name><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Huang</surname><given-names>B.</given-names></name></person-group><article-title>Simple and efficient analysis of disease association with missing genotype data</article-title><source>Am. J. Hum. Genet.</source><year>2008</year><volume>82</volume><fpage>444</fpage><lpage>452</lpage><pub-id pub-id-type="doi">10.1016/j.ajhg.2007.11.004</pub-id><pub-id pub-id-type="pmid">18252224</pub-id></element-citation></ref><ref id="B21-genes-10-00652"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolae</surname><given-names>D.L.</given-names></name></person-group><article-title>Testing Untyped Alleles (TUNA)&#x02014;applications to genome&#x02013;wide association studies</article-title><source>Genet. Epidemiol. Off. Publ. Int. Genet. Epidemiol. Soc.</source><year>2006</year><volume>30</volume><fpage>718</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1002/gepi.20182</pub-id></element-citation></ref><ref id="B22-genes-10-00652"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>G.C.</given-names></name><name><surname>Esposito</surname><given-names>L.</given-names></name><name><surname>Barratt</surname><given-names>B.J.</given-names></name><name><surname>Smith</surname><given-names>A.N.</given-names></name><name><surname>Heward</surname><given-names>J.</given-names></name><name><surname>Di Genova</surname><given-names>G.</given-names></name><name><surname>Ueda</surname><given-names>H.</given-names></name><name><surname>Cordell</surname><given-names>H.J.</given-names></name><name><surname>Eaves</surname><given-names>I.A.</given-names></name><name><surname>Dudbridge</surname><given-names>F.</given-names></name></person-group><article-title>Haplotype tagging for the identification of common disease genes</article-title><source>Nat. Genet.</source><year>2001</year><volume>29</volume><fpage>233</fpage><pub-id pub-id-type="doi">10.1038/ng1001-233</pub-id><?supplied-pmid 11586306?><pub-id pub-id-type="pmid">11586306</pub-id></element-citation></ref><ref id="B23-genes-10-00652"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>D.M.</given-names></name><name><surname>Cardon</surname><given-names>L.R.</given-names></name><name><surname>Morris</surname><given-names>A.P.</given-names></name></person-group><article-title>Genotype prediction using a dense map of SNPs</article-title><source>Genet. Epidemiol. Off. Publ. Int. Genet. Epidemiol. Soc.</source><year>2004</year><volume>27</volume><fpage>375</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1002/gepi.20045</pub-id><?supplied-pmid 15543641?><pub-id pub-id-type="pmid">15543641</pub-id></element-citation></ref><ref id="B24-genes-10-00652"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Little</surname><given-names>R.J.</given-names></name><name><surname>Rubin</surname><given-names>D.B.</given-names></name></person-group><source>Statistical Analysis with Missing Data</source><publisher-name>Wiley</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2019</year><volume>Volume 793</volume></element-citation></ref><ref id="B25-genes-10-00652"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname><given-names>S.</given-names></name><name><surname>Tutz</surname><given-names>G.</given-names></name></person-group><article-title>Nearest neighbor imputation for categorical data by weighting of attributes</article-title><source>arXiv Preprint</source><year>2017</year><pub-id pub-id-type="arxiv">1710.01011</pub-id></element-citation></ref><ref id="B26-genes-10-00652"><label>26.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T.</given-names></name><name><surname>Tibshirani</surname><given-names>R.</given-names></name><name><surname>Sherlock</surname><given-names>G.</given-names></name><name><surname>Eisen</surname><given-names>M.</given-names></name><name><surname>Brown</surname><given-names>P.</given-names></name><name><surname>Botstein</surname><given-names>D.</given-names></name></person-group><source>Imputing Missing Data for Gene Expression Arrays</source><publisher-name>Stanford University Statistics Department Technical</publisher-name><publisher-loc>Stanford, CA, USA</publisher-loc><year>1999</year></element-citation></ref><ref id="B27-genes-10-00652"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harrell</surname><given-names>F.E.</given-names><suffix>Jr.</suffix></name></person-group><source>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2015</year></element-citation></ref><ref id="B28-genes-10-00652"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stekhoven</surname><given-names>D.J.</given-names></name></person-group><article-title>Missforest: Nonparametric missing value imputation using random forest</article-title><source>Astrophys. Source Code Libr.</source><year>2015</year><volume>28</volume><fpage>112</fpage><lpage>118</lpage></element-citation></ref><ref id="B29-genes-10-00652"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royston</surname><given-names>P.</given-names></name></person-group><article-title>Multiple imputation of missing values</article-title><source>Stata J.</source><year>2004</year><volume>4</volume><fpage>227</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1177/1536867X0400400301</pub-id></element-citation></ref><ref id="B30-genes-10-00652"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troyanskaya</surname><given-names>O.</given-names></name><name><surname>Cantor</surname><given-names>M.</given-names></name><name><surname>Sherlock</surname><given-names>G.</given-names></name><name><surname>Brown</surname><given-names>P.</given-names></name><name><surname>Hastie</surname><given-names>T.</given-names></name><name><surname>Tibshirani</surname><given-names>R.</given-names></name><name><surname>Botstein</surname><given-names>D.</given-names></name><name><surname>Altman</surname><given-names>R.B.</given-names></name></person-group><article-title>Missing value estimation methods for DNA microarrays</article-title><source>Bioinformatics</source><year>2001</year><volume>17</volume><fpage>520</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/17.6.520</pub-id><?supplied-pmid 11395428?><pub-id pub-id-type="pmid">11395428</pub-id></element-citation></ref><ref id="B31-genes-10-00652"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sentas</surname><given-names>P.</given-names></name><name><surname>Angelis</surname><given-names>L.</given-names></name></person-group><article-title>Categorical missing data imputation for software cost estimation by multinomial logistic regression</article-title><source>J. Syst. Softw.</source><year>2006</year><volume>79</volume><fpage>404</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1016/j.jss.2005.02.026</pub-id></element-citation></ref><ref id="B32-genes-10-00652"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>A.D.</given-names></name><name><surname>Bartlett</surname><given-names>J.W.</given-names></name><name><surname>Carpenter</surname><given-names>J.</given-names></name><name><surname>Nicholas</surname><given-names>O.</given-names></name><name><surname>Hemingway</surname><given-names>H.</given-names></name></person-group><article-title>Comparison of random forest and parametric imputation models for imputing missing data using MICE: A CALIBER study</article-title><source>Am. J. Epidemiol.</source><year>2014</year><volume>179</volume><fpage>764</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.1093/aje/kwt312</pub-id><pub-id pub-id-type="pmid">24589914</pub-id></element-citation></ref><ref id="B33-genes-10-00652"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="B34-genes-10-00652"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Hoi</surname><given-names>S.C.H.</given-names></name><name><surname>Wu</surname><given-names>P.</given-names></name><name><surname>Zhu</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>Deep learning for content-based image retrieval: A comprehensive study</article-title><source>Proceedings of the 22nd ACM international conference on Multimedia</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>3&#x02013;7 November 2014</conf-date><fpage>157</fpage><lpage>166</lpage></element-citation></ref><ref id="B35-genes-10-00652"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>N.</given-names></name><name><surname>Yeung</surname><given-names>D.Y.</given-names></name></person-group><article-title>Learning a deep compact image representation for visual tracking</article-title><source>Proceedings of the Advances in neural information processing systems</source><conf-loc>Nevada, NJ, USA</conf-loc><conf-date>5&#x02013;10 December 2013</conf-date><fpage>809</fpage><lpage>817</lpage></element-citation></ref><ref id="B36-genes-10-00652"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G.</given-names></name><name><surname>Deng</surname><given-names>L.</given-names></name><name><surname>Yu</surname><given-names>D.</given-names></name><name><surname>Dahl</surname><given-names>G.</given-names></name><name><surname>Mohamed</surname><given-names>A.R.</given-names></name><name><surname>Jaitly</surname><given-names>N.</given-names></name><name><surname>Senior</surname><given-names>A.</given-names></name><name><surname>Vanhoucke</surname><given-names>V.</given-names></name><name><surname>Nguyen</surname><given-names>P.</given-names></name><name><surname>Kingsbury</surname><given-names>B.</given-names></name></person-group><article-title>Deep neural networks for acoustic modeling in speech recognition</article-title><source>Ieee Signal Process. Mag.</source><year>2012</year><volume>29</volume><fpage>82</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1109/MSP.2012.2205597</pub-id></element-citation></ref><ref id="B37-genes-10-00652"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>D.</given-names></name><name><surname>Yao</surname><given-names>K.</given-names></name><name><surname>Su</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>G.</given-names></name><name><surname>Seide</surname><given-names>F.</given-names></name></person-group><article-title>KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition</article-title><source>Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>26&#x02013;31 May 2013</conf-date><fpage>7893</fpage><lpage>7897</lpage></element-citation></ref><ref id="B38-genes-10-00652"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Collobert</surname><given-names>R.</given-names></name><name><surname>Weston</surname><given-names>J.</given-names></name></person-group><article-title>A unified architecture for natural language processing: Deep neural networks with multitask learning</article-title><source>Proceedings of the 25th international conference on Machine learning</source><conf-loc>Helsinki, Finland</conf-loc><conf-date>5&#x02013;9 July 2008</conf-date><fpage>160</fpage><lpage>167</lpage></element-citation></ref><ref id="B39-genes-10-00652"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>T.</given-names></name><name><surname>Hazarika</surname><given-names>D.</given-names></name><name><surname>Poria</surname><given-names>S.</given-names></name><name><surname>Cambria</surname><given-names>E.</given-names></name></person-group><article-title>Recent trends in deep learning based natural language processing</article-title><source>Ieee Comput. Intell. Mag.</source><year>2018</year><volume>13</volume><fpage>55</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1109/MCI.2018.2840738</pub-id></element-citation></ref><ref id="B40-genes-10-00652"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Min</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>B.</given-names></name><name><surname>Yoon</surname><given-names>S.</given-names></name></person-group><article-title>Deep learning in bioinformatics</article-title><source>Brief. Bioinform.</source><year>2017</year><volume>18</volume><fpage>851</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1093/bib/bbw068</pub-id><?supplied-pmid 27473064?><pub-id pub-id-type="pmid">27473064</pub-id></element-citation></ref><ref id="B41-genes-10-00652"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poplin</surname><given-names>R.</given-names></name><name><surname>Chang</surname><given-names>P.C.</given-names></name><name><surname>Alexander</surname><given-names>D.</given-names></name><name><surname>Schwartz</surname><given-names>S.</given-names></name><name><surname>Colthurst</surname><given-names>T.</given-names></name><name><surname>Ku</surname><given-names>A.</given-names></name><name><surname>Newburger</surname><given-names>D.</given-names></name><name><surname>Dijamco</surname><given-names>J.</given-names></name><name><surname>Nguyen</surname><given-names>N.</given-names></name><name><surname>Afshar</surname><given-names>P.T.</given-names></name></person-group><article-title>A universal SNP and small-indel variant caller using deep neural networks</article-title><source>Nat. Biotechnol.</source><year>2018</year><volume>36</volume><fpage>983</fpage><pub-id pub-id-type="doi">10.1038/nbt.4235</pub-id><?supplied-pmid 30247488?><pub-id pub-id-type="pmid">30247488</pub-id></element-citation></ref><ref id="B42-genes-10-00652"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Troyanskaya</surname><given-names>O.G.</given-names></name></person-group><article-title>Predicting effects of noncoding variants with deep learning&#x02013;based sequence model</article-title><source>Nat. Methods</source><year>2015</year><volume>12</volume><fpage>931</fpage><pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id><?supplied-pmid 26301843?><pub-id pub-id-type="pmid">26301843</pub-id></element-citation></ref><ref id="B43-genes-10-00652"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Pan</surname><given-names>Z.</given-names></name><name><surname>Ying</surname><given-names>Y.</given-names></name><name><surname>Xie</surname><given-names>Z.</given-names></name><name><surname>Adhikari</surname><given-names>S.</given-names></name><name><surname>Phillips</surname><given-names>J.</given-names></name><name><surname>Carstens</surname><given-names>R.P.</given-names></name><name><surname>Black</surname><given-names>D.L.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Xing</surname><given-names>Y.</given-names></name></person-group><article-title>Deep-learning augmented RNA-seq analysis of transcript splicing</article-title><source>Nat. Methods</source><year>2019</year><volume>16</volume><fpage>307</fpage><pub-id pub-id-type="doi">10.1038/s41592-019-0351-9</pub-id><?supplied-pmid 30923373?><pub-id pub-id-type="pmid">30923373</pub-id></element-citation></ref><ref id="B44-genes-10-00652"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>R.</given-names></name><name><surname>Jumper</surname><given-names>J.</given-names></name><name><surname>Kirkpatrick</surname><given-names>J.</given-names></name><name><surname>Sifre</surname><given-names>L.</given-names></name><name><surname>Green</surname><given-names>T.</given-names></name><name><surname>Qin</surname><given-names>C.</given-names></name><name><surname>Zidek</surname><given-names>A.</given-names></name><name><surname>Nelson</surname><given-names>A.</given-names></name><name><surname>Bridgland</surname><given-names>A.</given-names></name><name><surname>Penedones</surname><given-names>H.</given-names></name></person-group><article-title>De novo structure prediction with deeplearning based scoring</article-title><source>Annu. Rev. Biochem.</source><year>2018</year><volume>77</volume><fpage>363</fpage><lpage>382</lpage></element-citation></ref><ref id="B45-genes-10-00652"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J.</given-names></name></person-group><article-title>Distance-based protein folding powered by deep learning</article-title><source>Proc. Natl. Acad. Sci.</source><year>2019</year><volume>116</volume><fpage>16856</fpage><lpage>16865</lpage><pub-id pub-id-type="doi">10.1073/pnas.1821309116</pub-id><?supplied-pmid 31399549?><pub-id pub-id-type="pmid">31399549</pub-id></element-citation></ref><ref id="B46-genes-10-00652"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spencer</surname><given-names>M.</given-names></name><name><surname>Eickholt</surname><given-names>J.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name></person-group><article-title>A deep learning network approach to ab initio protein secondary structure prediction</article-title><source>IEEE/ACM Trans. Comput. Biol. Bioinform. (Tcbb)</source><year>2015</year><volume>12</volume><fpage>103</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1109/TCBB.2014.2343960</pub-id></element-citation></ref><ref id="B47-genes-10-00652"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jo</surname><given-names>T.</given-names></name><name><surname>Hou</surname><given-names>J.</given-names></name><name><surname>Eickholt</surname><given-names>J.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name></person-group><article-title>Improving protein fold recognition by deep learning networks</article-title><source>Sci. Rep.</source><year>2015</year><volume>5</volume><fpage>17573</fpage><pub-id pub-id-type="doi">10.1038/srep17573</pub-id><pub-id pub-id-type="pmid">26634993</pub-id></element-citation></ref><ref id="B48-genes-10-00652"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Weng</surname><given-names>S.</given-names></name><name><surname>Ma</surname><given-names>J.</given-names></name><name><surname>Tang</surname><given-names>Q.</given-names></name></person-group><article-title>DeepCNF-D: Predicting protein order/disorder regions by weighted deep convolutional neural fields</article-title><source>Int. J. Mol. Sci.</source><year>2015</year><volume>16</volume><fpage>17315</fpage><lpage>17330</lpage><pub-id pub-id-type="doi">10.3390/ijms160817315</pub-id><pub-id pub-id-type="pmid">26230689</pub-id></element-citation></ref><ref id="B49-genes-10-00652"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Hu</surname><given-names>H.</given-names></name><name><surname>Gong</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Cheng</surname><given-names>C.</given-names></name><name><surname>Zeng</surname><given-names>J.</given-names></name></person-group><article-title>A deep learning framework for modeling structural features of RNA-binding protein targets</article-title><source>Nucleic Acids Res.</source><year>2015</year><volume>44</volume><fpage>e32</fpage><pub-id pub-id-type="doi">10.1093/nar/gkv1025</pub-id><?supplied-pmid 26467480?><pub-id pub-id-type="pmid">26467480</pub-id></element-citation></ref><ref id="B50-genes-10-00652"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>B.</given-names></name></person-group><article-title>Protein remote homology detection based on bidirectional long short-term memory</article-title><source>BMC Bioinform.</source><year>2017</year><volume>18</volume><elocation-id>443</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-017-1842-2</pub-id><?supplied-pmid 29017445?><pub-id pub-id-type="pmid">29017445</pub-id></element-citation></ref><ref id="B51-genes-10-00652"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Narayan</surname><given-names>R.</given-names></name><name><surname>Subramanian</surname><given-names>A.</given-names></name><name><surname>Xie</surname><given-names>X.</given-names></name></person-group><article-title>Gene expression inference with deep learning</article-title><source>Bioinformatics</source><year>2016</year><volume>32</volume><fpage>1832</fpage><lpage>1839</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btw074</pub-id><?supplied-pmid 26873929?><pub-id pub-id-type="pmid">26873929</pub-id></element-citation></ref><ref id="B52-genes-10-00652"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leung</surname><given-names>M.K.</given-names></name><name><surname>Xiong</surname><given-names>H.Y.</given-names></name><name><surname>Lee</surname><given-names>L.J.</given-names></name><name><surname>Frey</surname><given-names>B.J.</given-names></name></person-group><article-title>Deep learning of the tissue-regulated splicing code</article-title><source>Bioinformatics</source><year>2014</year><volume>30</volume><fpage>i121</fpage><lpage>i129</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu277</pub-id><?supplied-pmid 24931975?><pub-id pub-id-type="pmid">24931975</pub-id></element-citation></ref><ref id="B53-genes-10-00652"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>C.</given-names></name><name><surname>Lee</surname><given-names>H.J.</given-names></name><name><surname>Reik</surname><given-names>W.</given-names></name><name><surname>Stegle</surname><given-names>O.</given-names></name></person-group><article-title>DeepCpG: Accurate prediction of single-cell DNA methylation states using deep learning</article-title><source>Genome Biol.</source><year>2017</year><volume>18</volume><fpage>67</fpage><pub-id pub-id-type="doi">10.1186/s13059-017-1189-z</pub-id><pub-id pub-id-type="pmid">28395661</pub-id></element-citation></ref><ref id="B54-genes-10-00652"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>An</surname><given-names>L.</given-names></name><name><surname>Xu</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Zheng</surname><given-names>W.J.</given-names></name><name><surname>Hu</surname><given-names>M.</given-names></name><name><surname>Tang</surname><given-names>J.</given-names></name><name><surname>Yue</surname><given-names>F.</given-names></name></person-group><article-title>Enhancing Hi-C data resolution with deep convolutional neural network HiCPlus</article-title><source>Nature communications</source><year>2018</year><volume>9</volume><fpage>750</fpage><pub-id pub-id-type="doi">10.1038/s41467-018-03113-2</pub-id><?supplied-pmid 29467363?><pub-id pub-id-type="pmid">29467363</pub-id></element-citation></ref><ref id="B55-genes-10-00652"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaulieu-Jones</surname><given-names>B.K.</given-names></name><name><surname>Moore</surname><given-names>J.H.</given-names></name></person-group><article-title>Missing data imputation in the electronic health record using deeply learned autoencoders</article-title><source>Pac. Symp. Biocomput.</source><year>2017</year><volume>22</volume><fpage>207</fpage><lpage>218</lpage><pub-id pub-id-type="pmid">27896976</pub-id></element-citation></ref><ref id="B56-genes-10-00652"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>Y.</given-names></name><name><surname>Lv</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>Y.L.</given-names></name><name><surname>Wang</surname><given-names>F.Y.</given-names></name></person-group><article-title>An efficient realization of deep learning for traffic data imputation</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2016</year><volume>72</volume><fpage>168</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2016.09.015</pub-id></element-citation></ref><ref id="B57-genes-10-00652"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marivate</surname><given-names>V.N.</given-names></name><name><surname>Nelwamondo</surname><given-names>F.V.</given-names></name><name><surname>Marwala</surname><given-names>T.</given-names></name></person-group><article-title>Investigation into the use of autoencoder neural networks, principal component analysis and support vector regression in estimating missing HIV data</article-title><source>IFAC Proc.</source><year>2008</year><volume>41</volume><fpage>682</fpage><lpage>689</lpage><pub-id pub-id-type="doi">10.3182/20080706-5-KR-1001.00115</pub-id></element-citation></ref><ref id="B58-genes-10-00652"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>Y.L.</given-names></name><name><surname>Zheng</surname><given-names>H.</given-names></name><name><surname>Gevaert</surname><given-names>O.</given-names></name></person-group><article-title>A deep learning framework for imputing missing values in genomic data</article-title><source>bioRxiv</source><year>2018</year><pub-id pub-id-type="doi">10.1101/406066</pub-id></element-citation></ref><ref id="B59-genes-10-00652"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloom</surname><given-names>J.S.</given-names></name><name><surname>Kotenko</surname><given-names>I.</given-names></name><name><surname>Sadhu</surname><given-names>M.J.</given-names></name><name><surname>Treusch</surname><given-names>S.</given-names></name><name><surname>Albert</surname><given-names>F.W.</given-names></name><name><surname>Kruglyak</surname><given-names>L.</given-names></name></person-group><article-title>Genetic interactions contribute less than additive effects to quantitative trait variation in yeast</article-title><source>Nat. Commun.</source><year>2015</year><volume>6</volume><fpage>8712</fpage><pub-id pub-id-type="doi">10.1038/ncomms9712</pub-id><?supplied-pmid 26537231?><pub-id pub-id-type="pmid">26537231</pub-id></element-citation></ref><ref id="B60-genes-10-00652"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terasaki</surname><given-names>P.I.</given-names></name><name><surname>Cai</surname><given-names>J.</given-names></name></person-group><article-title>Human leukocyte antigen antibodies and chronic rejection: From association to causation</article-title><source>Transplantation</source><year>2008</year><volume>86</volume><fpage>377</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1097/TP.0b013e31817c4cb8</pub-id><?supplied-pmid 18698239?><pub-id pub-id-type="pmid">18698239</pub-id></element-citation></ref><ref id="B61-genes-10-00652"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>R.</given-names></name><name><surname>Wen</surname><given-names>J.</given-names></name><name><surname>Quitadamo</surname><given-names>A.</given-names></name><name><surname>Cheng</surname><given-names>J.</given-names></name><name><surname>Shi</surname><given-names>X.</given-names></name></person-group><article-title>A deep auto-encoder model for gene expression prediction</article-title><source>BMC Genom.</source><year>2017</year><volume>18</volume><elocation-id>845</elocation-id><pub-id pub-id-type="doi">10.1186/s12864-017-4226-0</pub-id><?supplied-pmid 29219072?><pub-id pub-id-type="pmid">29219072</pub-id></element-citation></ref><ref id="B62-genes-10-00652"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldi</surname><given-names>P.</given-names></name></person-group><article-title>Autoencoders, unsupervised learning, and deep architectures</article-title><source>Proc. Icml Workshop Unsupervised Transf. Learn.</source><year>2012</year><volume>27</volume><fpage>37</fpage><lpage>49</lpage></element-citation></ref><ref id="B63-genes-10-00652"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>J.</given-names></name><name><surname>Moraga</surname><given-names>C.</given-names></name></person-group><article-title>The influence of the sigmoid function parameters on the speed of backpropagation learning</article-title><source>Lect. Notes Comput. Sci.</source><year>1995</year><fpage>195</fpage><lpage>201</lpage></element-citation></ref><ref id="B64-genes-10-00652"><label>64.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dahl</surname><given-names>G.E.</given-names></name><name><surname>Sainath</surname><given-names>T.N.</given-names></name><name><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>Improving deep neural networks for LVCSR using rectified linear units and dropout</article-title><source>Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and signal Processing (ICASSP 2013)</source><conf-loc>Vancouver, Canada</conf-loc><conf-date>26&#x02013;31 May 2013</conf-date><fpage>8609</fpage><lpage>8613</lpage></element-citation></ref><ref id="B65-genes-10-00652"><label>65.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>P.</given-names></name><name><surname>Larochelle</surname><given-names>H.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Manzagol</surname><given-names>P.A.</given-names></name></person-group><article-title>Extracting and composing robust features with denoising autoencoders</article-title><source>Proceedings of the 25th international conference on Machine learning</source><conf-loc>Helsinki, Finland</conf-loc><conf-date>5&#x02013;9 June 2008</conf-date><fpage>1096</fpage><lpage>1103</lpage></element-citation></ref><ref id="B66-genes-10-00652"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A.</given-names></name><name><surname>Metz</surname><given-names>L.</given-names></name><name><surname>Chintala</surname><given-names>S.</given-names></name></person-group><article-title>Unsupervised representation learning with deep convolutional generative adversarial networks</article-title><source>Arxiv Preprint</source><year>2015</year><pub-id pub-id-type="arxiv">1511.06434</pub-id></element-citation></ref><ref id="B67-genes-10-00652"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achille</surname><given-names>A.</given-names></name><name><surname>Soatto</surname><given-names>S.</given-names></name></person-group><article-title>Information dropout: Learning optimal representations through noisy computation</article-title><source>Ieee Trans. Pattern Anal. Mach. Intell.</source><year>2018</year><volume>40</volume><fpage>2897</fpage><lpage>2905</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2784440</pub-id><?supplied-pmid 29994167?><pub-id pub-id-type="pmid">29994167</pub-id></element-citation></ref><ref id="B68-genes-10-00652"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M.</given-names></name><name><surname>Agarwal</surname><given-names>A.</given-names></name><name><surname>Barham</surname><given-names>P.</given-names></name><name><surname>Brevdo</surname><given-names>E.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Citro</surname><given-names>C.</given-names></name><name><surname>Corrado</surname><given-names>G.S.</given-names></name><name><surname>Davis</surname><given-names>A.</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name><name><surname>Devin</surname><given-names>M.</given-names></name></person-group><article-title>Tensorflow: Large-scale machine learning on heterogeneous distributed systems</article-title><source>Arxiv Preprint</source><year>2016</year><pub-id pub-id-type="arxiv">1603.04467</pub-id></element-citation></ref><ref id="B69-genes-10-00652"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergstra</surname><given-names>J.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Random search for hyper-parameter optimization</article-title><source>J. Mach. Learn. Res.</source><year>2012</year><volume>13</volume><fpage>281</fpage><lpage>305</lpage></element-citation></ref><ref id="B70-genes-10-00652"><label>70.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Feurer</surname><given-names>M.</given-names></name><name><surname>Eggensperger</surname><given-names>K.</given-names></name><name><surname>Falkner</surname><given-names>S.</given-names></name><name><surname>Lindauer</surname><given-names>M.</given-names></name><name><surname>Hutter</surname><given-names>F.</given-names></name></person-group><article-title>Practical automated machine learning for the automl challenge 2018</article-title><source>Proceedings of the International Workshop on Automatic Machine Learning at ICML (ICML 2018)</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>14 July 2018</conf-date><fpage>1189</fpage><lpage>1232</lpage></element-citation></ref><ref id="B71-genes-10-00652"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Willer</surname><given-names>C.</given-names></name><name><surname>Sanna</surname><given-names>S.</given-names></name><name><surname>Abecasis</surname><given-names>G.</given-names></name></person-group><article-title>Genotype imputation</article-title><source>Annu. Rev. Genom. Hum. Genet.</source><year>2009</year><volume>10</volume><fpage>387</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1146/annurev.genom.9.081307.164242</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="genes-10-00652-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The visualization of genotype profiles of two samples with 5% missing values in (<bold>a</bold>) yeast (BY represents genotypes from a laboratory strain and RM stands for genotypes from a vineyard strain) and (<bold>b</bold>) human leukocyte antigen (HLA) datasets shows the different correlated patterns in the data. Synthetically generated missing values are denoted in white color, while typed genotypes are color coded.</p></caption><graphic xlink:href="genes-10-00652-g001"/></fig><fig id="genes-10-00652-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>An illustration of a (<bold>a</bold>) standard autoencoder and (<bold>b</bold>) denoising autoencoder. An autoencoder is composed of two parts: encoder and decoder. The encoder takes an input vector <italic>x</italic> and maps it to a hidden representation <italic>h</italic>. The decoder takes a hidden representation <italic>h</italic> to map it to a reconstructed vector <italic>z</italic>. The aim of an autoencoder is to generate a reconstruction <italic>z</italic> of the input data such that <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02248;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> by minimizing the loss function <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. A denoising autoencoder differs from a standard autoencoder in that the input <italic>x</italic> is corrupted with noises or missing values.</p></caption><graphic xlink:href="genes-10-00652-g002"/></fig><fig id="genes-10-00652-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The network architecture of SCDA. The sparse convolutional denoising autoencoders (SCDA) model consists of multiple convolution layers in a hierarchical way, and each convolution layer is regularized by an <italic>L</italic><sub>1</sub> penalty. SCDA takes corrupted data with missing values as input to learn a hidden representation, and then reconstructs the input based on hidden representations to impute missing values.</p></caption><graphic xlink:href="genes-10-00652-g003"/></fig><fig id="genes-10-00652-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Results of using the SCDA model on the yeast data. (<bold>a</bold>) The results of the five-layered SCDA with filter sizes ranging from 3 to 19. (<bold>b</bold>) The results of the five-layered SCDA with filter sizes ranging from 3 to 19.</p></caption><graphic xlink:href="genes-10-00652-g004"/></fig><fig id="genes-10-00652-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Results of SCDA with different number of kernels on yeast data. The kernel combination with (32, 64, 128, 128, 64, 1) achieves the best performance.</p></caption><graphic xlink:href="genes-10-00652-g005"/></fig><fig id="genes-10-00652-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Kernel visualization in the first convolution layer of the SCDA model. There are 32 kernels in the first convolution layer, and each kernel has five squares. The squares in green indicate positive weights, and those in magenta represent negative weights. The deeper the color, the larger the absolute value of the weight. The grey squares represent weights with zero values.</p></caption><graphic xlink:href="genes-10-00652-g006"/></fig><fig id="genes-10-00652-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Violin plots of the accuracy values of different imputation methods on yeast genotypes at three missing levels. KNN, k-nearest neighbors; SVD, singular value decomposition.</p></caption><graphic xlink:href="genes-10-00652-g007"/></fig><fig id="genes-10-00652-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Violin plots of the accuracy values different imputation methods on HLA genotypes at three missing levels. (<bold>a</bold>) Imputation results on HLA genotypes of EAS super population. (<bold>b</bold>) Imputation results on HLA genotypes of the entire human population including five super populations.</p></caption><graphic xlink:href="genes-10-00652-g008"/></fig><table-wrap id="genes-10-00652-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">genes-10-00652-t001_Table 1</object-id><label>Table 1</label><caption><p>Summary of existing genotype imputation methods. HMM, hidden Markov model; SNP, single nucleotide polymorphism; KNN, k-nearest neighbors; SVD, singular value decomposition.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Categories</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithms</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">References</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Reference panel required</td><td align="center" valign="middle" rowspan="1" colspan="1">fastPHASE</td><td align="center" valign="middle" rowspan="1" colspan="1">Haplotype-cluster</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B7-genes-10-00652" ref-type="bibr">7</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">IMPUTE</td><td align="center" valign="middle" rowspan="1" colspan="1">HMM</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B8-genes-10-00652" ref-type="bibr">8</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">IMPUTE2</td><td align="center" valign="middle" rowspan="1" colspan="1">HMM</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B11-genes-10-00652" ref-type="bibr">11</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">MACH</td><td align="center" valign="middle" rowspan="1" colspan="1">HMM</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B12-genes-10-00652" ref-type="bibr">12</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">BEAGLE</td><td align="center" valign="middle" rowspan="1" colspan="1">Graphical model</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B17-genes-10-00652" ref-type="bibr">17</xref>,<xref rid="B18-genes-10-00652" ref-type="bibr">18</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">PLINK</td><td align="center" valign="middle" rowspan="1" colspan="1">Tag SNP</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B19-genes-10-00652" ref-type="bibr">19</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">SNPMSTAT</td><td align="center" valign="middle" rowspan="1" colspan="1">Tag SNP</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B20-genes-10-00652" ref-type="bibr">20</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Reference panel-free</td><td align="center" valign="middle" rowspan="1" colspan="1">Row average</td><td align="center" valign="middle" rowspan="1" colspan="1">Mean value</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B24-genes-10-00652" ref-type="bibr">24</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">Nearest Neighbors</td><td align="center" valign="middle" rowspan="1" colspan="1">KNN</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B25-genes-10-00652" ref-type="bibr">25</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">Matrix decomposition</td><td align="center" valign="middle" rowspan="1" colspan="1">SVD</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B26-genes-10-00652" ref-type="bibr">26</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">Regression prediction</td><td align="center" valign="middle" rowspan="1" colspan="1">Logistic regression</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B27-genes-10-00652" ref-type="bibr">27</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification prediction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B28-genes-10-00652" ref-type="bibr">28</xref>]</td></tr></tbody></table></table-wrap><table-wrap id="genes-10-00652-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">genes-10-00652-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance comparison in terms of accuracy on yeast and human leukocyte antigen (HLA) genotypes at three missing scenarios. The average accuracy and standard deviation of each imputation method was calculated by running the sparse convolutional denoising autoencoders (SCDA) model 10 times at every missing scenario (5%, 10%, 20%). The overall average and standard deviation values were calculated by averaging the results of three missing scenarios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">5%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">10%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">20%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Total</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yeast</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9978</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mn>7.0</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9977</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm51"><mml:mrow><mml:mrow><mml:mn>3.9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9975</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:mn>7.0</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9977</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:mn>6.0</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HLA_EAS <sup>1</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9975</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:mn>6.0</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9952</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mn>1.4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9900</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:mn>4.2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9942</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:mn>2.1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HLA_Entire <sup>2</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9973</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mn>1.9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9949</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:mn>7.5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9896</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9939</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:mn>1.4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> HLA genotypes of EAS super population from 1000 Genome Project. <sup>2</sup> HLA genotypes from the entire dataset in five super populations in the 1000 Genome Project.</p></fn></table-wrap-foot></table-wrap><table-wrap id="genes-10-00652-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">genes-10-00652-t003_Table 3</object-id><label>Table 3</label><caption><p>Performance comparison on yeast genotypes at three missing scenarios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Methods</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">5%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">10%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">20%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Total</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4862</td><td align="center" valign="middle" rowspan="1" colspan="1">1.9 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.4885</td><td align="center" valign="middle" rowspan="1" colspan="1">9.5 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.4911</td><td align="center" valign="middle" rowspan="1" colspan="1">6.7 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.4886</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2 &#x000d7; 10<sup>&#x02212;4</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7248</td><td align="center" valign="middle" rowspan="1" colspan="1">1.7 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.7240</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.7222</td><td align="center" valign="middle" rowspan="1" colspan="1">9.4 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.7237</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3 &#x000d7; 10<sup>&#x02212;4</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SVD</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6580</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.6580</td><td align="center" valign="middle" rowspan="1" colspan="1">1.1 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.6576</td><td align="center" valign="middle" rowspan="1" colspan="1">1.1 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.6579</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3 &#x000d7; 10<sup>&#x02212;4</sup></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SCDA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9978</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9977</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9975</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9977</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0 &#x000d7; 10<sup>&#x02212;5</sup></td></tr></tbody></table></table-wrap><table-wrap id="genes-10-00652-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">genes-10-00652-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance comparison on HLA genotypes of EAS super population at three missing scenarios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Methods</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">5%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">10%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">20%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Total</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9549</td><td align="center" valign="middle" rowspan="1" colspan="1">3.2 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9549</td><td align="center" valign="middle" rowspan="1" colspan="1">1.7 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9549</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9549</td><td align="center" valign="middle" rowspan="1" colspan="1">2.1 &#x000d7; 10<sup>&#x02212;4</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9883</td><td align="center" valign="middle" rowspan="1" colspan="1">9.5 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9881</td><td align="center" valign="middle" rowspan="1" colspan="1">8.8 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9877</td><td align="center" valign="middle" rowspan="1" colspan="1">6.4 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9880</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2 &#x000d7; 10<sup>&#x02212;5</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SVD</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9899</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9899</td><td align="center" valign="middle" rowspan="1" colspan="1">9.0 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9898</td><td align="center" valign="middle" rowspan="1" colspan="1">6.9 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9899</td><td align="center" valign="middle" rowspan="1" colspan="1">8.6 &#x000d7; 10<sup>&#x02212;5</sup></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SCDA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9975</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.0 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9952</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.4 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9900</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.2 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9942</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.1 &#x000d7; 10<sup>&#x02212;4</sup></td></tr></tbody></table></table-wrap><table-wrap id="genes-10-00652-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">genes-10-00652-t005_Table 5</object-id><label>Table 5</label><caption><p>Performance comparison on HLA genotypes of the five super populations at three missing scenarios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Methods</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">5%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">10%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">20%</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Total</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9498</td><td align="center" valign="middle" rowspan="1" colspan="1">9.6 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9497</td><td align="center" valign="middle" rowspan="1" colspan="1">5.8 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9498</td><td align="center" valign="middle" rowspan="1" colspan="1">4.5 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9498</td><td align="center" valign="middle" rowspan="1" colspan="1">6.6 &#x000d7; 10<sup>&#x02212;5</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9873</td><td align="center" valign="middle" rowspan="1" colspan="1">5.8 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9871</td><td align="center" valign="middle" rowspan="1" colspan="1">4.3 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9867</td><td align="center" valign="middle" rowspan="1" colspan="1">3.5 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9870</td><td align="center" valign="middle" rowspan="1" colspan="1">4.5 &#x000d7; 10<sup>&#x02212;5</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SVD</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9809</td><td align="center" valign="middle" rowspan="1" colspan="1">9.2 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9809</td><td align="center" valign="middle" rowspan="1" colspan="1">4.4 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9809</td><td align="center" valign="middle" rowspan="1" colspan="1">2.9 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.9809</td><td align="center" valign="middle" rowspan="1" colspan="1">5.5 &#x000d7; 10<sup>&#x02212;5</sup></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SCDA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9973</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.9 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9949</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.5 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9896</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5 &#x000d7; 10<sup>&#x02212;4</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9939</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.4 &#x000d7; 10<sup>&#x02212;4</sup></td></tr></tbody></table></table-wrap></floats-group></article>