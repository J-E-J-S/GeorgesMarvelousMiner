<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-title-group><journal-title>Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1367-4811</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6792064</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btz199</article-id><article-id pub-id-type="publisher-id">btz199</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Papers</subject><subj-group subj-group-type="category-toc-heading"><subject>Systems Biology</subject></subj-group></subj-group></article-categories><title-group><article-title>Simulation-assisted machine learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Deist</surname><given-names>Timo M</given-names></name><xref ref-type="aff" rid="btz199-aff1">1</xref><xref ref-type="aff" rid="btz199-aff2">2</xref><xref ref-type="author-notes" rid="btz199-FM1"/></contrib><contrib contrib-type="author"><name><surname>Patti</surname><given-names>Andrew</given-names></name><xref ref-type="aff" rid="btz199-aff1">1</xref><xref ref-type="author-notes" rid="btz199-FM1"/></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhaoqi</given-names></name><xref ref-type="aff" rid="btz199-aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Krane</surname><given-names>David</given-names></name><xref ref-type="aff" rid="btz199-aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Sorenson</surname><given-names>Taylor</given-names></name><xref ref-type="aff" rid="btz199-aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3093-718X</contrib-id><name><surname>Craft</surname><given-names>David</given-names></name><xref ref-type="aff" rid="btz199-aff1">1</xref><xref ref-type="corresp" rid="btz199-cor1"/><!--<email>dcraft@broadinstitute.org</email>--></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Stegle</surname><given-names>Oliver</given-names></name><role>Associate Editor</role></contrib></contrib-group><aff id="btz199-aff1"><label>1</label>
<institution>Department of Radiation Oncology, Massachusetts General Hospital</institution>, Harvard Medical School, Boston, Massachusetts, USA</aff><aff id="btz199-aff2"><label>2</label>
<institution>The D-Lab: Decision Support for Precision Medicine, GROW - School for Oncology and Developmental Biology</institution>, Maastricht University Medical Centre, Maastricht ER, The Netherlands</aff><author-notes><corresp id="btz199-cor1">To whom correspondence should be addressed. <email>dcraft@broadinstitute.org</email></corresp><fn id="btz199-FM1"><p>The authors wish it to be known that, in their opinion, Timo M. Deist and Andrew Patti authors should be regarded as Joint First Authors.</p></fn></author-notes><pub-date pub-type="ppub"><day>15</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="epub" iso-8601-date="2019-03-23"><day>23</day><month>3</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>3</month><year>2019</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>35</volume><issue>20</issue><fpage>4072</fpage><lpage>4080</lpage><history><date date-type="received"><day>06</day><month>4</month><year>2018</year></date><date date-type="rev-recd"><day>15</day><month>11</month><year>2018</year></date><date date-type="accepted"><day>21</day><month>3</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2019. Published by Oxford University Press.</copyright-statement><copyright-year>2019</copyright-year><license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="btz199.pdf"/><abstract><title>Abstract</title><sec id="s1"><title>Motivation</title><p>In a predictive modeling setting, if sufficient details of the system behavior are known, one can build and use a simulation for making predictions. When sufficient system details are not known, one typically turns to machine learning, which builds a black-box model of the system using a large dataset of input sample features and outputs. We consider a setting which is between these two extremes: some details of the system mechanics are known but not enough for creating simulations that can be used to make high quality predictions. In this context we propose using approximate simulations to build a kernel for use in kernelized machine learning methods, such as support vector machines. The results of multiple simulations (under various uncertainty scenarios) are used to compute similarity measures between every pair of samples: sample pairs are given a high similarity score if they behave similarly under a wide range of simulation parameters. These similarity values, rather than the original high dimensional feature data, are used to build the kernel.</p></sec><sec id="s2"><title>Results</title><p>We demonstrate and explore the simulation-based kernel (SimKern) concept using four synthetic complex systems&#x02014;three biologically inspired models and one network flow optimization model. We show that, when the number of training samples is small compared to the number of features, the SimKern approach dominates over no-prior-knowledge methods. This approach should be applicable in all disciplines where predictive models are sought and informative yet approximate simulations are available.</p></sec><sec id="s3"><title>Availability and implementation</title><p>The Python SimKern software, the demonstration models (in MATLAB, R), and the datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/davidcraft/SimKern">https://github.com/davidcraft/SimKern</ext-link>.</p></sec><sec id="s4"><title>Supplementary information</title><p>
<xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p></sec></abstract><counts><page-count count="9"/></counts></article-meta></front><body><sec><title>1 Introduction and motivation</title><p>There are two general approaches to computationally predicting the behavior of complex systems, simulation and machine learning (ML). Simulation is the preferred method if the dynamics of the system being studied are known in sufficient detail that one can simulate its behavior with high fidelity and map the system behavior to the output to be predicted. ML is valuable when the system defies accurate simulation but enough data exist to train a general black-box machine learner, which could be anything from a linear regression or classification model to a neural network. In this work, we propose a technique to combine simulation and ML in order to leverage the best aspects of both and produce a system that is superior to either technique alone.</p><p>Our motivation is personalized medicine: how do we assign the right drug or drug combination to cancer patients? Across cultures and history, physicians prescribe medicines and interventions based on how the patient is predicted to respond. Currently these choices are made based on established patient-classification protocols, physician judgment, clinical trial eligibility and occasionally limited genomic profiling of the patient. All of these approaches, in one way or another, attempt to partition patients into groups based on some notion of similarity.</p><p>Genomics is especially relevant for computing the similarity between two cancer patients since cancer is associated with alterations to the DNA, which in turn causes the dysregulation of cellular behavior (<xref rid="btz199-B1" ref-type="bibr">Balmain <italic>et al.</italic>, 2003</xref>). Bioinformatic analysis has revealed that there is heterogeneity both within a patient tumor and across tumors; no two tumors are the same genomically (<xref rid="btz199-B10" ref-type="bibr">Felipe De Sousa <italic>et al.</italic>, 2013</xref>; <xref rid="btz199-B12" ref-type="bibr">Fisher <italic>et al.</italic>, 2013</xref>). Although in a small fraction of cases specific genetic conditions are used to guide therapy choices, for example, breast (commonly amplified gene: HER2), melanoma (BRAF mutation), lung (EML4-ALK fusion) and head-and-neck (HPV status for radiation dose de-escalation; see <xref rid="btz199-B20" ref-type="bibr">Mirghani and Blanchard, 2017</xref>), there remains a large variability in patient responses to these and other treatments, likely due to the fact that patients will usually have tens or hundreds of mutations and gene copy number variations, chromosomal structural rearrangements, not to mention a distinct germline genetic state (<xref rid="btz199-B15" ref-type="bibr">Hauser <italic>et al.</italic>, 2018</xref>), human leukocyte antigen (HLA) type (<xref rid="btz199-B6" ref-type="bibr">Chowell <italic>et al.</italic>, 2018</xref>), tumor epigenetic DNA modifications, microbiome, and comorbidity set. Even amidst this heterogeneity, the notion of patient similarity&#x02014;although currently not deeply understood due to the complexities of cancer biology&#x02014;is appealing both conceptually and for its value in the ML setting.</p><p>Simulating a drug is a task that far exceeds our current scientific capacity: it enters the patient, either intravenously or orally, and winds its way to the cancer cells, where it either influences the cancer cell via receptors on the cell membrane or penetrates into the cell and affects signaling pathways, cell metabolism, DNA repair, apoptosis, or some combination of these and other modules. Nevertheless, a vast amount of knowledge of cellular processes, residing in molecular biology textbooks and millions of scientific papers, has been accrued over the past century and it seems worthwhile to attempt to use that information, if unclear how. Most machine learning research efforts in the personalized medicine realm take a pure data approach. Given the complexity of patient biology and cancer, this approach will require vast amounts of high quality patient data that is suitably standardized for algorithmic processing.</p><p>With this drug sensitivity prediction problem as our backdrop, we develop a method to combine approximate simulations with ML and demonstrate using <italic>in silico</italic> experiments that a judicious combination can yield better predictions than either technique alone. The basic idea is a division of labor: coarse and approximate simulations are used to compute similarity measures, and these similarity measures are then used by the ML algorithm to build a predictive model, called SimKern ML (<xref ref-type="fig" rid="btz199-F1">Fig.&#x000a0;1</xref>). At this point in time, although vast details of cellular biology are known, we are not in a position to simulate with any fidelity complete cellular or <italic>in vivo</italic> cancerous processes. However, herein we present demonstrations that one could combine simulation results into machine learning and improve the overall predictive capability, a technique which may play a role in future drug recommendation systems.
</p><fig id="btz199-F1" orientation="portrait" position="float"><label>Fig. 1.</label><caption><p>Workflow comparison of Standard ML and SimKern ML. The feature matrix <italic>X</italic> and outcome data <italic>y</italic> are given (in this paper, we generate such &#x02018;ground truth&#x02019; datasets by simulating complex systems, a step which is not shown in this figure). Traditional feature-based ML is depicted in the upper orange part. SimKern, the simulation-based method, pre-processes the dataset by sending each sample through a number of approximate simulations. Each sample pair is given a similarity score based on how closely they behave under the various simulations (see <xref ref-type="fig" rid="btz199-F2">Fig.&#x000a0;2</xref> and Section 2.2 for more details). This information is stored in a kernel matrix <italic>K</italic>, where <italic>K</italic>(<italic>i</italic>, <italic>j</italic>) measures the similarity between samples <italic>i</italic> and <italic>j</italic>. Note that <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Useful SimKern simulations yield a kernel <italic>K</italic> that improves the downstream machine learning performance</p></caption><graphic xlink:href="btz199f1"/></fig></sec><sec><title>2 Materials and methods</title><p>Our method is centered on kernelized ML. Rather than feature vectors (a list of attributes for each sample), kernelized learning requires only a similarity score between pairs of samples. For training, one needs the outcome of each training sample and a measurement of the similarity between all pairs of training samples. For predicting the outcome of a new sample, one needs to provide the similarity of that sample to each training sample. It is well known in ML that good similarity measures, which come from expert domain knowledge, result in better ML performance (<xref rid="btz199-B24" ref-type="bibr">Sch&#x000f6;lkopf <italic>et al.</italic>, 2004</xref>). We assume that we can formulate a simulation of each sample&#x02019;s behavior based on its known individual characteristics (i.e. features). We also assume that we do not know exactly how to simulate the systems, so rather than a single simulation we have a family (possibly parametrized by real numbers, and thus infinite) of plausible simulations. Two samples are given a high similarity score if they behave similarly across a wide range of simulations.</p><p>We begin with a brief description of the four models we use to demonstrate and analyze the performance of SimKern. By describing these models, the reader has in mind a more concrete context with which to frame the SimKern development.</p><sec><title>2.1 Brief model descriptions</title><p>We investigate four models: radiation impact on cells, flowering time in plants, a Boolean cancer model and a network flow optimization problem. Full details and model implementation notes are given in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>.</p><p>For each model we begin by generating a dataset of <italic>N</italic> samples, each sample <italic>i</italic> is described by a feature vector <italic>x<sub>i</sub></italic> of length <italic>p</italic> and a response <italic>y<sub>i</sub></italic>, using the ground truth simulation (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S1</xref>). This produces an <italic>N&#x02009;</italic>&#x000d7;<italic>&#x02009;p</italic> feature matrix <italic>X</italic> and a response vector <italic>y</italic> of length <italic>N</italic>. This ground truth simulation (referred to as SIM0 in the code repository) is not part of our kernelized learning method, but the datasets created are needed to demonstrate the simulation-based kernel ML method. This ground truth simulation step is further described in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>. In an actual application of SimKern, this artificial data creation step would not be used.</p><p>The <bold>radiation cancer cell death model</bold> is a set of ordinary differential equations (ODEs) which represents a simplified view of the biochemical processes that happen after a cancer cell is hit by radiation. The core of the model involves the DNA damage response regulated by the phosphorylation of ATM and subsequent p53 tetramerization (<xref rid="btz199-B9" ref-type="bibr">Elia&#x00161; <italic>et al.</italic>, 2014</xref>). We have added cell cycle arrest terms, a DNA repair process, and apoptosis modules in order to capture the idea that cellular response to DNA damage involves the combined dynamics of these various processes. The model, which is depicted as a network graph, is displayed in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S3</xref>, and consists of 34 ODEs. The rate parameters were not tuned to realistic values (except for the ones from the original p53 core network, where we used the values provided by the authors; see <xref rid="btz199-B9" ref-type="bibr">Elia&#x00161; <italic>et al.</italic>, 2014</xref>). Instead, values were manually chosen such that the family of samples created had representatives in each of the four output classes: apoptosis, repaired and cycling, mitotic catastrophe, and quiescence. A population of distinct cell types is formed by varying 33 of the ODE rate constants and the mutation status of six genes (ARF, BAX, SIAH, Reprimo, p53 and APAF1), for a feature vector length of 39. The SimKern simulation uses the same underlying model as the original ODE model with two key differences: 87 of the ODE parameters are marked as uncertain and given Gaussian probability distributions around their true values, and the simulation outputs the time dynamics of the ODEs rather than a classification.</p><p>The <bold>flowering time model</bold> is a set of six ODEs that simulate the gene regulatory network governing the flowering of the Arabidopsis plant (<xref rid="btz199-B27" ref-type="bibr">Valentim et&#x000a0;al., 2015</xref>), and yields a regression problem. Nineteen mutants are modeled and experimentally validated by the authors. We use those 19 mutational states as well as 34 additional perturbations on the rate parameters to create a varied ground truth sample set. The output of the model is the time to flowering which, following the authors, is set to the time at which the protein AP1 exceeds a particular threshold. For the SimKern model we assume the same model but with uncertainty about the rate parameters. The SimKern simulation output is the time dynamics of the six ODEs.</p><p>The <bold>Boolean cancer model</bold> is a discrete dynamical system of cancer cellular states (<xref rid="btz199-B7" ref-type="bibr">Cohen <italic>et al.</italic>, 2015</xref>). Based on the steady state of the system, a sample is labelled as one of three categories: apoptotic, metastasizing, or other. There are no rate parameters since this is a Boolean model. We use the initial state vector (the on/off status of the 32 nodes in the network) as well as mutations of five of the genes (p53, AKT1, AKT2, NICD and TGF&#x003b2;) to create a varied sample population with 37 features. In the SimKern simulation, we use a reduced version of the model provided in the original publication. It is unclear how to map the initial conditions from the full model to the initial conditions of the modularly-reduced model, so for all of the modules we randomly choose the mapping, which gives rise to the uncertainty for the SimKern simulations. The output from the SimKern model (i.e. the data used to form the similarity matrix) is the same classification as from the ground truth model.</p><p>The <bold>network flow model</bold> is an optimization problem rather than a simulation. It falls into a subclass of linear optimization models called network flows which are used in a wide range of applications including production scheduling and transportation logistics (<xref rid="btz199-B3" ref-type="bibr">Bertsimas and Tsitsiklis, 1997</xref>). The network flow model takes arc costs as inputs, which are the costs of sending a unit of flow through a certain arc in the network. The model then simulates the optimal path of flow along arcs of a directed graph that minimizes the total arc cost along the path. The network is designed in layers and is such that the flow will pass through exactly one of the three arcs in the final layer, which gives us a classification problem (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S2</xref>). Changes in arc costs, which represent the features in this model, can lead to changes in the routing of the optimal flow. For the ground truth dataset, we generate samples by varying 12 out of the 80 arc costs. We build two separate SimKern simulations: the better simulation perturbs 23 arc costs, including the 12 costs that were varied to make the ground truth dataset, resulting in a less noisy kernel. The worse simulation varies 21 additional arc costs resulting in a noisier kernel.</p></sec><sec><title>2.2 SimKern simulation&#x02014;similarity matrix generation</title><p>Users must define a model (currently supported languages for the simulation modeling are MATLAB, Octave, and R) which simulates a sample. This simulation procedure, called SIM1 in the Python codebase, is used to generate the sample similarity kernel matrix and would be the starting point in an actual application of SimKern. <xref ref-type="fig" rid="btz199-F2">Figure&#x000a0;2</xref> illustrates the SimKern simulation process control.
</p><fig id="btz199-F2" orientation="portrait" position="float"><label>Fig. 2.</label><caption><p>Creation of the similarity matrix for use downstream in the machine learning</p></caption><graphic xlink:href="btz199f2"/></fig><p>We assume that there are parameters in this simulation model that we are uncertain about. Let &#x003b8; be a vector of these uncertain parameters. We assume we have a random variable description of each of these parameters, which can be very general. For example, a parameter could take the value of 0 or 1 if we have two ways of modeling a particular interaction. Then, in the simulation, depending on how that random variable gets instantiated, the code uses one of the two parameter values. Alternatively, we might be uncertain about the value of a rate constant, in which case we could use a Gaussian random variable with a specified mean and standard deviation. We assume independence of the random variables &#x003b8;, but one could also assume a covariance structure.</p><p>Each sample <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is characterized by a feature vector <italic>x<sub>i</sub></italic>, which constitutes sample-specific information that we use to perform the simulations; <italic>x<sub>i</sub></italic> could be for example a genomic description of patient <italic>i</italic>. For <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, where <italic>R</italic> is the number of trials to run, we instantiate a parameter vector, &#x003b8;<sub><italic>r</italic></sub>. These parameters as well as the sample data <italic>x<sub>i</sub></italic> are used to run simulation (<italic>i</italic>, <italic>r</italic>).</p><p>Let <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>&#x003b8;</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (or shorthand, <italic>S<sub>ir</sub></italic>) be the simulation output for sample <italic>i</italic> with uncertainty parameters equal to &#x003b8;<sub>r</sub>. Note that these outputs <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>&#x003b8;</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be scalars, a classification category, vectors, or any other object. There is no need for these outputs to be the same as what we are trying to predict, <italic>y<sub>i</sub></italic>. We simply assume that given two such outputs, say <italic>S<sub>ir</sub></italic> and <italic>S<sub>jr</sub></italic> for samples <italic>i</italic> and <italic>j</italic>, we have a way to measure the similarity between them. Let this similarity be given by <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We leave it up to the user to define this function in general (a concrete procedure, for simulations using ODEs, is given in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>).</p><p>Finally, the similarity <italic>K</italic>(<italic>i</italic>, <italic>j</italic>) between two samples <italic>i</italic> and <italic>j</italic> is the average similarity across the <italic>R</italic> simulation runs
<disp-formula id="E1"><mml:math id="M1"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The above SimKern kernel matrix generation procedure is implemented in Python and is fully described in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>.</p></sec><sec><title>2.3 Machine learning comparisons procedure</title><p>
<xref ref-type="fig" rid="btz199-F1">Figure&#x000a0;1</xref> shows a schematic of the differences in the data processing and machine learning steps for Standard ML and SimKern ML. We compare standard feature-based ML algorithms [orange/top: linear support vector machine (SVM), radial basis function (RBF) SVM and random forest (RF)] with simulation kernel based methods (green/bottom: kernelized SVM and kernelized RF). We also include results for one-nearest neighbor (NN) and kernelized one-nearest neighbor (SimKern NN). As NN-type algorithms are arguably the simplest non-trivial ML algorithms, including these algorithms allows us to understand the distinct contributions of ML algorithm sophistication and simulation-based kernels.</p><p>Since we can generate as many samples as we wish, we train the models and tune the hyperparameters on training and validation datasets which are distinct from the final testing set on which we compute prediction performance metrics (see Section 2.4). The ground truth simulation generates one dataset which is split into three parts (train/validation/test) using the standard proportions 50%/25%/25% (<xref rid="btz199-B14" ref-type="bibr">Hastie <italic>et al.</italic>, 2009</xref>, p. 222). SVM (<xref rid="btz199-B2" ref-type="bibr">Ben-Hur and Weston, 2010</xref>) and NN algorithms are dependent on feature scaling, therefore features are standardized to the interval <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> by subtracting the minimum value and scaling by the range. Categorical features are dummy-coded for SVM and NN algorithms. Each ML algorithm is trained on the training data for many hyperparameter configurations and the configuration with the best fit on the validation data is selected. The model given the selected configuration is applied on the test set to compute the performance metrics. See Alg. 1 in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> for the details of training, hyperparameter tuning, and testing procedures.</p><p>To investigate the performance of simulation-based kernels in scenarios with less data for training, we consider five scenarios in which we train the algorithms on subsamples comprising <italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>,&#x02026;, <italic>s</italic><sub>5</sub> of the training data. The subsampling percentages are chosen differently per model to highlight the interesting regions of curves that display the performance versus training set size. <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S3</xref> reports the subsampling percentages per model.</p></sec><sec><title>2.4 Performance metrics</title><p>For each of the simulation models, we estimate the generalization performance of an ML algorithm in test data, i.e. data unused for model training, as performance estimates on training data are of little practical value (<xref rid="btz199-B14" ref-type="bibr">Hastie <italic>et al.</italic>, 2009</xref>, p. 230). The learning tasks per model are either classification or regression. For classification, we consider prediction accuracy, which is defined as
<disp-formula id="E2"><mml:math id="M2"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>true</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>classification</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>count</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>number</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>of</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>samples</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where TP, TN, FP and FN are the counts of true positives, true negatives, false positives, and false negatives, respectively. For regression, we consider the coefficient of determination <italic>R</italic><sup>2</sup>, which is defined as
<disp-formula id="E3"><mml:math id="M3"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mtext>sum</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>of</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>squared</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>prediction</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>error</mml:mtext></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>of</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mtext>squares</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>y<sub>i</sub></italic> is the outcome for sample <italic>i</italic>, <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted outcome for sample <italic>i</italic> and <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the sample mean of the outcome. To attain a reliable estimate of the generalization performance, we consider the average test data performance in ten repetitions of a train/validation/test analysis, i.e. repeating training and hyperparameter tuning each time.</p></sec><sec><title>2.5 Standard ML versus SimKern ML comparison</title><p>For each model, we produce a box plot and/or a line plot that show algorithm performance versus training dataset size for the various ML algorithms in both algorithm groups, Standard ML and SimKern ML.
<list list-type="roman-lower"><list-item><p>Box plots display results for each algorithm separately for the Standard ML (linear SVM, RBF SVM, RF, NN) and SimKern ML algorithms (SimKern SVM, SimKern RF, SimKern NN). The horizontal lines indicate the sample median, the boxes are placed between the first and third quartile (<italic>q</italic><sub>1</sub>, <italic>q</italic><sub>3</sub>). Outliers are defined as samples outside <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mn>1.5</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1.5</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and are indicated by crosses.</p></list-item><list-item><p>Line plots further condense the findings by displaying the median performance metric of the best performing Standard ML and SimKern ML algorithms, excluding NN algorithms in both cases. The best performing algorithm is defined as the algorithm that most frequently produces the highest median performance metric over all five training dataset subsamples. Lines are interpolated for visual guidance.</p></list-item></list></p></sec><sec><title>2.6 Sensitivity analysis</title><p>To investigate possible factors affecting the SimKern algorithms&#x02019; prediction performance, we run the following sensitivity analyses:</p><p>
<bold>Varying prior knowledge</bold>
<list list-type="roman-lower"><list-item><p>Radiation model: we examine the results for two kernels which represent different levels of prior knowledge. Both cases utilize the same SimKern simulation, but the higher quality kernel uses the dynamics of only the compartments of the ODE set that are used in the classification of the samples in the initial ground truth simulation. The lower quality kernel uses all ODE equations, therefore not emphasizing the most important ones (<xref rid="btz199-B11" ref-type="bibr">Ferranti <italic>et al.</italic>, 2017</xref>).</p></list-item></list>
</p><p>
<bold>Varying simulation parameter noise/bias</bold>
<list list-type="simple"><list-item><p>ii. Network flow model: we generate two kernels for the network flow model. These kernels differ in the number of arc costs that are perturbed and the size of the perturbations (full details in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>).</p></list-item><list-item><p>iii. Flowering time model: along with the model that generates the baseline kernel, we study one less noisy, one noisier, and one biased version of the SimKern simulation. The baseline SimKern simulation uses multiplicative Gaussian noise on 34 of the rate parameters, using a mean of 1 and a standard deviation of 0.2. The less noisy model uses stdev=0.1 and the noisier model uses stdev=0.4. For a more radical, and non-centered, departure from the true rate parameters, we also run a model where we multiply each of the same 34 rate parameters with a random variable chosen uniformly from the discrete set <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mo>{</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>10</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item></list>
</p><p>
<bold>Varying the number of simulation trials, <italic>R</italic></bold>
<list list-type="simple"><list-item><p>iv. Network flow model: we analyze the effect of additional simulation trials on the prediction performance. We compare the prediction performance of SimKern algorithms when using a similarity kernel based on <italic>R</italic> = 3 simulation trials to the final kernel based on <italic>R</italic> = 10 trials. Furthermore, we track the convergence of the kernel matrix over <italic>R</italic> = 10 trials.</p></list-item></list>
</p></sec></sec><sec><title>3 Results</title><p>The general theme that emerges is that, for small training dataset sizes, the methods using the SimKern kernel outperform the Standard ML methods. For larger training sizes, however, the standard methods either approach the SimKern methods or exceed them, depending on the quality of the kernel.</p><p>For the radiation model, we see exactly this general pattern (<xref ref-type="fig" rid="btz199-F3">Fig.&#x000a0;3</xref>). For small training sizes (up to 50 samples), the SVM with the SimKern kernel dominates. We can attribute much of the performance gain to the similarity kernel itself given that the NN algorithm using the same similarity kernel also dominates over the no-prior-knowledge methods for all training sizes shown. The increase in accuracy by the Standard ML algorithms does not yet show signs of saturation by 500 training samples. These box plots are summarized by line plots in <xref ref-type="fig" rid="btz199-F4">Fig.&#x000a0;4</xref> (left), which also displays the results of the lower quality SimKern kernel, which was made with the same simulations but without focusing on the most relevant ODEs for the kernel matrix computation.
</p><fig id="btz199-F3" orientation="portrait" position="float"><label>Fig. 3.</label><caption><p>Machine learning results for the radiation cancer model </p></caption><graphic xlink:href="btz199f3"/></fig><fig id="btz199-F4" orientation="portrait" position="float"><label>Fig. 4.</label><caption><p>Varying prior knowledge experiments for the radiation model (left) and varying parameter noise experiments for the network flow model (right). Performance metrics of SimKern ML based on simulations with less and more prior knowledge (green) and Standard ML (orange). For each line, the best performing algorithm of SimKern ML or Standard ML is selected (see Section 2.5). Note, the waviness of the less noise case for the network flow model is an artifact of how the data from the box plots was converted into a line plot; the full data, <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S7</xref>, reveals a flat relationship</p></caption><graphic xlink:href="btz199f4"/></fig><p>The results of the flowering time model, which also display the clear dominance of SimKern learning for small training dataset sizes, show a trend of decreasing variance in predictive performance with increasing training sizes (<xref ref-type="fig" rid="btz199-F5">Fig.&#x000a0;5</xref>). SimKern learning is strongly dominant up to 75 training samples, after which the two learning styles converge to <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02248;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Another view of the improvement offered by the SimKern method for small training size set sizes is shown by plotting the predicted flowering times versus the actual flowering times, <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S9</xref>.
</p><fig id="btz199-F5" orientation="portrait" position="float"><label>Fig. 5.</label><caption><p>Machine learning results for the flowering time model </p></caption><graphic xlink:href="btz199f5"/></fig><p>The sensitivity results obtained by increasing the variance of the (centered) Gaussian noise that was applied to the flowering model&#x02019;s rate parameters display a robustness to these deviations (<xref ref-type="fig" rid="btz199-F6">Fig.&#x000a0;6</xref>, upper green curves and Gaussian box plots). However, the non-centered noise perturbation analysis shows a clear drop in ML accuracy (<xref ref-type="fig" rid="btz199-F6">Fig.&#x000a0;6</xref>, dark green dotted line and dark green box plot). With enough training data all SimKern kernels, including the ones with heavy noise, achieve an <italic>R</italic><sup>2</sup> above 0.95. We call such kernels <italic>sufficient</italic>.
</p><fig id="btz199-F6" orientation="portrait" position="float"><label>Fig. 6.</label><caption><p>Varying simulation parameter noise/bias experiments for the flowering time model. SimKern ML based on simulations with varying parameter noise (green), with parameter bias (dark green), and Standard ML (orange). Left: performance metrics of SimKern ML (green) and Standard ML (orange) trained on up to 150 samples. For each line, the best performing algorithm of SimKern ML or Standard ML is selected (see Section 2.5). Right: performance metrics box plots for the 25 training sample case</p></caption><graphic xlink:href="btz199f6"/></fig><p>In contrast, the Boolean cancer model kernel is based on a model reduction with additional uncertainty and produces what we call a <italic>biased</italic> kernel. There, the SimKern approach produces an accuracy that initially dominates but quickly plateaus to around 85% and is overtaken by no-prior-knowledge methods when more training data is available (<xref ref-type="fig" rid="btz199-F7">Fig.&#x000a0;7</xref>). The fact that the kernel learning barely improves with additional data implies that the feature space induced by the simulation kernel is simple enough to be learned by a small amount of samples (<xref rid="btz199-B4" ref-type="bibr">Bottou <italic>et al.</italic>, 1994</xref>). The kernelized NN method gets worse with more samples, and in general is worse than the other SimKern algorithms, which indicates that the space induced by the biased kernel is less cleanly separable compared to the flowering model case. Above 100 training samples, the no-prior-knowledge RF method is the superior technique.
</p><fig id="btz199-F7" orientation="portrait" position="float"><label>Fig. 7.</label><caption><p>Machine learning results for the Boolean cancer model</p></caption><graphic xlink:href="btz199f7"/></fig><p>For the network flow problem, we evaluate two separate kernels (<xref ref-type="fig" rid="btz199-F4">Fig.&#x000a0;4</xref>, right) based on different levels of noise in the SimKern simulation: the kernel based on a less noisy SimKern simulation dominates throughout, but even the kernel based on a noisier SimKern simulation is still useful in the very small training set size range. It is doubtful whether one can make general statements about how good a simulation needs to be in order to yield a useful kernel. However, the intuition that the simulations need only discover the similarity of samples, while not necessarily providing accurate (hence directly useful) simulation results, is described in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S8</xref>.</p><p>When comparing the individual Standard ML algorithms to the SimKern ML algorithms based on the noisier SimKern simulation (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S7</xref>), Standard RF eventually dominates. When comparing algorithms within the Standard ML group, RF is the dominant Standard ML algorithm for the network flow model (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S7</xref>) as well as for the Boolean cancer model (<xref ref-type="fig" rid="btz199-F7">Fig.&#x000a0;7</xref>). For these models, the dominance of RF is likely related to the discrete characteristics of the underlying models.</p><p>The quality of a simulation-generated kernel also depends on the number of trials <italic>R</italic> that are used to compute the kernel. <xref ref-type="fig" rid="btz199-F8">Figure&#x000a0;8</xref> displays both the convergence of the kernel (bottom) and the improved learning accuracy from the further converged kernel (top), for the less noisy network flow case. We see that the earliest kernel written, kernel three (we chose to not determine similarity kernels below <italic>R&#x02009;</italic>=<italic>&#x02009;</italic>3), performs noticeably worse than the final kernel. We can also visually observe the differences in the kernels by plotting the 500&#x02009;&#x000d7;&#x02009;500 kernels (<xref ref-type="fig" rid="btz199-F8">Fig.&#x000a0;8</xref>, bottom left and right). The kernel convergence plot is obtained by taking the Frobenius norms of the difference of the kernel matrices of iteration <italic>i</italic> &#x02013; 1 and <italic>i</italic>, until <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.
</p><fig id="btz199-F8" orientation="portrait" position="float"><label>Fig. 8.</label><caption><p>Varying simulation trials experiments for the network flow model. The upper two box plots compare the learning accuracy for a kernel from the third of <italic>R&#x02009;</italic>=<italic>&#x02009;</italic>10 trials versus the final kernel. The kernels themselves are displayed with the same color scale below, and centered at bottom displays the convergence of the kernel (measured using the Frobenius matrix norm) over the ten trials</p></caption><graphic xlink:href="btz199f8"/></fig></sec><sec><title>4 Discussion</title><p>We introduce simulation as a pre-processing step in a machine learning pipeline, in particular, as a way to include expert prior knowledge. One can consider simulation as a technique which regularizes data or as a specialized feature extraction method. In either view, the SimKern methodology offers a decomposition of an overall ML task into two steps: similarity computation followed by predictive modeling using the pairwise similarities. This decomposition highlights that to improve the performance of an ML model one can direct efforts into determining better similarity scores between all samples. This is in contrast to the more commonly heard call for &#x02018;more data&#x02019; to achieve better ML results. Of course, more samples are always desirable, but here we show that, particularly in limited data settings, sizable performance gains can come from high quality similarity scores.</p><p>The decomposition of simulation and machine learning steps also points out their individual contributions. The simulation-based kernel structures the space in which the samples live (or more technically, the dual of the space; see <xref rid="btz199-B18" ref-type="bibr">Kung, 2014</xref>), and ML finds the patterns in this simplified space. We see that in order to improve machine learning performance we can either improve the kernel or increase the number of samples to better populate the space. For the cases shown here, custom similarity measures show large improvements especially in limited data settings (up to a 20% increase in classification accuracy and a 2.5 fold increase in <italic>R</italic><sup>2</sup>, depending on the case and the amount of training data used). One could also use the output of the simulations as features for machine learning rather than the additional kernelization step that we employed. Using the simulation outputs directly is related to the field of model output statistics from weather forecasting, where low level data from primary simulations are used as inputs to a multiple regression model which outputs human-friendly weather predictions (<xref rid="btz199-B13" ref-type="bibr">Glahn and Lowry, 1972</xref>). In our case, we opted for kernelizing the simulation outputs to highlight the fundamental concept of similarity and because a similarity computation is natural when the output of the simulations is a set of time varying entities, e.g. in the case of ODEs.</p><p>Similar in spirit to SimKern, although differing in details, combining simulation and machine learning has been used in physics to predict object behaviour (<xref rid="btz199-B19" ref-type="bibr">Lerer <italic>et al.</italic>, 2016</xref>; <xref rid="btz199-B28" ref-type="bibr">Wu <italic>et al.</italic>, 2015</xref>). Simulation results are used to train networks to &#x02018;learn&#x02019; the physics. Varying the simulation conditions during training, called <italic>domain randomization</italic>, is used to improve model generalization (<xref rid="btz199-B26" ref-type="bibr">Tobin <italic>et al.</italic>, 2017</xref>). Inversely to the SimKern approach to exploit simulation to enhance ML algorithms, machine learning is also used to correct the inputs to physics simulations (<xref rid="btz199-B8" ref-type="bibr">Duraisamy <italic>et al.</italic>, 2015</xref>), an idea which is also pursued in the context of traffic prediction (<xref rid="btz199-B23" ref-type="bibr">Othman and Tan, 2018</xref>).</p><p>A novel potential application of the SimKern methodology, one that the authors are currently investigating, involves the prediction of peptides (chains of approximately nine amino acids) binding to a given HLA class 1 allele. Current technologies (e.g. <xref rid="btz199-B22" ref-type="bibr">Nielsen <italic>et al.</italic>, 2007</xref>) predict if a given peptide will bind to a given HLA allele using properties of the amino acids but without using 3D details of the chemical structure of the peptide or information on the structural binding of the peptide and HLA molecule. Computational predictions of binding are considered too difficult at the present time due to the sensitivity of the structural conformations to the detailed chemistry of peptides and the non-covalent interactions (<xref rid="btz199-B16" ref-type="bibr">Kar <italic>et al.</italic>, 2018</xref>). Nevertheless, simulations could be used to generate similarity scores between peptides, and then the supervised binding data can be used to train a kernelized classification algorithm.</p><p>Finally, the use of a SimKern kernel need not be an all-or-nothing decision, since two or more kernels can be combined to yield a single kernel. This allows one to explore the combination of &#x02018;standard&#x02019; kernelized learning (using uninformed kernels such as linear or RBF) with a SimKern kernel. In the case of a weighted linear sum as the method of kernel combining, one can optimize the weighting vector as part of the training procedure (<xref rid="btz199-B24" ref-type="bibr">Sch&#x000f6;lkopf <italic>et al.</italic>, 2004</xref>). Combining kernels allows one to mix traditional feature-based machine learning (which we called Standard ML above) with prior knowledge similarity matrix-based learning.</p></sec><sec><title>5 Conclusions</title><p>It remains to be seen which approaches will be the most fruitful as we make our way towards personalized cancer medicine. Direct testing of chemotherapeutic agents on biopsied patient tissues is a straightforward and promising &#x02018;hardware-based&#x02019; approach (<xref rid="btz199-B21" ref-type="bibr">Montero <italic>et al.</italic>, 2015</xref>). In the machine learning realm, expert feature selection may turn out to be more feasible than the simulation-based kernel methods described in this report. A key question is: can we make simulation-based kernels that&#x02014;although almost certainly biased&#x02014;will still be useful (see, e.g. <xref ref-type="fig" rid="btz199-F4">Fig.&#x000a0;4</xref>)? Progress in detailed biological simulation, such as the full simulation of the cell cycle of the bacterium <italic>Mycoplasma genitalium</italic> (<xref rid="btz199-B17" ref-type="bibr">Karr <italic>et al.</italic>, 2012</xref>), the OpenWorm project (<xref rid="btz199-B25" ref-type="bibr">Szigeti <italic>et al.</italic>, 2014</xref>), and integrated cancer signaling pathways for predicting proliferation and cell death (<xref rid="btz199-B5" ref-type="bibr">Bouhaddou <italic>et al.</italic>, 2017</xref>) offer some encouragement, but cancer influences human biology at all levels, from minute phosphorylations to immune system rewiring. It is thus by no means clear if we are close to simulations that can be useful in this context. However, the magnitude of the problem&#x02014;both in economic terms and for the number of future patients at stake&#x02014;suggests pressing forward on all fronts that display conceptual promise.</p><p>
<italic>Conflict of Interest</italic>: none declared.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="sup1"><label>btz199_Supplementary_Data</label><media xlink:href="btz199_supplementary_data.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ref-list id="REF1"><title>References</title><ref id="btz199-B1"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Balmain</surname><given-names>A.</given-names></name></person-group>
<etal>et al</etal> (<year>2003</year>) 
<article-title>The genetics and genomics of cancer</article-title>. <source>Nat. Genet</source>., <volume>33</volume>, <fpage>238.</fpage><pub-id pub-id-type="pmid">12610533</pub-id></mixed-citation></ref><ref id="btz199-B2"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ben-Hur</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Weston</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>) 
<article-title>A user&#x02019;s guide to support vector machines</article-title>. <source>Data Mining Techniques for the Life Sciences</source>, <fpage>223</fpage>&#x02013;<lpage>239</lpage>.</mixed-citation></ref><ref id="btz199-B3"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Bertsimas</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Tsitsiklis</surname><given-names>J.</given-names></name></person-group> (<year>1997</year>) <source>Introduction to Linear Optimization</source>. 
<publisher-name>Athena Scientific</publisher-name>.</mixed-citation></ref><ref id="btz199-B4"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Bottou</surname><given-names>L.</given-names></name></person-group>
<etal>et al</etal> (<year>1994</year>) On the effective vc dimension. <italic>Technical Report</italic>, bottou-effvc.ps.Z, Neuroprose.</mixed-citation></ref><ref id="btz199-B5"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Bouhaddou</surname><given-names>M.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>An integrated mechanistic model of pan-cancer driver pathways predicts stochastic proliferation and death</article-title>. <source>BioRxiv</source>, <fpage>128801</fpage>.</mixed-citation></ref><ref id="btz199-B6"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Chowell</surname><given-names>D.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>Patient hla class i genotype influences cancer response to checkpoint blockade immunotherapy</article-title>. <source>Science</source>, <volume>359</volume>, <fpage>582</fpage>&#x02013;<lpage>587</lpage>.<pub-id pub-id-type="pmid">29217585</pub-id></mixed-citation></ref><ref id="btz199-B7"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Cohen</surname><given-names>D.P.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Mathematical modelling of molecular pathways enabling tumour cell invasion and migration</article-title>. <source>PLoS Comput. Biol</source>., <volume>11</volume>, <fpage>e1004571.</fpage><pub-id pub-id-type="pmid">26528548</pub-id></mixed-citation></ref><ref id="btz199-B8"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Duraisamy</surname><given-names>K.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) <chapter-title>New approaches in turbulence and transition modeling using data-driven techniques.</chapter-title> In: <source>53rd AIAA Aerospace Sciences Meeting</source>, p. <fpage>1284</fpage>.</mixed-citation></ref><ref id="btz199-B9"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Elia&#x00161;</surname><given-names>J.</given-names></name></person-group>
<etal>et al</etal> (<year>2014</year>) 
<article-title>The p53 protein and its molecular network: modelling a missing link between dna damage and cell fate</article-title>. <source>Biochim. Biophys. Acta</source>, <volume>1844</volume>, <fpage>232</fpage>&#x02013;<lpage>247</lpage>.<pub-id pub-id-type="pmid">24113167</pub-id></mixed-citation></ref><ref id="btz199-B10"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Felipe De Sousa</surname><given-names>E.M.</given-names></name></person-group>
<etal>et al</etal> (<year>2013</year>) 
<article-title>Cancer heterogeneity-a multifaceted view</article-title>. <source>EMBO Rep</source>., <volume>14</volume>, <fpage>686</fpage>&#x02013;<lpage>695</lpage>.<pub-id pub-id-type="pmid">23846313</pub-id></mixed-citation></ref><ref id="btz199-B11"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ferranti</surname><given-names>D.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) 
<article-title>The value of prior knowledge in machine learning of complex network systems</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>3610</fpage>&#x02013;<lpage>3618</lpage>.<pub-id pub-id-type="pmid">29036404</pub-id></mixed-citation></ref><ref id="btz199-B12"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Fisher</surname><given-names>R.</given-names></name></person-group>
<etal>et al</etal> (<year>2013</year>) 
<article-title>Cancer heterogeneity: implications for targeted therapeutics</article-title>. <source>Br. J. Cancer</source>, <volume>108</volume>, <fpage>479</fpage>&#x02013;<lpage>485</lpage>.<pub-id pub-id-type="pmid">23299535</pub-id></mixed-citation></ref><ref id="btz199-B13"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Glahn</surname><given-names>H.R.</given-names></name>, <name name-style="western"><surname>Lowry</surname><given-names>D.A.</given-names></name></person-group> (<year>1972</year>) 
<article-title>The use of model output statistics (MOS) in objective weather forecasting</article-title>. <source>J. Appl. Meteorol</source>., <volume>11</volume>, <fpage>1203</fpage>&#x02013;<lpage>1211</lpage>.</mixed-citation></ref><ref id="btz199-B14"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T.</given-names></name></person-group>
<etal>et al</etal> (<year>2009</year>) <source>The Elements of Statistical Learning</source>, Vol. <volume>2</volume>
<publisher-name>Springer-Verlag</publisher-name>, 
<publisher-loc>New York</publisher-loc>.</mixed-citation></ref><ref id="btz199-B15"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hauser</surname><given-names>A.S.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>Pharmacogenomics of gpcr drug targets</article-title>. <source>Cell</source>, <volume>172</volume>, <fpage>41</fpage>&#x02013;<lpage>54</lpage>.<pub-id pub-id-type="pmid">29249361</pub-id></mixed-citation></ref><ref id="btz199-B16"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Kar</surname><given-names>P.</given-names></name></person-group>
<etal>et al</etal> (<year>2018</year>) 
<article-title>Current methods for the prediction of t-cell epitopes</article-title>. <source>Peptide Sci</source>., <volume>110</volume>, <fpage>e24046.</fpage></mixed-citation></ref><ref id="btz199-B17"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Karr</surname><given-names>J.R.</given-names></name></person-group>
<etal>et al</etal> (<year>2012</year>) 
<article-title>A whole-cell computational model predicts phenotype from genotype</article-title>. <source>Cell</source>, <volume>150</volume>, <fpage>389</fpage>&#x02013;<lpage>401</lpage>.<pub-id pub-id-type="pmid">22817898</pub-id></mixed-citation></ref><ref id="btz199-B18"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Kung</surname><given-names>S.Y.</given-names></name></person-group> (<year>2014</year>) <source>Kernel Methods and Machine Learning</source>. 
<publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref><ref id="btz199-B19"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Lerer</surname><given-names>A.</given-names></name></person-group>
<etal>et al</etal> (<year>2016</year>) Learning physical intuition of block towers by example. <italic>arXiv Preprint arXiv: 1603.01312</italic>.</mixed-citation></ref><ref id="btz199-B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Mirghani</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Blanchard</surname><given-names>P.</given-names></name></person-group> (<year>2017</year>) 
<article-title>Treatment de-escalation for HPV-driven oropharyngeal cancer: where do we stand?</article-title><source>Clin. Transl. Radiat. Oncol</source>., <volume>8</volume>, <fpage>4</fpage>&#x02013;<lpage>11</lpage>.<pub-id pub-id-type="pmid">29594236</pub-id></mixed-citation></ref><ref id="btz199-B21"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Montero</surname><given-names>J.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>Drug-induced death signaling strategy rapidly predicts cancer response to chemotherapy</article-title>. <source>Cell</source>, <volume>160</volume>, <fpage>977</fpage>&#x02013;<lpage>989</lpage>.<pub-id pub-id-type="pmid">25723171</pub-id></mixed-citation></ref><ref id="btz199-B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Nielsen</surname><given-names>M.</given-names></name></person-group>
<etal>et al</etal> (<year>2007</year>) 
<article-title>Netmhcpan, a method for quantitative predictions of peptide binding to any hla-a and-b locus protein of known sequence</article-title>. <source>PLoS One</source>, <volume>2</volume>, <fpage>e796.</fpage><pub-id pub-id-type="pmid">17726526</pub-id></mixed-citation></ref><ref id="btz199-B23"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Othman</surname><given-names>M.S.B.</given-names></name>, <name name-style="western"><surname>Tan</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>) <chapter-title>Predictive simulation of public transportation using deep learning</chapter-title> In: <source>Asian Simulation Conference</source>. 
<publisher-name>Springer</publisher-name>, pp. <fpage>96</fpage>&#x02013;<lpage>106</lpage>.</mixed-citation></ref><ref id="btz199-B24"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Sch&#x000f6;lkopf</surname><given-names>B.</given-names></name></person-group>
<etal>et al</etal> (<year>2004</year>) <source>Kernel Methods in Computational Biology</source>. 
<publisher-name>MIT press</publisher-name>.</mixed-citation></ref><ref id="btz199-B25"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Szigeti</surname><given-names>B.</given-names></name></person-group>
<etal>et al</etal> (<year>2014</year>) 
<article-title>Openworm: an open-science approach to modeling caenorhabditis elegans</article-title>. <source>Front. Comput. Neurosci</source>., <volume>8</volume>, <fpage>137</fpage>.<pub-id pub-id-type="pmid">25404913</pub-id></mixed-citation></ref><ref id="btz199-B26"><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Tobin</surname><given-names>J.</given-names></name></person-group>
<etal>et al</etal> (<year>2017</year>) <chapter-title>Domain randomization for transferring deep neural networks from simulation to the real world</chapter-title> In <source>Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on</source>, 
<publisher-name>IEEE</publisher-name>, pp. <fpage>23</fpage>&#x02013;<lpage>30</lpage>.</mixed-citation></ref><ref id="btz199-B27"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Valentim</surname><given-names>F.L.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) 
<article-title>A quantitative and dynamic model of the arabidopsis flowering time gene regulatory network</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0116973.</fpage><pub-id pub-id-type="pmid">25719734</pub-id></mixed-citation></ref><ref id="btz199-B28"><mixed-citation publication-type="other">
<person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name></person-group>
<etal>et al</etal> (<year>2015</year>) Galileo: perceiving physical object properties by integrating a physics engine with deep learning. In: <italic>Proceedings of the First 12 Conferences</italic>, <italic>Advances in Neural Information Processing Systems</italic>, pp. <fpage>127</fpage>&#x02013;<lpage>135</lpage>.</mixed-citation></ref></ref-list></back></article>