<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id><journal-title-group><journal-title>BMC Bioinformatics</journal-title></journal-title-group><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6642501</article-id><article-id pub-id-type="publisher-id">2957</article-id><article-id pub-id-type="doi">10.1186/s12859-019-2957-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Methodology Article</subject></subj-group></article-categories><title-group><article-title>Visualizing complex feature interactions and feature sharing in genomic deep neural networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Ge</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Zeng</surname><given-names>Haoyang</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1709-4034</contrib-id><name><surname>Gifford</surname><given-names>David K.</given-names></name><address><email>gifford@mit.edu</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution>Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution></institution-wrap>Cambridge, Massachusetts, USA </aff></contrib-group><pub-date pub-type="epub"><day>19</day><month>7</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>19</day><month>7</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>20</volume><elocation-id>401</elocation-id><history><date date-type="received"><day>14</day><month>11</month><year>2018</year></date><date date-type="accepted"><day>18</day><month>6</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2019</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p>Visualization tools for deep learning models typically focus on discovering key input features without considering how such low level features are combined in intermediate layers to make decisions. Moreover, many of these methods examine a network&#x02019;s response to specific input examples that may be insufficient to reveal the complexity of model decision making.</p></sec><sec><title>Results</title><p>We present DeepResolve, an analysis framework for deep convolutional models of genome function that visualizes how input features contribute individually and combinatorially to network decisions. Unlike other methods, DeepResolve does not depend upon the analysis of a predefined set of inputs. Rather, it uses gradient ascent to stochastically explore intermediate feature maps to 1) discover important features, 2) visualize their contribution and interaction patterns, and 3) analyze feature sharing across tasks that suggests shared biological mechanism. We demonstrate the visualization of decision making using our proposed method on deep neural networks trained on both experimental and synthetic data. DeepResolve is competitive with existing visualization tools in discovering key sequence features, and identifies certain negative features and non-additive feature interactions that are not easily observed with existing tools. It also recovers similarities between poorly correlated classes which are not observed by traditional methods. DeepResolve reveals that DeepSEA&#x02019;s learned decision structure is shared across genome annotations including histone marks, DNase hypersensitivity, and transcription factor binding. We identify groups of TFs that suggest known shared biological mechanism, and recover correlation between DNA hypersensitivities and TF/Chromatin marks.</p></sec><sec><title>Conclusions</title><p>DeepResolve is capable of visualizing complex feature contribution patterns and feature interactions that contribute to decision making in genomic deep convolutional networks. It also recovers feature sharing and class similarities which suggest interesting biological mechanisms. DeepResolve is compatible with existing visualization tools and provides complementary insights.</p></sec><sec><title>Electronic supplementary material</title><p>The online version of this article (10.1186/s12859-019-2957-4) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Visualization</kwd><kwd>Deep neural networks</kwd><kwd>Combinatorial interactions</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U01HG007037</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01CA218094</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p>Deep learning has proven to be powerful on a wide range of tasks in computer vision and natural language processing [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref>]. Recently, several applications of deep learning in genomic data have shown state of art performance across a variety of prediction tasks, such as transcription factor (TF) binding prediction [<xref ref-type="bibr" rid="CR6">6</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref>], DNA methylation prediction [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>], chromatin accessibility [<xref ref-type="bibr" rid="CR12">12</xref>], cell type-specific epigenetic[<xref ref-type="bibr" rid="CR13">13</xref>], and enhancer-promoter interaction prediction [<xref ref-type="bibr" rid="CR14">14</xref>] However, the composition of non-linear elements in deep neural networks makes interpreting these models difficult [<xref ref-type="bibr" rid="CR15">15</xref>], and thus limits model derived biological insight.</p><p>There have been several attempts to interpret deep networks trained on genomic sequence data. One approach scores every possible single point mutation of the input sequence [<xref ref-type="bibr" rid="CR6">6</xref>]. Similarly, DeepSEA analyzed the effects of base substitutions on chromatin feature predictions [<xref ref-type="bibr" rid="CR8">8</xref>]. These &#x02018;in silico saturated mutagenesis&#x02019; approaches reveal individual base contributions, but fail to identify higher order base interactions as they experience a combinatorial explosion of possibilities as the number of mutations increases.</p><p>The second class of efforts to visualize neural networks uses internal model metrics such as gradients or activation levels to reveal key input features that drive network decisions. Zeiler et al. used a de-convolutional structure to visualize features that activate certain convolutional neurons [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. Simonyan et al. proposed saliency maps which use the input space gradient to visualize the importance of pixels to annotate a given input [<xref ref-type="bibr" rid="CR18">18</xref>]. Simonyan&#x02019;s gradient based method inspired variants, such as guided back-propagation [<xref ref-type="bibr" rid="CR19">19</xref>] which only considers gradients that have positive error signal, or simply multiplying the gradient with the input signal. Bach et al. [<xref ref-type="bibr" rid="CR20">20</xref>] proposed layer-wise relevance propagation to visualize the relevance of the pixels to the output of the network. Shrikumar et al. [<xref ref-type="bibr" rid="CR21">21</xref>] proposed DeepLIFT which scores the importance of each pixel, by defining a &#x02018;gradient&#x02019; that compares the activations to a reference sequence, which can resolve the saturation problem in certain types of non-linear neuron paths. LIME [<xref ref-type="bibr" rid="CR22">22</xref>] creates a linear approximation that mimics a model on a small local neighborhood of a given input. Other input-dependent visualization methods include using Shapley values [<xref ref-type="bibr" rid="CR23">23</xref>], integrated gradients [<xref ref-type="bibr" rid="CR24">24</xref>], or maximum entropy [<xref ref-type="bibr" rid="CR25">25</xref>]. While these methods can be fine-grained, they have the limitation of being only locally faithful to the model because they are based upon the selection of an input. The non-linearity and complex combinatorial logic in a neural network may limit network interpretation from a single input. In order to extract generalized class knowledge, unbiased selection of input samples and non-trivial post-processing steps are needed to get a better overall understanding of a class. Moreover these methods have the tendency to highlight existing patterns in the input due to the nature of their design, while the network could also make decisions based on patterns that are absent.</p><p>Another class of methods for interpreting networks directly synthesize novel inputs that maximize the network activation, without using reference inputs. For example, Simonyan et al. [<xref ref-type="bibr" rid="CR18">18</xref>] uses gradient ascent on input space to maximize the predicted score of a class, and DeepMotif [<xref ref-type="bibr" rid="CR26">26</xref>] is an implementation of this method on genomic data. These gradient ascent methods explore the input space with less bias. However their main focus is generating specific input patterns that represent a class without interpreting the reasoning process behind these patterns. Moreover when applied to computer vision networks the images they generate are usually unnatural [<xref ref-type="bibr" rid="CR27">27</xref>]. Thus gradient methods are typically less informative than input-dependent methods for visual analysis. The unnaturalness of gradient images can be caused by the breaking of spatial constraints between convolutional filters.</p><p>While all of the above methods aim to generate visual representations in input space, few have focused on the interpretation of <italic>feature maps</italic> that encode how input features are combined in subsequent layers. In genomic studies, lower level convolutional filters capture short motifs, while upper layers learn the combinatorial &#x02018;grammar&#x02019; of these motifs. Recovering these combinatorial interactions may reveal biological mechanism and allow us to extract more biological insights.</p><p>Here we introduce DeepResolve, a gradient ascent based visualization framework for feature map interpretation. DeepResolve computes and visualizes <italic>feature importance maps</italic> and <italic>feature importance vectors</italic> which describe the activation patterns of channels at a intermediate layer that maximizes a specific class output. We show that even though gradient ascent methods are less informative when used to generate representations in input space, gradient methods are very useful when conducted in feature map space as a tool to interpret the internal logic of a neural network. By using multiple random initializations and allowing negative values, we explore the feature space efficiently to cover the diverse set of patterns that a model learns about a class. A key insight of DeepResolve is that the visualization of the diverse states of an internal network layer reveals complex feature contribution patterns (e.g. negatively contributing or non-linearly contributing features) and combinatorial feature interactions which can not be easily achieved using other existing visualization tools that operate on input space. The correlation of the positive feature importance vector for distinct classes reveals shared features between classes and can lead to an understanding of shared mechanism. Our automatic pipeline is capable of generating analysis results on feature importance, feature interactions and class similarity, which can be used for biological studies. DeepResolve requires no input dataset or massive post-processing steps and thus is spatially efficient.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Visualizing feature importance and combinatorial interactions</title><sec id="Sec4"><title>Class Specific Feature Importance Map and Feature Importance Vector</title><p>Unlike methods which use gradient-ascent to generate sequence representations in the input layer[<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR26">26</xref>], DeepResolve uses gradient-ascent to compute a class-specific optimal feature map <italic>H</italic><sub><italic>c</italic></sub> in a chosen intermediate layer <italic>L</italic>. We maximize the objective function: 
<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$H_{c} = \mathop{\arg\max}\limits_{H} S_{c}(H)-\lambda||H||_{2}^{2}$$ \end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>arg max</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math><graphic xlink:href="12859_2019_2957_Article_Equa.gif" position="anchor"/></alternatives></disp-formula><italic>S</italic><sub><italic>c</italic></sub> is the score of class <italic>c</italic>, which is the <italic>c</italic>-th output in the last layer before transformation to probability distribution (before sigmoid or soft-max). The class-specific optimal feature map is <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$H_{c} \in \mathcal {R}^{K \times W}$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq1.gif"/></alternatives></inline-formula> for a layer having <italic>K</italic> feature maps of size <italic>W</italic> (<italic>W</italic> is the width of the feature maps after max-pooling and <italic>W</italic>=1 when global max-pooling is used). <italic>K</italic> is the number of sets of neurons that share parameters. Each set of neurons that share parameters is called a channel, and each channel captures unique local features within a receptive field. We name <italic>H</italic><sub><italic>c</italic></sub> a <italic>feature importance map</italic> (FIM) for class <italic>c</italic>, and each map entry <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$(H^{k}_{i})_{c}$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq2.gif"/></alternatives></inline-formula> evaluates the contribution of a neuron from channel <italic>k</italic> in a specific position <italic>i</italic> in a layer. When local max-pooling is used, a FIM is capable of capturing the spatial pattern of feature importance within each channel. In typical biological genomic neural networks, spatial specificity is in general low because of the stochasticity in input feature locations. Therefore we compute a feature importance score <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\phi ^{k}_{c}$\end{document}</tex-math><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq3.gif"/></alternatives></inline-formula> for each of the <italic>K</italic> channels by taking the spatial average of the feature importance map (<italic>H</italic><sup><italic>k</italic></sup>)<sub><italic>c</italic></sub> of that channel. These scores collectively forms a <italic>feature importance vector</italic> (FIV) <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\Phi _{c}=((\phi ^{1}_{c}),(\phi ^{2}_{c}),\ldots,(\phi ^{k}_{c}))$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq4.gif"/></alternatives></inline-formula>: 
<disp-formula id="Equb"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\phi^{k}_{c}=\frac{1}{W}\sum\limits_{i=1}^{W} (H^{k}_{i})_{c}$$ \end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2019_2957_Article_Equb.gif" position="anchor"/></alternatives></disp-formula> Note that although the natural domain of feature map is <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbb {R}^{+}_{0}$\end{document}</tex-math><mml:math id="M14"><mml:msubsup><mml:mrow><mml:mi>&#x0211d;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq5.gif"/></alternatives></inline-formula> if ReLU units are used, we allow FIMs to have negative values during gradient ascent so as to distinguish channels with negative scores from those with close to zero scores. The feature importance score for each channel represents its contribution pattern to the output prediction and a channel can contribute positively, negatively or trivially. Positive channels usually associate with features that are &#x02019;favored&#x02019; by the class, whereas negative channels represents features that can be used to negate the prediction. We found that negative channels contain rich information about the reasoning of network decisions. Negative channels can capture patterns that do not exist in positive samples or non-linearly interacting patterns.</p></sec><sec id="Sec5"><title>Visualizing complex feature contribution patterns and interactions</title><p>Since deep neural networks have the capacity to learn multiple patterns for a single class, the learned function space can be multimodal. Moreover, the channels may contribute differently in different modes and their contributions may condition on the other channels, which indicate complex feature contribution patterns and interactions. However an input dependent visualization method usually explores only one of the modes when a specific sample is given. To explore the optimums in the space more efficiently, we repeat gradient ascent multiple times (<italic>T</italic> times) for each target class <italic>c</italic> using different random initialization sampled from normal distribution. This generates an ensemble of FIMs <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\{H^{t}_{c}\}$\end{document}</tex-math><mml:math id="M16"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq6.gif"/></alternatives></inline-formula> and FIVs <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\{\Phi _{c}^{t}\}$\end{document}</tex-math><mml:math id="M18"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq7.gif"/></alternatives></inline-formula> for each class.</p><p>To reduce the effect of bad initializations we weight each gradient ascent result using the output class score. We add an offset to the scores such that all trials have non-negative weights. The ensemble of FIVs exhibits diverse representations of feature space patterns learned by the corresponding class, with some channels having more inconsistent contribution than others. We evaluate the weighted variance of the feature importance score of each channel <italic>k</italic> in the ensemble, and use it as a metric to evaluate the inconsistency level (IL) of the channel <italic>k</italic> for target class <italic>c</italic>: 
<disp-formula id="Equc"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$IL_{c}^{k}=\text{Var}[(\phi_{c}^{k})^{t}]$$ \end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>I</mml:mi><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>Var</mml:mtext><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2019_2957_Article_Equc.gif" position="anchor"/></alternatives></disp-formula> Channels with a low inconsistency level contribute to the output either positively, negatively, or not at all. We define this type of channel as a <italic>additive channel</italic> because their contributions can be combined additively (e.g. AND/OR/NOT logic). We define channels with high inconsistency as <italic>non-additive channels</italic> since their contribution is inconsistent and usually conditioned on the other channels (e.g. XOR logic). We visualize the signs and magnitudes of FIV scores of the entire ensemble of FIVs as shown in Figs.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>. In this way both individual and combinatorial interactions between channels can be easily perceived. In the results section below we show the effectiveness of this visualization using synthesized data in discovering XOR logic where two channels always have opposite contributions.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Illustration of DeepResolve&#x02019;s working flow. <bold>a</bold> Feature Importance Vectors calculation. After a network is trained and a intermediate layer is selected, DeepResolve first computes the feature importance maps (FIM) of each of the channels using gradient ascent. Then for each channel, the Feature Importance Vector (FIV) score is calculated as the spatial average of its FIM scores. <bold>b</bold> Overall Feature Importance Vector calculation. For each class, DeepResolve repeats the FIV calculation <italic>T</italic> times with different random initializations. The weighted variance over the <italic>T</italic> times is then calculated as an indicator of inconsistency level (IL) of each channel. A Gaussian Mixture Model is trained on IL scores to determine the non-additiveness of a channel. For each channel, the <italic>T</italic> FIVs are combined with the reference to the inconsistency level to generate an Overall Feature Importance Vector (OFIV) which summarizes all &#x02018;favored&#x02019; and &#x02018;unfavored&#x02019; patterns of a class. Finally, we use the non-negative OFIVs of each class to analyze class similarity and the OFIVs to analyze class differences</p></caption><graphic xlink:href="12859_2019_2957_Fig1_HTML" id="MO1"/></fig>
<fig id="Fig2"><label>Fig. 2</label><caption><p>Illustration of the generation of OFIV from FIVs generated by all 10 runs of gradient ascent in synthetic data set I. Red circles on the X-axis represent positive channels and blue circles represent negative channels. Circle size is proportional to the absolute FIV value. The weighted variance (IL score) of each channel is plotted below the FIVs, where the darkness and circle size is proportional to the variance. The OFIV is visualized below, where the circle size reflect the overall importance score of a channel. The channels that are predicted as non-additive by the Gaussian Mixture Model fitted on the IL scores are labeled by a star. A seqlogo visualization of the filter weight is plotted next to the corresponding channel. Filter {a,f} and {c,d} which capture sequences that involve in XOR logic are correctly predicted as non-additive. Among the remaining filters, the top-OFIV ones {b,c,g} which capture the sequence that involve in AND logic are correctly predicted as additive</p></caption><graphic xlink:href="12859_2019_2957_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec6"><title>Summarizing feature contributions using Overall Feature Importance Vector</title><p>We summarize the contribution of a feature using an <italic>overall feature importance vector</italic> (OFIV) <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\bar {\Phi }_{c}$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq8.gif"/></alternatives></inline-formula> that takes into account the rich information of the magnitude and direction of the feature contribution embedded in the ensemble of FIVs.</p><p>We first calculate the weighted variance of the FIVs for each channel to get the inconsistency level (IL). Three Gaussian mixture models with the number of components varying from one to three are fitted over the IL scores to account for channels that are additive and non-additive. The final number of mixture components is picked to minimize the Bayesian Information Criterion (BIC).</p><p>We next categorize the channels by IL score and the sign of contribution to calculate category-specific OFIVs that properly characterizes the feature importance. The channels in the mixture component with the lowest mean are considered as either additive or unimportant. The remaining mixture components (if any) are considered as non-additive channels and can be further categorized by whether the sign of its FIVs in the ensemble is consistent. For channels considered as additive, unimportant, or non-additive with consistent sign, the OFIV is calculated as the weighted average of its scores across all FIVs. For channels considered as non-additive with inconsistent sign, the OFIV is calculated as the weighted average of the positive FIVs in the ensemble to reflect the feature contribution in cases where the channel is not used to negate the prediction.</p><p>Visualizing OFIVs and IL scores together, we recover both the importance level of different features and the presence of non-additive channels. We automatically produce a list of important features, and a list of non-additive features that are highly likely to involved in complex interactions.</p></sec></sec><sec id="Sec7"><title>Visualizing feature sharing and class relationship</title><p>The weight sharing mechanism of multi-task neural networks allows the reuse of features among classes that share similar patterns. In past studies, the weight matrix in the last layer has been used to examine class similarity. However, this is potentially problematic because the high-level features in a network&#x02019;s last layer tend to be class-specific. This method also fails to discover lower level feature sharing between classes that are rarely labeled positive together. Using OFIVs proposed above, we revisit the feature sharing problem to enable the discovery of lower-level feature sharing when the class labels are poorly correlated.</p><p>We observe that the network learns to use negative channels to capture class-specific patterns in other classes as a process of elimination to maximize the prediction accuracy. This potentially increases the distance of those classes in hidden space despite the fact that they may share other features. Thus, while neurons with both strong positive and negative OFIV scores are potentially important for making the prediction, only the ones with positive OFIV scores are truly associated with the target class. Inspired by this finding, we introduce a class similarity matrix <italic>A</italic> by taking pair-wise Pearson correlation of non-negative OFIV of all the classes. 
<disp-formula id="Equd"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$A_{C_{i}C_{j}}=\frac{\text{Cov}\left(\bar{\Phi}_{c_{i}}^{+},\bar{\Phi}_{c_{j}}^{+}\right)}{\sigma_{\bar{\Phi}_{c_{i}}^{+}} \sigma_{\bar{\Phi}_{c_{j}}^{+}}}$$ \end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2019_2957_Article_Equd.gif" position="anchor"/></alternatives></disp-formula><inline-formula id="IEq9"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\bar {\Phi }_{c}^{+}$\end{document}</tex-math><mml:math id="M26"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2957_Article_IEq9.gif"/></alternatives></inline-formula> encodes the composition of all positive contributing features for a given class in intermediate layer. By taking the difference of OFIV of a pair of classes, we can also generate a class difference map. 
<disp-formula id="Eque"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$D_{C_{i}C_{j}}=\bar{\Phi}_{c_{i}}-\bar{\Phi}_{c_{j}}$$ \end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2019_2957_Article_Eque.gif" position="anchor"/></alternatives></disp-formula> This map highlights features that are favored by one class but not favored by the other. This is especially helpful when studying cell-type specific problems where a key feature deciding differential expression or binding in different cell type might be crucial.</p></sec><sec id="Sec8"><title>Implementation details</title><p>We trained all of our models with Keras version 1.2 and the DeepSEA network is downloaded from the official website. We convert the torch DeepSEA model into Caffe using torch2caffe and the resulting model has same performance as the original network. We implemented DeepResolve for both Caffe and Keras. As baselines, we implemented saliency map and DeepMotif in Keras, and used DeepLIFT v0.5.1 for generating DeepLIFT scores.</p></sec></sec><sec id="Sec9" sec-type="results"><title>Results</title><sec id="Sec10"><title>Synthetic datasets</title><sec id="Sec11"><title>Recovering important features and combinatorial interactions</title><p>We tested if FIVs would highlight important features and identify complex feature interactions in a synthetic data set which contains both additive and non-additive combinatorial logic. Synthetic dataset I contains 100,000 DNA sequences, each containing patterns chosen from CGCTTG, CAGGTC and GCTCAT in random positions. We label a sequence 1 only when CAGGTC and one of (GCTCAT, CGCTTG) present, and otherwise 0. This is the combination of AND logic and XOR logic. We also include 20,000 sequences that are totally random and label them as 0. We trained a convolutional neural network with a single convolutional layer with 32 8bp filters and local max-pooling with stride 4, followed by a fully connected layer with 64 hidden units. 20% of the data were held out as a test set and the resulting test AUC was 0.985. We applied DeepResolve on the layer in between convolutional layer and fully connected layer, and each channel correspond to a convolutional filter that can be visualized as Position Weight Matrix after normalization.</p><p>As shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, when ranked by OFIV, the top filters predicted to be non-additive capture CGCTTG and GCTCAT, the pair of motifs that non-linearly (XOR) interact with each other. The top filters predicted to be additive characterize CAGGTC, the motif that additively (AND) interacts with the other ones. Furthermore, the FIVs correctly unveil the non-additive XOR interaction between GCTCAT and CGCTTG as the corresponding filters tend to have opposite signs all the time. The optimal number of Gaussian mixture components of the IL score is 3 (Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S1), indicating the existence of non-additiveness.</p><p>We further compared three types of input-dependent visualizations: DeepLIFT, saliency map, and saliency map multiplied by input. For our comparison we used positive and negative examples from synthetic dataset I, where the positive example contains GCTCAT and CAGGTC, and the negative example contains all three patterns. The network prediction on these examples are correct, suggesting that it has learned the XOR logic. Note that the original saliency map takes the absolute value of the gradients which never assign negative scores and thus limits the interpretation of the internal logic of a network. Thus we used the saliency map without taking the absolute value to allow for more complex visualizations. We compute attribution scores for each base pair in the input with regard to the positive class&#x02019;s softmax logit. As shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, the visualization on positive example can be biased by the choice of input since only the 2 patterns that present in the input will be highlighted and the third pattern is always missing. On the other hand, when a negative example is used as input, all three methods assign scores with the same signs to all three patterns, making the XOR logic indistinguishable from AND logic. DeepLIFT assigns positive score to both GCTCAT and CAGGTC even though their co-existence lead to negative prediction. Moreoever, the saliency methods incorrectly assign negative score to CAGGTC which is designed to always exists in positive class. This shows that saliency methods can be unstable in attributing positively contributing patterns when complex non-linear logic exists.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Input-dependent visualizations produce unstable results on XOR logic and fail to capture the XOR interaction. Three types of input-dependent visualizations on example positive and negative sequence from synthetic data set I. The visualization using positive example (left) only highlight two of the 3 predefined patterns because a positive sample can only contain one of GCTCAT,CGCTTG, while the third pattern will always be missing. When using negative example which contains all three patterns as the input, all of the methods assign either all positive or all negative scores to the three patterns (right), failing to capture the XOR interaction between GCTCAT and CGCTTG. The saliency methods predict negative score for CAGGTC, a pattern that should always exists in positive examples, suggesting that these methods are not stable enough when dealing with complex logic</p></caption><graphic xlink:href="12859_2019_2957_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec12"><title>Recovering class relationships</title><p>We synthesized dataset II to test our ability to discover feature sharing when the labels are poorly correlated. Synthetic dataset II has 4 classes of DNA sequences with one class label assigned to each sequence. Class 1 contains GATA and CAGATG, class 2 contains TCAT and CAGATG, Class3 contains GATA and TCAT, while class 4 only contains CGCTTG. The introduced sequence patterns are deliberately selected such that three of the classes share half of their patterns, while class 4 is totally different. These four classes are never labeled as 1 at the same time, thus the labels yield zero information about their structural similarities. We trained a multi-task CNN with a single convolutional layer that has 32 8bp long filters, one fully connected layer with 64 hidden neurons, and a four-neuron output layer with sigmoid activation to predict the class probability distribution. The test AUC is 0.968, 0.967, 0.979, 0.994 for class 1 to 4.</p><p>Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a shows the OFIV for each of the classes, and the optimal number of Gaussian mixture components of the IL score for all of the classes is one (Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S1), correctly indicating that only additive channels exist in these classes. We observe that the channels with the top OFIV (red) correctly capture the sequence determinants of the corresponding class. We observe strong negative terms (blue) in OFIVs for all classes, representing sequence patterns &#x02018;favored&#x02019; by other alternative classes, which validates our hypothesis that the &#x02019;process of elimination&#x02019; truly exists. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>b compares class similarity matrices generated by our method and using the last layer weight matrix. The non-negative OFIV correlation matrix successfully assigned higher similarity score to class 1+2, class 1+3 and class 2+3, while the other methods failed to do so. Note that for class 1+3 and class 2+3, the similarity scores estimated by the last layer weight dot product are strongly negative, suggesting that the same features will lead to the opposite predictions between these pairs of classes. While consistent with label correlation, this interpretation is contradictory to the fact that those classes are actually similar in feature composition, showing limitations of conventional methods that are based on the last layer weight. The correlation when using both positive and negative ONIV scores suggest similar pattern as the last layer weight, showing that the negative terms confounds the similarity analysis.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Visualization of DeepResolve in multi-task networks. <bold>a</bold> Overall Feature Importance Vector for Synthetic dataset II class 1 - 4. Each circle on the X-axis represents a channel, with red representing positive OFIV score and blue representing negative OFIV score. Each column corresponds to one of the 32 channels that is shared among all four classes. OFIV successfully ranks predefined sequence features as the most important features for each of the classes, while reveals &#x02018;unfavored&#x02019; features that are used to separate a class from its competing classes. <bold>b</bold> Correlation matrix of class based features shows the benefit of non-negative OFIV scores. The predefined sequence pattern for each class is shown (<bold>a</bold>). Our proposed Class Similarity Matrix (top-left) successfully assigns high correlation to (Class1, Class2), (Class2, Class3) and (Class1, Class3) and low correlation to all pairs with Class 4. The matrix in top right corner suggest low correlation between the labels of each class. The matrix on the bottom left is the Pearson correlation of ONIV score without removing the negative terms, and the bottom right matrix is calculated by taking the cosine of the corresponding rows in last layer weight matrix. The bottom two both fail to assign higher similarity score to combinations of classes that share sequence features</p></caption><graphic xlink:href="12859_2019_2957_Fig4_HTML" id="MO4"/></fig></p></sec></sec><sec id="Sec13"><title>Experimental datasets</title><p>We analyzed two experimental datasets to examine DeepResolve&#x02019;s ability to recover biologically important features, and to discover correlation in features that might relate to mechanism.</p><sec id="Sec14"><title>Identifying key motifs in models of TF binding</title><p>We applied DeepResolve to convolutional neural networks trained on 422 Transcription Factor ChIP-Seq experiments for which the TF motifs are available in the non-redundant CORE motifs for vertebrates in JASPAR 2015 ([<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]) and only one motif exists for each TF. The positive set contains 101-bp sequences centered at motif instances that overlap with the ChIP-seq peaks. For each TF, the JASPAR motif for the corresponding factor (Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Table S1) is used to identify motif instances using FIMO. The negative set are shuffled positive sequences with matching dinucleotide composition. Each sequence is embedded into 2-D matrices using one-hot encoding. We train a single-class CNN for each experiment using one convolutional layer with 16 filters of size 25 with global max-pooling, and 1 fully connected layer with 32 hidden units. The mean of the AUC for these 422 experiments is 0.937 and the standard deviation is 0.035. We then generate FIMs and OFIVs for each experiment on the last convolutional layer, and rank the filters using OFIV scores. 420 of the 422 experiments contain only additively contributing features (Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S1).We convert the top filters into position weight matrices (PWMs) and match them with known motif for the target TF using TOMTOM [<xref ref-type="bibr" rid="CR28">28</xref>], and count how many times we hit the known motif in top 1, top 3 and top 5 filters with matching score <italic>p</italic>-value less than 0.5 and 0.05. We compare our method to DeepMotif ([<xref ref-type="bibr" rid="CR26">26</xref>]), a visualization tool that generates important sequence features by conducting gradient ascent directly on the input layer. We improved DeepMotif&#x02019;s initialization strategy to allow multiple random initializations instead of using an all 0.25 matrix (naming it enhanced-DeepMotif), and take the most informative 25bp fragment of generated sequences with top 5 class score. We also compared with three gradient-based methods, deepLIFT,saliency map, and its variation where the gradients are multiplied by the inputs to the neurons. However we conducted them on an intermediate layer instead of on input layer. We used all sequences from the positive training set, and took the average of scores assigned to a channel as an indication of the importance of a channel.</p><p>Shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>, our method successfully proposes known matching motifs as top 5 features in all of the 422 experiments with TOMTOM <italic>p</italic>-value less than 0.5, and in 421 out of 422 experiments with <italic>p</italic>-value less than 0.05, which outperforms enhanced DeepMotif by &#x0223c;&#x02009;3-fold. Our method also outperforms saliency map and its variation in top-1, top-3, top-5 accuracy, and outperforms deepLIFT in top-3, top-5 accuracy with TOMTOM <italic>p</italic>-value less than 0.5. We selected the top filter that matched a known canonical motif with lowest TOMTOM <italic>p</italic>-value from each experiment, and conducted Mann-Whitney Ranksum (unpaired) and Wilcoxon (paired) rank test between the ranks that DeepResolve and input-dependent methods assign to these filters. Our method is significantly better (<italic>p</italic>&#x0003c;0.000001) then the saliency map method and its variation on both tests and is comparable to DeepLIFT even though we did not refer to any input dataset when calculating our OFIVs. The distribution of optimal numbers of Gaussian mixture components for all the experiments is plotted in Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S1, where only 2 of the experiments have potentially non-additive channels. This result demonstrates that the logic for single TF binding is mostly additive and complex feature interactions such as XOR logic are unlikely. It also shows that the convolutional filters in genomic studies can capture motifs accurately by themselves, which lays a good foundation for hierarchical feature extraction and interpretation tools like DeepResolve.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Top-1, top-3, top-5 accuracy in identifying matching motif for TF binding (out of 422 experiments) with similarity score (<italic>p</italic>-value) smaller than 0.5 and 0.05, and the paired/unpaired rank tests of the proposed ranks of best matching filters between our method and the input-dependent methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">Top 1</th><th align="left" colspan="2">Top 3</th><th align="left" colspan="2">Top 5</th><th align="left">Ranksum</th><th align="left">Wilcoxon</th></tr><tr><th align="left">TOMTOM <italic>P</italic>-value</th><th align="left">0.5</th><th align="left">0.05</th><th align="left">0.5</th><th align="left">0.05</th><th align="left">0.5</th><th align="left">0.05</th><th align="left"><italic>p</italic>-values</th><th align="left"><italic>p</italic>-values</th></tr></thead><tbody><tr><td align="left">DeepResolve (ours)</td><td align="left">412</td><td align="left">407</td><td align="left">421</td><td align="left">418</td><td align="left">422</td><td align="left">421</td><td align="left">N/A</td><td align="left">N/A</td></tr><tr><td align="left">DeepLIFT</td><td align="left">418</td><td align="left">418</td><td align="left">420</td><td align="left">420</td><td align="left">421</td><td align="left">421</td><td align="left">0.784</td><td align="left">0.412</td></tr><tr><td align="left">Saliency*activation</td><td align="left">404</td><td align="left">393</td><td align="left">419</td><td align="left">419</td><td align="left">420</td><td align="left">420</td><td align="left">7.478&#x000d7;10<sup>&#x02212;7</sup></td><td align="left">1.049&#x000d7;10<sup>&#x02212;9</sup></td></tr><tr><td align="left">Saliency</td><td align="left">388</td><td align="left">377</td><td align="left">417</td><td align="left">416</td><td align="left">420</td><td align="left">420</td><td align="left">4.63&#x000d7;10<sup>&#x02212;7</sup></td><td align="left">4.63&#x000d7;10<sup>&#x02212;15</sup></td></tr><tr><td align="left">enhanced DeepMotif</td><td align="left">217</td><td align="left">89</td><td align="left">310</td><td align="left">123</td><td align="left">343</td><td align="left">147</td><td align="left">N/A</td><td align="left">N/A</td></tr></tbody></table></table-wrap></p><p>We further analyzed the learned convolutional filters from all 422 TF binding models by visualizing their activation patterns and relevance to known motifs. We grouped them into four groups by the ranks of ONIV score and plotted the distribution of the averaged activation scores across all negative and positive examples. We also plotted the distribution of TOMTOM <italic>p</italic>-values of the corresponding motif for each group. As shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, the top ranking group (right most) has highest activation in positive examples and lowest activation in negative examples, and has the most significant motif matching <italic>p</italic>-values. This suggest that ONIV successfully selected highly relevant and informative filters that can separate the positive and negative sets.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Distribution of positive sample activation level, negative sample activation level and motif matching <italic>p</italic>-values of filters grouped by their ONIV score ranking. We collected convolutional filters from all 422 TF binding models and group them into four groups by the ranks of ONIV score, each containing 1688 filters. Each panel represents one of the groups and the ONIV ranks increase from left to the right. The averaged activation scores across all negative and positive examples are calculated for each filter, and is normalized to [0,1] within each network. The top ranking group (right most) has high activation in positive examples while low activation in negative examples, and has the most significant motif matching pvals. This is suggesting that DeepResolve ranks highly relevant and informative filters that can separate positive and negative set well</p></caption><graphic xlink:href="12859_2019_2957_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec15"><title>Identifying sequence feature sharing and class correlations in DeepSEA</title><p>We evaluated DeepResolve&#x02019;s ability to discover important features and identify shared features and class similarities across distinct classes in the DeepSEA network[<xref ref-type="bibr" rid="CR8">8</xref>], a classic multi-task convolutional network trained on whole genome data to predict 919 different features including chromatin accessibility, TF binding and histone marks across a variety of cell types. DeepSEA compresses a large training set into its parameters and thus we sought to interpret DeepSEA&#x02019;s parameters to uncover biological mechanism.</p><p>In DeepSEA, input sequences are 1000bp long, and the labels are 919 long binary vectors. The network has 3 convolutional layers with 320, 480, 960 filters, and 1 fully connected layer. We chose the input to the 3<italic>r</italic><italic>d</italic> convolutional layer as <italic>H</italic> to generate feature importance maps, where the activation of a channel is determined by a 51bp sequence segment in the input (receptive field). We visualized the sequence features of a channel by <italic>l</italic><sub>2</sub>-regularized gradient ascent over its receptive field to maximize the channel activation. We initialized the input with the top ten 51bp fragment from the training sequences that maximize the channel activation. We applied a heuristic thresholding to the optimized input segments and normalized them to sum up to one in each column, and used TOMTOM to compare the resulting position weight matrix with known JASPAR motifs. Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> left panel shows the -log10 of the TOMTOM Q-values for each pair of channel and its top matching motifs. We discovered 218 channels that capture sequence features that match with 200 known JASPAR motifs with Q-value smaller than 0.005, and we observed channels that capture single motif, multiple motifs, consecutive motif with its reverse compliment (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>). We show that a single channel can capture both a motif and its reverse compliment depending on the input sequences, and we captures this dynamic by using multiple initializations for the gradient ascent.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Visualization of sequence features captured by the 480 channels in 2nd convolutional layer of DeepSEA. The sequences are generated using gradient ascent (see section <xref rid="Sec15" ref-type="sec">1</xref>). The matrix represents -log10 of TOMTOM Q-values for each pair of channel and its top matching motifs. Each row represents a known JASPAR motif which has been ranked as top 1 matching motif for at least one of the channels. Only pairs that achieve less than 0.005 Q-value are represented with actual Q-value, and the dark blue region represents default value for low Q-values. In the right panel, the left column shows the SeqLogo visualizations of representative gradient ascent outputs of 5 of the channels, and the top matching motifs are shown in the right column. Channel 116 and 451 captures single motif of Alx4 and MafG. Channel 280 captures 3 consecutive motifs (GATA1,Myod1 and GATA2), while channel 77 captures consecutive NFYB/YA motif and its reverse compliment. Channel 179 captures either REST or its reverse compliment depending on the input sequences used for initialization</p></caption><graphic xlink:href="12859_2019_2957_Fig6_HTML" id="MO6"/></fig></p><p>We next computed a class similarity matrix based upon OFIVs and found that the resulting matrix revealed similarities between the decision functions that underlie distinct classes, even when the classes themselves were not strongly correlated. We first calculated FIVs and their weighted variances for each class. The distribution of optimal numbers of Gaussian mixture components for all the experiments is plotted in Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S1, where only 2 of the experiments have potentially non-additive channels. This indicates that the majority of the classes in DeepSEA employ additive logic where binding can be determined by the additive contribution of several motifs. We then generated a class similarity matrix as described in Section <xref rid="Sec7" ref-type="sec">1</xref>. Given that DeepSEA takes in 1000bp long sequences around the biological event, it captures upstream and downstream sequence context. Therefore our proposed metric measures similarities between the contextual structures of a pair of regulators, which could imply interesting correlations in functionality and mechanism. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> compares DeepResolve&#x02019;s class similarity matrix with the label correlation matrix and the dot product matrix of last layer weights for all classes. DeepResolve&#x02019;s class similarity matrix revealed strong correlation between pairs of TFs/histone marks/DNase hypersensitivity that do not necessarily co-appear within 200 bp or having strong last layer weight correlation, but are functionally relevant.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Class similarity map for DeepSEA. X and Y axis represents 919 different experiments including DNase I hypersensitivity, TF binding and histone marks across different cell types. The sub-matrix highlighted by the red box is used for DNase correlation pattern analysis in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref></p></caption><graphic xlink:href="12859_2019_2957_Fig7_HTML" id="MO7"/></fig></p><p>We then examined the correlation pattern between selected TF/histone marks and DNase I hypersensitivity across different cell types to explore the shared components of their decision functions. Figure&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>a shows the bi-clustering result on the TF-histone mark/DNase similarity matrix. We observed clusters of TFs and histone marks sharing similar patterns, and some of them exhibit cell-type specific effect on DNase hypersensitivity (see Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S2). We collapsed the map into 1-D by calculating number of strong positive similarity (larger than 0.52, 85% quantile of all correlations) and negative similarity (smaller than 0, 15% quantile of all correlations) with DNase experiments for each TF/Chromatin mark. As shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>b, we characterized each TF and histone mark&#x02019;s association with chromatin accessibility using these indexes. We identified groups of TFs/histone marks that are highly correlated with DNase hypersensitivity (located to the left side of the histogram), and most of them are known to be involved in Chromatin Regulation / Acetylation Pathway, e.g. CTCF, POL2, CHD1/2, PLU1(KDM5B), SMC3, RAD21, GTF2B/GTF2F1, TBP, etc., or known to be essential for transcription activation, e.g. PHF8, USF2, H3K4me2, H3K27ac. We also identified groups of TFs/histone marks that are negatively correlated with DNase hypersensitivity and observe that most of them are well-known transcriptional repressors and repressive marks, e.g. ZNF274, EZH2, SUZ12,H3K9me3, H3K27me3 (see Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S3 for detailed list of the TFs/histone marks inside the box plotted in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>).
<fig id="Fig8"><label>Fig. 8</label><caption><p><bold>a</bold> Bi-clustering of TF/histone mark - DNase hypersensitivity similarity map (the highlighted box in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>), x-axis are the TF/histone mark experiments and y-axis are DNase hypersensitivity experiments across 125 different cell types. A zoom-in of the clusters can be found in Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S1. <bold>b</bold> Bar-plot of number of strong positive (red) and strong negative class similarity (blue) with DNase experiments for each of the TFs and histone marks. Majority of the TF/histone marks in the left box are known chromatin regulators, and majority of TF/histone marks in the right box are known transcription repressor. A zoom-in of the bar-plot can be found in Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S2</p></caption><graphic xlink:href="12859_2019_2957_Fig8_HTML" id="MO8"/></fig></p><p>Another way of utilizing the class similarity matrix is to directly use it as a metric of distance for clustering. We performed hierarchical clustering of the 919 ChIP-seq experiments and identified meaningful clusters where targets within the same cluster are known to be similar to each other, including groups of the same TF across different cell types, or groups of different TFs in same cell type (Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>). We found many of the clusters consist of TFs that are known to be interacting, such as forming a complex or cohesin (c-Fos and JunD [<xref ref-type="bibr" rid="CR29">29</xref>]; SMC3 and Rad21 [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR31">31</xref>]),co-repression(KAP1 and ZNF263 [<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR33">33</xref>]), competing (ELK1 and GABP [<xref ref-type="bibr" rid="CR34">34</xref>]) or known to be essential for each other to regulate transcription (EZH2, SUZ12 and H3K27me3 [<xref ref-type="bibr" rid="CR35">35</xref>, <xref ref-type="bibr" rid="CR36">36</xref>];Pol III (RPC155),TFIIIB (BRF1/2 and BDP1 are subunits for TFIIIB) and TFIIIC). We contrast the result from DeepResolve with the label correlation matrix for each cluster and show that even though label correlation pick up some of the above mentioning pairs (e.g. SMC3 and Rad21), it can sometimes miss some pairs (e.g. c-Fos and JunD, KAP1 and ZNF263) while DeepResolve captures these pairs even when data from different cell types are used. We further visualize the OFIV of clusters that exhibit cell type or TF specificity, and recognize sequence features that are potentially contributing to cell type specific binding or the binding of a single TF across different cell types (see Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S4).
<fig id="Fig9"><label>Fig. 9</label><caption><p>Hierarchical clustering results of 919 biological targets using correlation of positive OFIV as distance metric. Each panel represents a cluster, in which the left matrix is the sub-matrix of the class similarity map in 2nd convolutional layer(see Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>) among classes in the cluster, and the right matrix is the sub-matrix of label correlation between the classes. Each of the clusters consist of TFs that are known to be interacting, such as forming a complex or cohesin (c-Fos and JunD (<bold>b</bold>), SMC3 and Rad21 (<bold>a</bold>)), co-repression (KAP1 and ZNF263 (<bold>c</bold>)), competing (ELK1 and GABP (<bold>d</bold>) or known to be essential for each other to regulate transcription (EZH2, SUZ12 and H3K27me3 (<bold>f</bold>)). Cluster (<bold>e</bold>) consists of the subunits of Pol III (RPC155) and 2 essential transcription factors for Pol III : TFIIIB (BRF1/2 and BDP1 are subunits for TFIIIB) and TFIIIC. We show that even when the label correlation is not significant, our class similarity matrix can still capture the functional relevance of the interacting TFs</p></caption><graphic xlink:href="12859_2019_2957_Fig9_HTML" id="MO9"/></fig></p></sec></sec></sec><sec id="Sec16" sec-type="discussion"><title>Discussion</title><sec id="Sec17"><title>Potential artifacts in minor cases</title><p>Our method is designed to preserve positively attributed channels when generating an ONIV. It is possible that a channel detects the existence of an input feature through reduction of activation, and a negatively attributed channels of this type can be positively contributing to the output. We visualize the information content of positive and negative weights from all convolutional filters in the 422 TF binding experiments (see Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S5), and we show that networks tend to learn more information from positively weighted evidence than negatively weighted evidence. This can be in part explained by the bias of back-propagating gradients for positively activated neurons when ReLU is used. Our observations suggest that negative-negative paths in neural networks are infrequent and thus our design choice towards biasing the positive channels is not very likely to be confounded by these paths.</p><p>We noticed that in some experiments, high ranking filters do not always match the known ground truth. While these filters may be artifacts, we found their existence highly relevant to the network and the training data and thus they should not be ignored. We analyzed the normalized activation level in the postive examples, information content and the motif matching <italic>p</italic>-values of all convolutional filters in the 422 TF experiments. As shown in Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S5B, there exist strongly activated filters with high information content while their <italic>p</italic>-value for motif matching is not significant. Moreover, we divided filters into four groups depending on the ranks that DeepResolve assigned to them, and we visualized their activation level in positive examples verses the motif matching <italic>p</italic>-values, colored by the information content of its positive weights. As shown in Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S5C and Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, the top ONIV ranked filters are highly activated in positive samples and have low activation in negative examples, and match known motifs with high significance. Filters located on the right top corners are strongly activated in positive training example while not matching a known motif. These could either be the result of over-fitting the training set or true patterns in the training set that are not covered by the chosen known motif. There exist some top ranking filters that are low in both activation and motif matching significance (circled in green in Additional file&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>: Figure S5C), we consider this type of filters as artifacts of the visualization procedure. Among 1688 filters in the top 25% group, only 67 (less than 4%) of them belong to this type (<italic>p</italic>-value larger than 0.5, activation level within bottom 25%). We also found that this artifact exists in all visualization methods that we examined, 12 in DeepLIFT and 35 in saliency map.</p></sec><sec id="Sec18"><title>Intermediate layer selection for analysis</title><p>DeepResolve can learn feature contribution and interaction patterns at any layer of a network with regard to any desired output neuron, and thus it is important to select a layer for network interpretation that is informative for a specific task. We find that a good heuristic is to select a layer <italic>L</italic> such that its neuron activation correspond to local sequence patterns comparable to motifs. In addition, the selected layer should not be distant from an output neuron of interest. This is because additional intervening non-linear layers introduce excessive instability that can inhibit learning accurate feature interactions. For many existing networks for predicting genomic functional regulatory elements the optimal choice for analysis is the layer located between the fully connected layers and convolutional layers [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]. For DeepSEA [<xref ref-type="bibr" rid="CR8">8</xref>] which has 3 convolutional layers, we found the input to last convolutional layer is most informative. We also observed that as we pick layers that are closer to the input, the similarity matrix becomes denser because the sharing of lower level features is more likely than the sharing of higher level features. Thus picking the right layer for analyzing class similarity depends on the feature granularity desired.</p></sec><sec id="Sec19"><title>Selection of hyper-parameters</title><p>The L2 norm in the objective function for gradient ascent is essential in controlling the scale of generated feature maps. We experimented with different L2 coefficients <italic>&#x003bb;</italic> ranging from 0.3 to 2.8 and observed that <italic>&#x003bb;</italic> does not substantially affect the ranking of channels in general, even though the scale of generated FIVs varies with the choice of <italic>&#x003bb;</italic>. A good heuristic for picking <italic>&#x003bb;</italic> is to select a <italic>&#x003bb;</italic> such that the resulting feature importance map has a norm that is comparable to the norm of mean feature map activation which can be calculated using a small set of realistic input sequences randomly sampled from the training set. We tested different step sizes including 0.1,0.01,and 0.001, and we also found that the step size of gradient ascent does not have a significant effect on the results when it is reasonably selected. It should not be so large that the objective does not increase and not so small such that the convergence rate is extremely slow. In practice we use <italic>learning rate decay</italic> to gradually reduce the learning rate with the number of steps. 
<disp-formula id="Equf"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$lr=lr_{0}*max((step-start\_decay)^{-\alpha},min\_lr)$$ \end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mtext mathvariant="italic">lr</mml:mtext><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02217;</mml:mo><mml:mtext mathvariant="italic">max</mml:mtext><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">step</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mtext mathvariant="italic">start_decay</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext mathvariant="italic">min_lr</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2019_2957_Article_Equf.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec20"><title>Complex logic and feature sharing in biological problems</title><p>While we observed the DeepSEA model consists mainly of additive logic with a few non-additive channels, XOR logic may exist. The fact that XOR logic was not more obvious could be the consequence of the unbalanced training data in DeepSEA where most of the sequences have negative labels for a single class, which makes the learning of complex logic difficult. DeepResolve is defined to uncover non-additive interactions when they are present in a model, while the training of model with robust non-additive interactions can be difficult. Biological systems do contain TFs that bind differently but have partially shared features, including TFs that associate with different co-factors and shared pioneer factors[<xref ref-type="bibr" rid="CR37">37</xref>]. In these interactions a pioneer factor opens chromatin that enables a distinct TF specific co-factor to bind. Our capability of discovering feature space correlations that are not present in label space can suggest interesting similarities between TFs that partially share a co-factor or functional role.</p></sec><sec id="Sec21"><title>Combining DeepResolve with existing tools</title><p>DeepResolve is designed to visualize how complex intermediate layer channel interactions contribute to decisions about a network task. It can be combined with any existing input-level visualization tools such as a saliency map or deepLIFT, which can provide fine-grained visualization of sequence features captured by the important channels that DeepResolve identifies. Similar work-flow was used to discover epistatic feature interactions [<xref ref-type="bibr" rid="CR38">38</xref>]. The use of DeepResolve can ease the computational burden for input-space visualization tools by reducing the number of layers and the length of receptive field for traditional methods which can lead to better location specific and more accurate visualizations.</p></sec></sec><sec id="Sec22" sec-type="conclusion"><title>Conclusions</title><p>DeepResolve is a gradient ascent based method that summarizes feature importance maps for visualizing and interpreting a network&#x02019;s behavior in feature space that is reference input free. DeepResolve visualizes the complex combinatorial interactions of lower level features that are crucial to model decision making. It also recovers feature space similarities between poorly correlated classes which may suggest shared biological mechanism. It is compatible to existing methods in discovering important sequence features and provides complimentary insights.</p></sec><sec sec-type="supplementary-material"><title>Additional file</title><sec id="Sec23"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="12859_2019_2957_MOESM1_ESM.pdf"><label>Additional file 1</label><caption><p>Supplementary Figures S1-S5, Supplementary Table S1. (PDF 10,216 kb)</p></caption></media></supplementary-material>
</p></sec></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>BIC</term><def><p>Bayesian information criterion</p></def></def-item><def-item><term>CNN</term><def><p>Convolutional neural network</p></def></def-item><def-item><term>FIM</term><def><p>Feature importance map</p></def></def-item><def-item><term>FIV</term><def><p>Feature importance vector</p></def></def-item><def-item><term>IL</term><def><p>Inconsistent level</p></def></def-item><def-item><term>OFIV</term><def><p>Overall feature importance vector</p></def></def-item><def-item><term>PWM</term><def><p>Position weight matrix</p></def></def-item><def-item><term>TF</term><def><p>Transcription factor</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>A preliminary version of this work has been presented at Workshop on Visualization for Deep Learning at ICML 2017 with the title &#x0201c;Visualizing Feature Maps in Deep Neural Networks using DeepResolve-A Genomics Case Study&#x0201d;.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>DKG and GL initiated the study. GL devised the computational methodology and implemented the algorithms for visualization. HZ trained the TF binding prediction models and provided thoughtful comments on the methods. The manuscript was prepared by GL, HZ and DKG. All authors discussed the results and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Funding was provided by NIH grants U01HG007037 and R01CA218094. The funding body has no involvement in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>The DeepSEA datasets can be downloaded from <ext-link ext-link-type="uri" xlink:href="http://deepsea.princeton.edu/help/">http://deepsea.princeton.edu/help/</ext-link>. The TF binding datasets can be downloaded from <ext-link ext-link-type="uri" xlink:href="http://gerv.csail.mit.edu/deepresolve/data">http://gerv.csail.mit.edu/deepresolve/data</ext-link>. The JASPAR motifs used in the analysis can be found in: <ext-link ext-link-type="uri" xlink:href="http://gerv.csail.mit.edu/deepresolve/JASPAR_CORE_vertebrates_nonredundant_20151026">http://gerv.csail.mit.edu/deepresolve/JASPAR_CORE_vertebrates_nonredundant_20151026</ext-link>. The other datasets used and/or analysed during the current study and the code for DeepResolve are available in <ext-link ext-link-type="uri" xlink:href="https://github.com/lgsaber/DeepResolve">https://github.com/lgsaber/DeepResolve</ext-link>.;</p></notes><notes><title>Ethics approval and consent to participate</title><p>Not applicable.</p></notes><notes><title>Consent for publication</title><p>Not applicable.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1</label><mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks In: Pereira F, Burges CJC, Bottou L, Weinberger KQ, editors. Advances in neural information processing systems 25. Curran Associates, Inc.: 2012. p. 1097&#x02013;1105. <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link>.</mixed-citation></ref><ref id="CR2"><label>2</label><mixed-citation publication-type="other">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition: 2015. p. 1&#x02013;9.</mixed-citation></ref><ref id="CR3"><label>3</label><mixed-citation publication-type="other">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings: 2015. http://arxiv.org/abs/1409.1556.</mixed-citation></ref><ref id="CR4"><label>4</label><mixed-citation publication-type="other">Sutskever I, Vinyals O, Le QV. Sequence to sequence learning with neural networks In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ, editors. Advances in Neural Information Processing Systems 27. Curran Associates, Inc.: 2014. p. 3104&#x02013;3112. <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</ext-link>.</mixed-citation></ref><ref id="CR5"><label>5</label><mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings: 2015. http://arxiv.org/abs/1409.0473.</mixed-citation></ref><ref id="CR6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>B</given-names></name><name><surname>Delong</surname><given-names>A</given-names></name><name><surname>Weirauch</surname><given-names>MT</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name></person-group><article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title><source>Nat Biotechnol</source><year>2015</year><volume>33</volume><issue>8</issue><fpage>3</fpage><lpage>1</lpage><pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id><pub-id pub-id-type="pmid">25574612</pub-id></element-citation></ref><ref id="CR7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Edwards</surname><given-names>MD</given-names></name><name><surname>Liu</surname><given-names>G</given-names></name><name><surname>Gifford</surname><given-names>DK</given-names></name></person-group><article-title>Convolutional neural network architectures for predicting DNA&#x02013;protein binding</article-title><source>Bioinformatics</source><year>2016</year><volume>32</volume><issue>12</issue><fpage>i121</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btw255</pub-id><pub-id pub-id-type="pmid">27307608</pub-id></element-citation></ref><ref id="CR8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title><source>Nat Methods</source><year>2015</year><volume>12</volume><issue>10</issue><fpage>931</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id><pub-id pub-id-type="pmid">26301843</pub-id></element-citation></ref><ref id="CR9"><label>9</label><mixed-citation publication-type="other">Quang D, Xie X. FactorNet: A deep learning framework for predicting cell type specific transcription factor binding from nucleotide-resolution sequential data. Methods. 2019. 10.1016/j.ymeth.2019.03.020. <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1046202318303293">http://www.sciencedirect.com/science/article/pii/S1046202318303293</ext-link>.</mixed-citation></ref><ref id="CR10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Gifford</surname><given-names>DK</given-names></name></person-group><article-title>Predicting the impact of non-coding variants on DNA methylation</article-title><source>Nucleic Acids Res</source><year>2017</year><volume>45</volume><issue>11</issue><fpage>e99</fpage><lpage>e99</lpage><pub-id pub-id-type="doi">10.1093/nar/gkx177</pub-id><pub-id pub-id-type="pmid">28334830</pub-id></element-citation></ref><ref id="CR11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>HJ</given-names></name><name><surname>Reik</surname><given-names>W</given-names></name><name><surname>Stegle</surname><given-names>O</given-names></name></person-group><article-title>DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning</article-title><source>Genome Biol</source><year>2017</year><volume>18</volume><issue>1</issue><fpage>67</fpage><pub-id pub-id-type="doi">10.1186/s13059-017-1189-z</pub-id><pub-id pub-id-type="pmid">28395661</pub-id></element-citation></ref><ref id="CR12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Snoek</surname><given-names>J</given-names></name><name><surname>Rinn</surname><given-names>JL</given-names></name></person-group><article-title>Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title><source>Genome Res</source><year>2016</year><volume>26</volume><issue>7</issue><fpage>990</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1101/gr.200535.115</pub-id><pub-id pub-id-type="pmid">27197224</pub-id></element-citation></ref><ref id="CR13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Reshef</surname><given-names>YA</given-names></name><name><surname>Bileschi</surname><given-names>M</given-names></name><name><surname>Belanger</surname><given-names>D</given-names></name><name><surname>McLean</surname><given-names>CY</given-names></name><name><surname>Snoek</surname><given-names>J</given-names></name></person-group><article-title>Sequential regulatory activity prediction across chromosomes with convolutional neural networks</article-title><source>Genome Res</source><year>2018</year><volume>28</volume><issue>5</issue><fpage>739</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1101/gr.227819.117</pub-id><pub-id pub-id-type="pmid">29588361</pub-id></element-citation></ref><ref id="CR14"><label>14</label><mixed-citation publication-type="other">Singh S, Yang Y, Poczos B, Ma J. Predicting enhancer-promoter interaction from genomic sequence with deep neural networks. bioRxiv. 2016;:085241.</mixed-citation></ref><ref id="CR15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelvecchi</surname><given-names>D.</given-names></name></person-group><article-title>Can we open the black box of AI?</article-title><source>Nat News</source><year>2016</year><volume>538</volume><issue>7623</issue><fpage>20</fpage><pub-id pub-id-type="doi">10.1038/538020a</pub-id></element-citation></ref><ref id="CR16"><label>16</label><mixed-citation publication-type="other">Zeiler MD, Krishnan D, Taylor GW, Fergus R. Deconvolutional networks. Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. 2010;:2528&#x02013;35. 10.1109/CVPR.2010.5539957.</mixed-citation></ref><ref id="CR17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>Md</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><article-title>Visualizing and understanding convolutional networks</article-title><source>Comput Vis&#x02013;ECCV 2014</source><year>2014</year><volume>8689</volume><fpage>818</fpage><lpage>33</lpage></element-citation></ref><ref id="CR18"><label>18</label><mixed-citation publication-type="other">Simonyan K, Vedaldi A, Zisserman A. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. In: 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings: 2014. http://arxiv.org/abs/1312.6034.</mixed-citation></ref><ref id="CR19"><label>19</label><mixed-citation publication-type="other">Springenberg JT, Dosovitskiy A, Brox T, Riedmiller M. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806. 2014.</mixed-citation></ref><ref id="CR20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>Sebastian</given-names></name><name><surname>Binder</surname><given-names>Alexander</given-names></name><name><surname>Montavon</surname><given-names>Gr&#x000e9;goire</given-names></name><name><surname>Klauschen</surname><given-names>Frederick</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>Klaus-Robert</given-names></name><name><surname>Samek</surname><given-names>Wojciech</given-names></name></person-group><article-title>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation</article-title><source>PLOS ONE</source><year>2015</year><volume>10</volume><issue>7</issue><fpage>e0130140</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0130140</pub-id><pub-id pub-id-type="pmid">26161953</pub-id></element-citation></ref><ref id="CR21"><label>21</label><mixed-citation publication-type="other">Shrikumar A, Greenside P, Kundaje A. Learning Important Features Through Propagating Activation Differences. In: Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017: 2017. p. 3145&#x02013;3153. <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v70/shrikumar17a.html">http://proceedings.mlr.press/v70/shrikumar17a.html</ext-link>.</mixed-citation></ref><ref id="CR22"><label>22</label><mixed-citation publication-type="other">Ribeiro MT, Singh S, Guestrin C. Why should i trust you?: Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016;:1135&#x02013;44.</mixed-citation></ref><ref id="CR23"><label>23</label><mixed-citation publication-type="other">Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. In: Advances in Neural Information Processing Systems: 2017. p. 4768&#x02013;4777.</mixed-citation></ref><ref id="CR24"><label>24</label><mixed-citation publication-type="other">Sundararajan M, Taly A, Yan Q. Axiomatic attribution for deep networks. In: Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017: 2017. p. 3319&#x02013;3328. <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v70/sundararajan17a.html">http://proceedings.mlr.press/v70/sundararajan17a.html</ext-link>.</mixed-citation></ref><ref id="CR25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finnegan</surname><given-names>A</given-names></name><name><surname>Song</surname><given-names>JS</given-names></name></person-group><article-title>Maximum entropy methods for extracting the learned features of deep neural networks</article-title><source>PLoS Comput Biol</source><year>2017</year><volume>13</volume><issue>10</issue><fpage>e1005836</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005836</pub-id><pub-id pub-id-type="pmid">29084280</pub-id></element-citation></ref><ref id="CR26"><label>26</label><mixed-citation publication-type="other">Lanchantin J, Singh R, Wang B, Qi Y. Deep motif dashboard: Visualizing and understanding genomic sequences using deep neural networks. In: PACIFIC SYMPOSIUM ON BIOCOMPUTING 2017. World Scientific: 2017. p. 254&#x02013;265.</mixed-citation></ref><ref id="CR27"><label>27</label><mixed-citation publication-type="other">Nguyen A, Yosinski J, Clune J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition: 2015. p. 427&#x02013;436.</mixed-citation></ref><ref id="CR28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>Shobhit</given-names></name><name><surname>Stamatoyannopoulos</surname><given-names>John A</given-names></name><name><surname>Bailey</surname><given-names>Timothy L</given-names></name><name><surname>Noble</surname><given-names>William</given-names></name></person-group><article-title>Quantifying similarity between motifs</article-title><source>Genome Biology</source><year>2007</year><volume>8</volume><issue>2</issue><fpage>R24</fpage><pub-id pub-id-type="doi">10.1186/gb-2007-8-2-r24</pub-id><pub-id pub-id-type="pmid">17324271</pub-id></element-citation></ref><ref id="CR29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Dai</surname><given-names>W</given-names></name><name><surname>Xu</surname><given-names>M</given-names></name></person-group><article-title>Role of c-Fos/JunD in protecting stress-induced cell death</article-title><source>Cell Prolif</source><year>2007</year><volume>40</volume><issue>3</issue><fpage>431</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2184.2007.00444.x</pub-id><pub-id pub-id-type="pmid">17531086</pub-id></element-citation></ref><ref id="CR30"><label>30</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brooker</surname><given-names>Amanda S.</given-names></name><name><surname>Berkowitz</surname><given-names>Karen M.</given-names></name></person-group><article-title>The Roles of Cohesins in Mitosis, Meiosis, and Human Health and Disease</article-title><source>Methods in Molecular Biology</source><year>2014</year><publisher-loc>New York, NY</publisher-loc><publisher-name>Springer New York</publisher-name><fpage>229</fpage><lpage>266</lpage></element-citation></ref><ref id="CR31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuin</surname><given-names>J</given-names></name><name><surname>Dixon</surname><given-names>JR</given-names></name><name><surname>van der Reijden</surname><given-names>MI</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Kolovos</surname><given-names>P</given-names></name><name><surname>Brouwer</surname><given-names>RW</given-names></name><etal/></person-group><article-title>Cohesin and CTCF differentially affect chromatin architecture and gene expression in human cells</article-title><source>Proc Natl Acad Sci</source><year>2014</year><volume>111</volume><issue>3</issue><fpage>996</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1073/pnas.1317788111</pub-id><pub-id pub-id-type="pmid">24335803</pub-id></element-citation></ref><ref id="CR32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groner</surname><given-names>AC</given-names></name><name><surname>Meylan</surname><given-names>S</given-names></name><name><surname>Ciuffi</surname><given-names>A</given-names></name><name><surname>Zangger</surname><given-names>N</given-names></name><name><surname>Ambrosini</surname><given-names>G</given-names></name><name><surname>D&#x000e9;nervaud</surname><given-names>N</given-names></name><etal/></person-group><article-title>KRAB&#x02013;zinc finger proteins and KAP1 can mediate long-range transcriptional repression through heterochromatin spreading</article-title><source>PLoS Genet</source><year>2010</year><volume>6</volume><issue>3</issue><fpage>e1000869</fpage><pub-id pub-id-type="doi">10.1371/journal.pgen.1000869</pub-id><pub-id pub-id-type="pmid">20221260</pub-id></element-citation></ref><ref id="CR33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lupo</surname><given-names>A</given-names></name><name><surname>Cesaro</surname><given-names>E</given-names></name><name><surname>Montano</surname><given-names>G</given-names></name><name><surname>Zurlo</surname><given-names>D</given-names></name><name><surname>Izzo</surname><given-names>P</given-names></name><name><surname>Costanzo</surname><given-names>P</given-names></name></person-group><article-title>KRAB-zinc finger proteins: a repressor family displaying multiple biological functions</article-title><source>Curr Genomics</source><year>2013</year><volume>14</volume><issue>4</issue><fpage>268</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.2174/13892029113149990002</pub-id><pub-id pub-id-type="pmid">24294107</pub-id></element-citation></ref><ref id="CR34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>P</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Tartakoff</surname><given-names>A</given-names></name><name><surname>Tao</surname><given-names>T</given-names></name></person-group><article-title>Competitive regulation of IPO4 transcription by ELK1 and GABP</article-title><source>Gene</source><year>2017</year><volume>613</volume><fpage>30</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1016/j.gene.2017.02.030</pub-id><pub-id pub-id-type="pmid">28254634</pub-id></element-citation></ref><ref id="CR35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasini</surname><given-names>D</given-names></name><name><surname>Bracken</surname><given-names>AP</given-names></name><name><surname>Jensen</surname><given-names>MR</given-names></name><name><surname>Denchi</surname><given-names>EL</given-names></name><name><surname>Helin</surname><given-names>K</given-names></name></person-group><article-title>Suz12 is essential for mouse development and for EZH2 histone methyltransferase activity</article-title><source>EMBO J</source><year>2004</year><volume>23</volume><issue>20</issue><fpage>4061</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/sj.emboj.7600402</pub-id><pub-id pub-id-type="pmid">15385962</pub-id></element-citation></ref><ref id="CR36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><article-title>SUZ12 is required for both the histone methyltransferase activity and the silencing function of the EED-EZH2 complex</article-title><source>Mol Cell</source><year>2004</year><volume>15</volume><issue>1</issue><fpage>57</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.molcel.2004.06.020</pub-id><pub-id pub-id-type="pmid">15225548</pub-id></element-citation></ref><ref id="CR37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherwood</surname><given-names>RI</given-names></name><name><surname>Hashimoto</surname><given-names>T</given-names></name><name><surname>O&#x02019;donnell</surname><given-names>CW</given-names></name><name><surname>Lewis</surname><given-names>S</given-names></name><name><surname>Barkal</surname><given-names>AA</given-names></name><name><surname>Van Hoff</surname><given-names>JP</given-names></name><etal/></person-group><article-title>Discovery of directional and nondirectional pioneer transcription factors by modeling DNase profile magnitude and shape</article-title><source>Nat Biotechnol</source><year>2014</year><volume>32</volume><issue>2</issue><fpage>171</fpage><pub-id pub-id-type="doi">10.1038/nbt.2798</pub-id><pub-id pub-id-type="pmid">24441470</pub-id></element-citation></ref><ref id="CR38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenside</surname><given-names>P</given-names></name><name><surname>Shimko</surname><given-names>T</given-names></name><name><surname>Fordyce</surname><given-names>P</given-names></name><name><surname>Kundaje</surname><given-names>A</given-names></name></person-group><article-title>Discovering epistatic feature interactions from neural network models of regulatory DNA sequences</article-title><source>Bioinformatics</source><year>2018</year><volume>34</volume><issue>17</issue><fpage>i629</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty575</pub-id><pub-id pub-id-type="pmid">30423062</pub-id></element-citation></ref></ref-list></back></article>